{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fd91e009",
      "metadata": {},
      "source": [
        "# Firstly: Preprocessing Workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "986c574a",
      "metadata": {},
      "source": [
        "## Step 1: Downloading PDFs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81c32c08",
      "metadata": {},
      "source": [
        "#### In this step, we will download all the provided technical-report PDFs, extracts text from each, and combines everything into a single txt file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "583b6a18",
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "import pathlib, re, logging\n",
        "import requests\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e5ffd4c",
      "metadata": {},
      "source": [
        "The provided PDFs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d81142c3",
      "metadata": {},
      "outputs": [],
      "source": [
        "PDF_LIST = [\n",
        "    (\"GPT-3 (Language Models are Few-Shot Learners, 2020)\", \"https://arxiv.org/pdf/2005.14165.pdf\"),\n",
        "    (\"GPT-4 Technical Report (2023)\", \"https://arxiv.org/pdf/2303.08774.pdf\"),\n",
        "    (\"PaLM (2022)\", \"https://arxiv.org/pdf/2204.02311.pdf\"),\n",
        "    (\"PaLM 2 Technical Report (2023)\", \"https://arxiv.org/pdf/2305.10403.pdf\"),\n",
        "    (\"Gemini 1.0 (2023)\", \"https://arxiv.org/pdf/2312.11805.pdf\"),\n",
        "    (\"Gemini 1.5 (2024)\", \"https://arxiv.org/pdf/2403.05530.pdf\"),\n",
        "    (\"Gemma (2024)\", \"https://arxiv.org/pdf/2403.08295.pdf\"),\n",
        "    (\"Gemma 2 (2024)\", \"https://arxiv.org/pdf/2408.00118.pdf\"),\n",
        "    (\"Gemma 3 Technical Report (2025)\", \"https://arxiv.org/pdf/2503.19786.pdf\"),\n",
        "    (\"CodeGemma (2024)\", \"https://arxiv.org/pdf/2406.11409.pdf\"),\n",
        "    (\"RecurrentGemma (2024)\", \"https://arxiv.org/pdf/2404.07839.pdf\"),\n",
        "    (\"LLaMA (2023)\", \"https://arxiv.org/pdf/2302.13971.pdf\"),\n",
        "    (\"Llama 2 (2023)\", \"https://arxiv.org/pdf/2307.09288.pdf\"),\n",
        "    (\"Llama 3 (2024)\", \"https://arxiv.org/pdf/2407.21783.pdf\"),\n",
        "    (\"Mistral 7B (2023)\", \"https://arxiv.org/pdf/2310.06825.pdf\"),\n",
        "    (\"Mixtral of Experts 8x7B (2024)\", \"https://arxiv.org/pdf/2401.04088.pdf\"),\n",
        "    (\"Nemotron-4 340B Technical Report (2024)\", \"https://arxiv.org/pdf/2406.11704.pdf\"),\n",
        "    (\"NVLM 1.0 (2024)\", \"https://arxiv.org/pdf/2409.11402.pdf\"),\n",
        "    (\"Qwen2 Technical Report (2024)\", \"https://arxiv.org/pdf/2407.10671.pdf\"),\n",
        "    (\"Qwen2-VL (2024)\", \"https://arxiv.org/pdf/2409.12191.pdf\"),\n",
        "    (\"Qwen2-Audio (2024)\", \"https://arxiv.org/pdf/2407.10759.pdf\"),\n",
        "    (\"Qwen2.5 Technical Report (2024)\", \"https://arxiv.org/pdf/2412.15115.pdf\"),\n",
        "    (\"Qwen2.5-VL Technical Report (2025)\", \"https://arxiv.org/pdf/2502.13923.pdf\"),\n",
        "    (\"Qwen2.5-Omni Technical Report (2025)\", \"https://arxiv.org/pdf/2503.20215.pdf\"),\n",
        "    (\"Qwen3 Technical Report (2025)\", \"https://arxiv.org/pdf/2505.09388.pdf\"),\n",
        "    (\"DeepSeek-V2 (2024)\", \"https://arxiv.org/pdf/2405.04434.pdf\"),\n",
        "    (\"DeepSeek-V3 Technical Report (2024)\", \"https://arxiv.org/pdf/2412.19437.pdf\"),\n",
        "    (\"DeepSeek-R1 (2025)\", \"https://arxiv.org/pdf/2501.12948.pdf\"),\n",
        "    (\"DeepSeek-Coder (2024)\", \"https://arxiv.org/pdf/2401.14196.pdf\"),\n",
        "    (\"GLM-130B (2022)\", \"https://arxiv.org/pdf/2210.02414.pdf\"),\n",
        "    (\"InternLM2 Technical Report (2024)\", \"https://arxiv.org/pdf/2403.17297.pdf\"),\n",
        "    (\"InternVL 2.5 (2024)\", \"https://arxiv.org/pdf/2412.05271.pdf\"),\n",
        "    (\"Phi-3 Technical Report (2024)\", \"https://arxiv.org/pdf/2404.14219.pdf\"),\n",
        "    (\"Phi-3 Safety Post-Training (2024)\", \"https://arxiv.org/pdf/2407.13833.pdf\"),\n",
        "    (\"Jamba: Hybrid Transformer–Mamba (2024)\", \"https://arxiv.org/pdf/2403.19887.pdf\"),\n",
        "    (\"PanGu-Σ (2023)\", \"https://arxiv.org/pdf/2303.10845.pdf\"),\n",
        "    (\"Yi: Open Foundation Models (2024)\", \"https://arxiv.org/pdf/2403.04652.pdf\"),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36e8b5ae",
      "metadata": {},
      "source": [
        "### (A)  Here `download_pdfs()` will download all the PDF files.\n",
        "\n",
        "**Outputs:**\n",
        "- `data/raw_pdfs/` – all downloaded PDFs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5fc23f3a",
      "metadata": {},
      "outputs": [],
      "source": [
        "OUT_DIR = pathlib.Path('data')\n",
        "PDF_DIR = OUT_DIR / 'raw_pdfs'\n",
        "TXT_DIR = OUT_DIR / 'text'\n",
        "\n",
        "def ensure_dirs(): # To ensure directories exist.\n",
        "    PDF_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    TXT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "ensure_dirs()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04791c20",
      "metadata": {},
      "source": [
        "**Note:** The step can take about 30 mins to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8d67012b",
      "metadata": {},
      "outputs": [],
      "source": [
        "USER_AGENT = 'HW1-Downloader/1.0 (+https://example.com)'\n",
        "def safe_filename(name: str) -> str: # To create a safe filename from the title.\n",
        "    name = re.sub(r\"[^\\w\\-. ]+\", \"_\", name).strip()\n",
        "    name = re.sub(r\"\\s+\", \"_\", name)\n",
        "    return name[:150]\n",
        "\n",
        "def download_pdfs(entries):\n",
        "    session = requests.Session()\n",
        "    retries = Retry(total=5, backoff_factor=0.8, status_forcelist=[429, 500, 502, 503, 504])\n",
        "    session.mount('https://', HTTPAdapter(max_retries=retries))\n",
        "    session.headers.update({'User-Agent': USER_AGENT})\n",
        "    records = []\n",
        "    for title, url in entries:\n",
        "        fname = safe_filename(title) + '.pdf'\n",
        "        fpath = PDF_DIR / fname\n",
        "        try:\n",
        "            if fpath.exists() and fpath.stat().st_size > 0:\n",
        "                logging.info(f'Skip (exists): {fname}')\n",
        "            else:\n",
        "                logging.info(f'Downloading: {url}')\n",
        "                with session.get(url, stream=True, timeout=60) as r:\n",
        "                    r.raise_for_status()\n",
        "                    with open(fpath, 'wb') as f:\n",
        "                        for chunk in r.iter_content(chunk_size=1048576):\n",
        "                            if chunk:\n",
        "                                f.write(chunk)\n",
        "            size_mb = fpath.stat().st_size / (1024*1024)\n",
        "            records.append((title, url, str(fpath), f'{size_mb:.2f}'))\n",
        "        except Exception as e:\n",
        "            logging.error(f'Failed: {title} — {e}')\n",
        "            records.append((title, url, str(fpath), 'ERROR'))\n",
        "    return records\n",
        "\n",
        "recs = download_pdfs(PDF_LIST) # To download the PDFs if needed."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bd1ba63",
      "metadata": {},
      "source": [
        "### B) Here `extract_text_one()` extract text from each PDF.\n",
        "\n",
        "### C) And `extract_all_to_combined()` uses `extract_text_one()` to combine everything into a single txt file `combined_raw.txt`.\n",
        "\n",
        "**Outputs:**\n",
        "- `data/text/combined_raw.txt` – concatenated raw text with document boundaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41349ea5",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P2' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P1' is an invalid float value\n"
          ]
        }
      ],
      "source": [
        "COMBINED_TXT = TXT_DIR / 'combined_raw.txt'\n",
        "\n",
        "# logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s: %(message)s', datefmt='%H:%M:%S')\n",
        "# To extract text from each file. \n",
        "def extract_text_one(pdf_path: pathlib.Path) -> str: \n",
        "    try:\n",
        "        from pdfminer.high_level import extract_text\n",
        "        text = extract_text(str(pdf_path))\n",
        "        if text and text.strip():\n",
        "            return text\n",
        "    except Exception as e:\n",
        "        logging.warning(f'pdfminer failed on {pdf_path.name}: {e}')\n",
        "    try:\n",
        "        import PyPDF2\n",
        "        text_parts = []\n",
        "        with open(pdf_path, 'rb') as f:\n",
        "            reader = PyPDF2.PdfReader(f)\n",
        "            for i, page in enumerate(reader.pages):\n",
        "                try:\n",
        "                    text_parts.append(page.extract_text() or '')\n",
        "                except Exception as e:\n",
        "                    logging.warning(f'PyPDF2 failed on page {i} of {pdf_path.name}: {e}')\n",
        "        return '\\n'.join(text_parts)\n",
        "    except Exception as e:\n",
        "        logging.error(f'All extractors failed on {pdf_path.name}: {e}')\n",
        "        return ''\n",
        "\n",
        "# to extract text from all the files and save the to the combined txt file.\n",
        "def extract_all_to_combined(records):\n",
        "    with open(COMBINED_TXT, 'w', encoding='utf-8', errors='ignore') as out:\n",
        "        for title, url, fpath, status in records:\n",
        "            pdf_path = pathlib.Path(fpath)\n",
        "            if status == 'ERROR' or not pdf_path.exists() or pdf_path.stat().st_size == 0:\n",
        "                logging.warning(f'Skipping extraction (bad download): {title}')\n",
        "                continue\n",
        "            logging.info(f'Extracting: {pdf_path.name}')\n",
        "            text = extract_text_one(pdf_path) # To extract text from each file.\n",
        "            out.write('\\n\\n===== DOCUMENT START =====\\n')\n",
        "            out.write(f'TITLE: {title}\\nURL: {url}\\nFILE: {pdf_path.name}\\n\\n')\n",
        "            out.write(text)\n",
        "            out.write('\\n===== DOCUMENT END =====\\n')\n",
        "\n",
        "extract_all_to_combined(recs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "957a4edb",
      "metadata": {},
      "source": [
        "## Step 2 — Cleaning the Combined Text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3dff1fb6",
      "metadata": {},
      "source": [
        "### A) In this step, we cleaned the raw combined text from the PDFs to make it suitable for later tokenization and modeling."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34ecda2a",
      "metadata": {},
      "source": [
        "#### Functions Implemented:\n",
        "- **remove_header_footer_lines** → Remove repeating headers/footers and page numbers.\n",
        "- **fix_hyphenation** → Merge words split across lines with `-` (e.g., infor-\\nmation → information).\n",
        "- **normalize_unicode** → Fix ligatures (ﬁ → fi), normalize unicode.\n",
        "- **drop_toc_and_garbage** → Remove table of contents lines or other PDF-extracted garbage (e.g., “Contents”, “Figure list”, stray navigation text).\n",
        "- **unwrap_soft_linebreaks** → Convert soft newlines inside paragraphs to spaces, keep paragraph breaks.\n",
        "- **drop_figure_table_captions** → Remove figure/table/algorithm captions that are not natural text.\n",
        "- **trim_references_at_end** → Cut off “References” or “Bibliography” sections at the end of documents.\n",
        "- **drop_author_block_preserve_title** → Remove initial author/affiliation block while preserving the paper title.\n",
        "- **clean_one_doc** → Apply all cleaning steps in order to one document.\n",
        "- **split_into_docs_within_combined** → Split the raw combined file into individual documents for cleaning.\n",
        "- **strip_boundaries_and_metadata** → Drop START/END markers and metadata.\n",
        "- **drop_orphan_numeric_lines** → Remove lines that contain only numbers (page numbers, section counters).\n",
        "- **drop_artifact_lines** → Remove random short artifacts.\n",
        "- **remove_cjk** → Remove Chinese/Korean characters and punctuation.\n",
        "- **remove_links** → Strip out URLs.\n",
        "- **remove_empty_lines** → Collapse whitespace by removing empty or whitespace-only lines.\n",
        "\n",
        "**Outputs:**\n",
        "- `data/text/combined_clean.txt` – Cleaned combined text file (ready for tokenization)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96f4c01a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import re, unicodedata\n",
        "from collections import Counter\n",
        "from typing import List\n",
        "\n",
        "# This function removes headers/footers from the text.\n",
        "def remove_header_footer_lines(text: str) -> str: \n",
        "    lines = text.splitlines()\n",
        "    cnt = Counter(lines)\n",
        "\n",
        "    # Lines that look like page numbers: \"12\", \"Page 3\", \"3 / 20\", \"p. 7\" will be removed.\n",
        "    def is_page_num(line: str) -> bool:\n",
        "        line_stripped = line.strip().lower()\n",
        "        if re.fullmatch(r\"\\d{1,4}\", line_stripped):\n",
        "            return True\n",
        "        if re.fullmatch(r\"(page|p\\.?)\\s*\\d{1,4}(\\s*(of|/)\\s*\\d{1,4})?\", line_stripped):\n",
        "            return True\n",
        "        if re.fullmatch(r\"\\d{1,4}\\s*/\\s*\\d{1,4}\", line_stripped):\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    # Lines that look like running headers: short, often repeated, not sentence-like, will be removed.\n",
        "    header_like = {\n",
        "        l for l, c in cnt.items()\n",
        "        if c >= 3                                  # appears many times\n",
        "        and 3 <= len(l.strip()) <= 80              # typically short\n",
        "        and not re.search(r\"[.!?]$\", l.strip())    # not ending like a sentence\n",
        "        and re.search(r\"[A-Za-z]\", l)              # ensure it has at least one letter\n",
        "    }\n",
        "\n",
        "    kept = []\n",
        "    for l in lines:\n",
        "        if is_page_num(l):          # drop page numbers\n",
        "            continue\n",
        "        if l in header_like:        # drop frequent headers/footers\n",
        "            continue\n",
        "        kept.append(l)\n",
        "    return \"\\n\".join(kept)\n",
        "\n",
        "# PDF line-wrapping often splits words with \"-\\n\". Merge only when next token is lowercase.\n",
        "# e.g., \"infor-\\nmation\" -> \"information\", but keep genuine hyphens like \"state-of-the-art\".\n",
        "def fix_hyphenation(text: str) -> str:\n",
        "    text = re.sub(r\"(?<=\\w)-\\n(?=[a-z])\", \"\", text)\n",
        "    return text\n",
        "\n",
        "# Normalize diacritics & ligatures (ﬁ, ﬂ, …) so tokens are consistent across PDFs.\n",
        "def normalize_unicode(s: str) -> str: \n",
        "    s = unicodedata.normalize(\"NFKC\", s)\n",
        "    s = s.replace(\"ﬁ\", \"fi\").replace(\"ﬂ\", \"fl\")\n",
        "    return s\n",
        "\n",
        "# Preserve paragraphs by templating them, then flatten remaining newlines to spaces.\n",
        "def unwrap_soft_linebreaks(text: str) -> str:\n",
        "    text = text.replace(\"\\r\\n\", \"\\n\")\n",
        "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)            # collapse 3+ newlines -> 2\n",
        "    text = text.replace(\"\\n\\n\", \"<PARA_BREAK>\")       # protect paragraph breaks\n",
        "    text = re.sub(r\"\\n+\", \" \", text)                  # single \\n -> space\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)               # extra spaces -> single\n",
        "    text = text.replace(\"<PARA_BREAK>\", \"\\n\\n\")       # restore paragraphs\n",
        "    return text\n",
        "\n",
        "# This function removes figure and table captions from the text.\n",
        "# Remove blocks that start with Figure/Fig./Table/Algorithm + number.\n",
        "# This targets captions that start at the beginning of a line, so in-sentence mentions like “as shown in Figure 1.1” are preserved.\n",
        "def drop_figure_table_captions(text: str) -> str:\n",
        "    header_pat = r\"(?:extended\\s+data\\s+figure|supplementary\\s+figure|figure|fig\\.|table|algorithm)\"\n",
        "    # ID can be: 2, 2.3, 2.3a, S1, or appendix style like C.1, C.1.2a\n",
        "    id_pat = r\"(?:s?\\d+(?:\\.\\d+)*[a-z]?|[A-Za-z]+(?:\\.\\d+)+[a-z]?)\"\n",
        "\n",
        "    block_pat = re.compile(\n",
        "        rf\"\"\"(?ims)                      # DOTALL + IGNORECASE + MULTILINE\n",
        "        ^\\s*{header_pat}\\s+{id_pat}      # e.g., Figure 1.1 / Table C.1\n",
        "        \\s*(?:[:.\\-–—]\\s*)?               # optional punctuation\n",
        "        .*?                               # caption text (greedy, but bounded by lookahead)\n",
        "        (?=\\n\\s*\\n                        # stop at blank line\n",
        "           |^\\s*{header_pat}\\s+           # or next caption header\n",
        "           |\\Z)                            # or end of text\n",
        "        \"\"\",\n",
        "        re.VERBOSE,\n",
        "    )\n",
        "\n",
        "    single_pat = re.compile(\n",
        "        rf\"\"\"(?im)\n",
        "        ^\\s*{header_pat}\\s+{id_pat}\\s*[:.\\-–—]\\s+.*$\n",
        "        \"\"\",\n",
        "        re.VERBOSE,\n",
        "    )\n",
        "\n",
        "    text = block_pat.sub(\"\\n\", text)\n",
        "    text = single_pat.sub(\"\", text)\n",
        "    return text\n",
        "\n",
        "# This function trims reference sections at the end of each PDF text.\n",
        "# Find a terminal heading like \"References\" / \"Bibliography\" near the end and cut everything after it.\n",
        "def trim_references_at_end(text: str) -> str:\n",
        "    refs = list(re.finditer(r\"(?mi)^\\s*(references|bibliography)\\s*$\", text))\n",
        "    if refs:\n",
        "        cut = refs[-1].start()\n",
        "        # Keep a small lead-in to avoid chopping mid-paragraph if false positive.\n",
        "        return text[:cut].rstrip()\n",
        "    return text\n",
        "\n",
        "# This function drops author blocks but preserves the title.\n",
        "def drop_author_block_preserve_title(text: str) -> str:\n",
        "    lines = text.lstrip().splitlines()\n",
        "    if not lines:\n",
        "        return text\n",
        "\n",
        "    # ---- keep the first non-empty line as the title ----\n",
        "    i = 0\n",
        "    while i < len(lines) and not lines[i].strip():\n",
        "        i += 1\n",
        "    if i >= len(lines):\n",
        "        return text\n",
        "    title = lines[i].strip()\n",
        "    i += 1  # start scanning after title\n",
        "\n",
        "    # ---- stopping points where body typically begins ----\n",
        "    marker_re = re.compile(\n",
        "        r'^(abstract|summary|introduction|keywords|key\\s*words|1[\\.\\)]?\\s|i[\\.\\)]?\\s|contents)\\b',\n",
        "        re.I,\n",
        "    )\n",
        "\n",
        "    # ---- heuristics for author/affiliation lines ----\n",
        "    footnote_syms = set(\"*†‡§#^\")\n",
        "    affil_kw = re.compile(\n",
        "        r'(university|institute|laborator|school|department|dept\\.|research|computer|science|engineering|ai|ml|cs|'\n",
        "        r'google|deepmind|meta|openai|microsoft|alibaba|nvidia|huawei|zhipu|epfl|stanford|mit|oxford|berkeley)',\n",
        "        re.I,\n",
        "    )\n",
        "\n",
        "    def looks_like_author_line(s: str) -> bool:\n",
        "        s = s.strip()\n",
        "        if not s:\n",
        "            return True  # blank lines between author lines\n",
        "        if '@' in s:\n",
        "            return True\n",
        "        if any(sym in s for sym in footnote_syms):\n",
        "            return True\n",
        "        if affil_kw.search(s):\n",
        "            return True\n",
        "        # short-ish and mostly Proper-Name tokens\n",
        "        if len(s) <= 80:\n",
        "            tokens = re.findall(r\"[A-Za-z\\.'\\-]+\", s)\n",
        "            if not tokens:\n",
        "                return True\n",
        "            caps = sum(1 for t in tokens if t[:1].isupper())\n",
        "            lower = sum(1 for t in tokens if t[:1].islower())\n",
        "            if caps >= max(2, int(0.6 * len(tokens))) and lower <= 3:\n",
        "                return True\n",
        "        # author lines like \"Tom B. Brown, Benjamin Mann, …\"\n",
        "        if re.match(r\"^[A-Z][A-Za-z\\.'\\-]+(?:\\s+[A-Z][A-Za-z\\.'\\-]+)*(?:\\s*,\\s*[A-Z][A-Za-z\\.'\\-]+.*)+$\", s):\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    # ---- skip author/affiliation block ----\n",
        "    j = i\n",
        "    while j < len(lines):\n",
        "        s = lines[j].strip()\n",
        "        if marker_re.match(s):\n",
        "            break\n",
        "        if looks_like_author_line(s):\n",
        "            j += 1\n",
        "            continue\n",
        "        # long prose -> likely body\n",
        "        if len(s) > 100 and sum(ch.islower() for ch in s) > 10:\n",
        "            break\n",
        "        # conservative stop\n",
        "        break\n",
        "\n",
        "    out = [title, \"\"] + lines[j:]\n",
        "    return \"\\n\".join(out)\n",
        "\n",
        "# This function removes table of contents, dotted leaders, and standalone junk lines.\n",
        "def drop_toc_and_garbage(text: str) -> str:\n",
        "    # First, strip out any block between 'Contents' and 'Introduction' (case-insensitive)\n",
        "    text = re.sub(\n",
        "        r'(?is)^\\s*contents\\s*.*?(?=^\\s*introduction\\s*$)',\n",
        "        '', text, flags=re.MULTILINE | re.DOTALL\n",
        "    )\n",
        "\n",
        "    # Safer TOC + junk removal (only early in the doc; keep normal prose)\n",
        "    lines = text.splitlines()\n",
        "    cleaned = []\n",
        "    in_toc = False\n",
        "    seen_nonempty = 0\n",
        "\n",
        "    # TOC patterns: dotted leaders **with a page number** at the end.\n",
        "    dotted_leader = re.compile(r'^[ \\t]*(?:\\d+(?:\\.\\d+)*)?[ \\t]*.+?\\.{3,}[ \\t]*\\d{1,4}[ \\t]*$')\n",
        "    # Junk line like: \"I u J\" (single letters separated by spaces)\n",
        "    spaced_single_letters = re.compile(r'^[A-Za-z](?:\\s+[A-Za-z]){1,6}$')\n",
        "    # Punctuation-only line\n",
        "    punct_only = re.compile(r'^[\\W_]+$')\n",
        "\n",
        "    for i, l in enumerate(lines):\n",
        "        s = l.strip()\n",
        "\n",
        "        # Count non-empty to detect the very beginning of the doc\n",
        "        if s:\n",
        "            seen_nonempty += 1\n",
        "\n",
        "        # Enter TOC only very early (first ~80 non-empty lines) if header is present\n",
        "        if not in_toc and seen_nonempty <= 80 and re.match(r'(?i)^(contents|table of contents)\\s*$', s):\n",
        "            in_toc = True\n",
        "            continue\n",
        "\n",
        "        if in_toc:\n",
        "            # Stay in TOC while lines look like TOC entries (dotted leaders with page nums)\n",
        "            if dotted_leader.match(s):\n",
        "                continue\n",
        "            # Exit TOC on the first blank or clearly non-TOC line\n",
        "            if not s or ('.' not in s and not dotted_leader.match(s)):\n",
        "                in_toc = False\n",
        "            if in_toc:\n",
        "                continue  # still skipping TOC lines\n",
        "\n",
        "        # Gentle junk filters:\n",
        "        if not s:\n",
        "            cleaned.append(l)\n",
        "            continue\n",
        "        if punct_only.match(s):\n",
        "            continue\n",
        "        if spaced_single_letters.match(s):\n",
        "            continue\n",
        "        # If the line has at least one word with >=3 letters, it's likely real text -> keep.\n",
        "        if re.search(r'\\b[A-Za-z]{3,}\\b', s):\n",
        "            cleaned.append(l)\n",
        "            continue\n",
        "        # Otherwise, keep by default (be conservative).\n",
        "        cleaned.append(l)\n",
        "\n",
        "    return \"\\n\".join(cleaned)\n",
        "\n",
        "# This function drops orphan numeric lines that are likely table/figure artifacts.\n",
        "def drop_orphan_numeric_lines(text: str) -> str:\n",
        "    ALPHA3_WORD_RE = re.compile(r'\\b[A-Za-z]{3,}\\b')                 # any normal word (>=3 letters)\n",
        "    HAS_DIGIT_RE   = re.compile(r'\\d')                               # contains a digit\n",
        "    ONLY_NUMERIC_CHARS_RE = re.compile(r'^[\\d\\.\\s,:%/()+-]+$')       # numbers & punctuation only\n",
        "    FRACTION_RE    = re.compile(r'^\\s*\\d+\\s*/\\s*\\d+\\s*$')            # like \"87 / 150\"\n",
        "    PERCENT_LINE_RE= re.compile(r'^\\s*\\d+(?:\\.\\d+)?\\s*%$')           # like \"75 %\"\n",
        "    SHORT_LABEL_RE = re.compile(r'^(?:\\d{1,2}[A-Za-z]{1,3}[+\\-]?|[A-Za-z]{1,3}\\d{1,2}[+\\-]?)$')\n",
        "    out_lines = []\n",
        "    for l in text.splitlines():\n",
        "        s = l.strip()\n",
        "        if not s:\n",
        "            out_lines.append(l); continue\n",
        "\n",
        "        # Keep immediately if there's a normal word (>=3 letters) or line is long\n",
        "        if ALPHA3_WORD_RE.search(s) or len(s) > 40 or not HAS_DIGIT_RE.search(s):\n",
        "            out_lines.append(l); continue\n",
        "\n",
        "        alpha_cnt = sum(c.isalpha() for c in s)\n",
        "        digit_cnt = sum(c.isdigit() for c in s)\n",
        "\n",
        "        # Candidate orphan numeric line: digits present, very few letters\n",
        "        if digit_cnt and alpha_cnt <= 2:\n",
        "            # Drop common table/figure leftovers\n",
        "            if (ONLY_NUMERIC_CHARS_RE.fullmatch(s)\n",
        "                or FRACTION_RE.fullmatch(s)\n",
        "                or PERCENT_LINE_RE.fullmatch(s)\n",
        "                or SHORT_LABEL_RE.fullmatch(s)):\n",
        "                continue  # drop\n",
        "        out_lines.append(l)\n",
        "    return \"\\n\".join(out_lines)\n",
        "\n",
        "# This function drops artifact lines that are likely non-content (short, non-prose artifacts).\n",
        "def drop_artifact_lines(text: str) -> str:\n",
        "    def is_artifact(s: str) -> bool:\n",
        "        s0 = s.strip()\n",
        "        if not s0:\n",
        "            return False\n",
        "\n",
        "        # Spaced letters like: \"A V a L L\"\n",
        "        if re.fullmatch(r'(?:[A-Za-z]\\s+){2,}[A-Za-z]', s0):\n",
        "            return True\n",
        "\n",
        "        alnum = re.sub(r'[^A-Za-z0-9]', '', s0)\n",
        "        if not alnum:\n",
        "            return False\n",
        "\n",
        "        if len(alnum) <= 12:\n",
        "            upper = sum(c.isupper() for c in alnum)\n",
        "            digits = sum(c.isdigit() for c in alnum)\n",
        "            vowels = sum(c in 'AEIOUaeiou' for c in alnum)\n",
        "            flips = sum(\n",
        "                (alnum[i].islower() and alnum[i-1].isupper()) or\n",
        "                (alnum[i].isupper() and alnum[i-1].islower())\n",
        "                for i in range(1, len(alnum))\n",
        "            )\n",
        "\n",
        "            upper_ratio = upper / len(alnum)\n",
        "            digit_ratio = digits / len(alnum)\n",
        "            vowel_ratio = vowels / len(alnum)\n",
        "            flip_ratio = flips / max(1, (len(alnum) - 1))\n",
        "\n",
        "            # All-caps heavy, digit-heavy, vowel-poor, or high case-flip noise\n",
        "            if upper_ratio > 0.7 or digit_ratio > 0.5 or vowel_ratio < 0.2 or flip_ratio >= 0.5:\n",
        "                return True\n",
        "\n",
        "            # Camel-case blips with few vowels (e.g., 'vviXra')\n",
        "            if re.fullmatch(r'[a-z]+[A-Z][a-z]+', alnum) and vowel_ratio < 0.35 and len(alnum) <= 8:\n",
        "                return True\n",
        "\n",
        "            # Short ALL-CAPS with ≤1 vowel (e.g., 'FGM', 'OTPG')\n",
        "            if re.fullmatch(r'[A-Z0-9]{3,}', alnum) and vowels <= 1:\n",
        "                return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    return \"\\n\".join(l for l in text.splitlines() if not is_artifact(l))\n",
        "\n",
        "# This function removes empty lines from the text.\n",
        "def remove_empty_lines(text: str) -> str:\n",
        "    return \"\\n\".join(line for line in text.splitlines() if line.strip())\n",
        "\n",
        "# This function removes Chinese and Korean characters from the text.\n",
        "def remove_cjk(text: str) -> str:\n",
        "    return re.sub(r'[\\u4e00-\\u9fff\\uac00-\\ud7af\\u1100-\\u11ff\\u3130-\\u318f\\u3000-\\u303F\\uFF00-\\uFFEF]+', '', text)\n",
        "\n",
        "# This function removes http/https URLs from the text.\n",
        "def remove_links(text: str) -> str:\n",
        "    return re.sub(r'http[s]?://\\S+', '', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "2c935dba",
      "metadata": {},
      "outputs": [],
      "source": [
        "# This function applies all cleaning steps to a single document's text.\n",
        "def clean_one_doc(doc_text: str) -> str:\n",
        "    t = doc_text\n",
        "    t = normalize_unicode(t)                 # normalize first\n",
        "    t = drop_toc_and_garbage(t)              # drop TOC & junk lines\n",
        "    t = remove_header_footer_lines(t)        # drop repeating noise & page numbers\n",
        "    t = fix_hyphenation(t)                   # merge \"word-\\nwrap\"\n",
        "    t = drop_figure_table_captions(t)        # remove caption blocks\n",
        "    t = remove_cjk(t)                        # strip Chinese and Korean characters\n",
        "    t = remove_links(t)                      # strip links\n",
        "    t = drop_orphan_numeric_lines(t)         # drop orphan numeric lines that are likely table/figure artifacts\n",
        "    t = trim_references_at_end(t)            # cut terminal references section\n",
        "    t = unwrap_soft_linebreaks(t)            # unwrap paragraph-internal line breaks\n",
        "    t = drop_artifact_lines(t)               # drop other artifact lines\n",
        "    t = drop_author_block_preserve_title(t)  # drop author block but keep title\n",
        "    t = remove_empty_lines(t)                # drop empty/whitespace-only lines\n",
        "\n",
        "    # final tidy\n",
        "    t = re.sub(r\"[ \\t]+\\n\", \"\\n\", t)         # strip trailing spaces at line ends\n",
        "    t = re.sub(r\"\\n{3,}\", \"\\n\\n\", t)         # normalize blank lines\n",
        "    return t.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "c8714f96",
      "metadata": {},
      "outputs": [],
      "source": [
        "# This function splits the combined raw text into individual documents.\n",
        "def split_into_docs_within_combined(raw: str) -> List[str]:\n",
        "    parts = re.split(r\"(?m)^===== DOCUMENT START =====\\s*$\", raw)\n",
        "    docs = []\n",
        "    for p in parts:\n",
        "        if not p.strip():\n",
        "            continue\n",
        "        # each part ends before the corresponding END marker; keep within it\n",
        "        end_split = re.split(r\"(?m)^===== DOCUMENT END =====\\s*$\", p, maxsplit=1)\n",
        "        doc_chunk = end_split[0]\n",
        "        docs.append(doc_chunk.strip())\n",
        "    return docs\n",
        "\n",
        "# This function removes the STEP 1 scaffolding lines to keep only paper body.\n",
        "# Drop the START/END markers and TITLE/URL/FILE metadata lines.\n",
        "def strip_boundaries_and_metadata(block: str) -> str:\n",
        "    block = re.sub(r\"^===== DOCUMENT START =====\\s*$\", \"\", block, flags=re.M)\n",
        "    block = re.sub(r\"^===== DOCUMENT END =====\\s*$\", \"\", block, flags=re.M)\n",
        "    block = re.sub(r\"^(TITLE|URL|FILE):.*\\n?\", \"\", block, flags=re.M)\n",
        "    return block.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6934d76",
      "metadata": {},
      "source": [
        "Here, `data/text/combined_clean.txt` is created, which contains cleaned combined text file (ready for tokenization)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "daf57b14",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved cleaned txt file with 37 documents.\n"
          ]
        }
      ],
      "source": [
        "COMBINED_PATH = \"data/text/combined_raw.txt\"\n",
        "\n",
        "with open(COMBINED_PATH, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "    combined_raw = f.read()\n",
        "\n",
        "# Remove scaffolding & metadata, then clean each paper independently\n",
        "docs_raw = split_into_docs_within_combined(combined_raw)\n",
        "docs_raw = [strip_boundaries_and_metadata(d) for d in docs_raw]\n",
        "docs_cleaned = [clean_one_doc(d) for d in docs_raw]\n",
        "\n",
        "# Concatenate back to a single cleaned corpus (no file is written here)\n",
        "cleaned_corpus = (\"\\n\\n\" + (\"=\" * 80) + \"\\n\\n\").join(docs_cleaned)\n",
        "\n",
        "# Save to file\n",
        "CLEAN_PATH = pathlib.Path(\"data/text/combined_clean.txt\")\n",
        "\n",
        "CLEAN_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "with open(CLEAN_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(cleaned_corpus)\n",
        "\n",
        "print(f\"Saved cleaned txt file with {len(docs_cleaned)} documents.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17e5f04a",
      "metadata": {},
      "source": [
        "## Step 3 — Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5f68e10",
      "metadata": {},
      "source": [
        "### In this step, we explore and compare two different tokenization methods:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f7b8b31",
      "metadata": {},
      "source": [
        "### (A) Regex-based Tokenizer\n",
        "- Implemented by splitting text into **words, punctuation, and whitespace**.\n",
        "- Vocabulary is built directly from the tokens.\n",
        "- Includes special tokens: `<|endoftext|>` and `<|unk|>`.\n",
        "- Implemented as a simple class `SimpleTokenizerV2` with `encode` and `decode`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b7e099c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total regex tokens: 441487\n",
            "Regex vocabulary size: 26936\n",
            "Total regex IDs tokens: 441487\n"
          ]
        }
      ],
      "source": [
        "CLEAN_PATH = pathlib.Path(\"data/text/combined_clean.txt\")\n",
        "clean_text = CLEAN_PATH.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "# ====== Regex-based tokenizer ======\n",
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', clean_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(\"Total regex tokens:\", len(preprocessed))\n",
        "\n",
        "# Build vocabulary\n",
        "vocab = sorted(list(set(preprocessed)))\n",
        "vocab.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "vocab_size = len(vocab)\n",
        "print(\"Regex vocabulary size:\", vocab_size)\n",
        "\n",
        "class SimpleTokenizerV2:\n",
        "    def __init__(self, vocab):\n",
        "        self.tokens2ids = {token: id for id, token in enumerate(vocab)}\n",
        "        self.ids2tokens = {id: token for token, id in self.tokens2ids.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        tokens = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "        tokens = [item.strip() for item in tokens if item.strip()]\n",
        "        tokens = [t if t in self.tokens2ids else \"<|unk|>\" for t in tokens]\n",
        "        return [self.tokens2ids[t] for t in tokens]\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.ids2tokens[i] for i in ids])\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text\n",
        "\n",
        "# Initialize regex tokenizer\n",
        "tokenizer_v2 = SimpleTokenizerV2(vocab)\n",
        "\n",
        "# Encode the full text\n",
        "regex_ids = tokenizer_v2.encode(clean_text)\n",
        "print(\"Total regex IDs tokens:\", len(regex_ids))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1cc8499",
      "metadata": {},
      "source": [
        "### (B) Byte Pair Encoding (BPE) Tokenizer\n",
        "- Implemented using the pretrained **GPT-2** vocabulary.\n",
        "- Rare or long words are broken into multiple subword units."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f961b393",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BPE vocabulary size: 50257\n",
            "Total BPE tokens: 564244\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "# ====== BPE Tokenizer ======\n",
        "tokenizer_gpt2 = tiktoken.get_encoding(\"gpt2\")  # GPT-2 vocab\n",
        "bpe_vocab_size = tokenizer_gpt2.n_vocab\n",
        "print(\"BPE vocabulary size:\", bpe_vocab_size)\n",
        "\n",
        "# Encode the full text\n",
        "bpe_ids = tokenizer_gpt2.encode(clean_text, allowed_special={\"<|endoftext|>\"})\n",
        "print(\"Total BPE tokens:\", len(bpe_ids))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9199c32",
      "metadata": {},
      "source": [
        "### (C) Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c3dd1c4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Regex vocabulary size: 26936\n",
            "BPE vocabulary size: 50257\n",
            "Vocabulary count difference (BPE - regex): 23321\n",
            "Total regex tokens: 441487\n",
            "Total BPE tokens: 564244\n",
            "Token count difference (BPE - regex): 122757\n",
            "\n",
            "Unknown word test:\n",
            "Regex IDs: [3458, 25743, 26935, 147]\n",
            "Regex decoded: Alice visited <|unk|>.\n",
            "BPE IDs: [44484, 8672, 2208, 86, 8623, 1044, 13]\n",
            "BPE decoded: Alice visited superwonderland.\n",
            "BPE ID: 44484 \tToken: Alice\n",
            "BPE ID: 8672 \tToken:  visited\n",
            "BPE ID: 2208 \tToken:  super\n",
            "BPE ID: 86 \tToken: w\n",
            "BPE ID: 8623 \tToken: onder\n",
            "BPE ID: 1044 \tToken: land\n",
            "BPE ID: 13 \tToken: .\n"
          ]
        }
      ],
      "source": [
        "print(\"Regex vocabulary size:\", vocab_size)\n",
        "print(\"BPE vocabulary size:\", bpe_vocab_size)\n",
        "print(\"Vocabulary count difference (BPE - regex):\",  bpe_vocab_size - vocab_size)\n",
        "\n",
        "print(\"Total regex tokens:\", len(regex_ids))\n",
        "print(\"Total BPE tokens:\", len(bpe_ids))\n",
        "print(\"Token count difference (BPE - regex):\", len(bpe_ids) - len(regex_ids))\n",
        "\n",
        "# Test with an unknown word.\n",
        "test_sentence = \"Alice visited superwonderland.\"\n",
        "\n",
        "print(\"\\nUnknown word test:\")\n",
        "print(\"Regex IDs:\", tokenizer_v2.encode(test_sentence))\n",
        "print(\"Regex decoded:\", tokenizer_v2.decode(tokenizer_v2.encode(test_sentence)))\n",
        "\n",
        "print(\"BPE IDs:\", tokenizer_gpt2.encode(test_sentence, allowed_special={\"<|endoftext|>\"}))\n",
        "print(\"BPE decoded:\", tokenizer_gpt2.decode(tokenizer_gpt2.encode(test_sentence, allowed_special={\"<|endoftext|>\"})))\n",
        "\n",
        "# Split wording test\n",
        "for i in tokenizer_gpt2.encode(test_sentence, allowed_special={\"<|endoftext|>\"}):\n",
        "    print(\"BPE ID:\", i, \"\\tToken:\", tokenizer_gpt2.decode([i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "761e9d07",
      "metadata": {},
      "source": [
        "- **Regex Tokenizer**  \n",
        "  - Simpler and preserves whole words.  \n",
        "  - text-dependent vocabulary → higher OOV rate when capped.  \n",
        "  - Assigns `<|unk|>` for unseen words.\n",
        "\n",
        "- **BPE Tokenizer**  \n",
        "  - Fixed vocabulary (pretrained, reusable across text).  \n",
        "  - Handles unknown words by splitting them into smaller pieces.  \n",
        "  - Produces more tokens for rare/complex words but avoids `<|unk|>`.  \n",
        "  \n",
        "  ### Insights:\n",
        "- Regex is straightforward and often yields fewer tokens for frequent words.  \n",
        "- BPE is more **robust** and **generalizable**, ensuring full coverage. \n",
        "- For training modern LLMs, **BPE is preferred** due to its balance of vocabulary size, coverage, and efficiency."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec6b5d28",
      "metadata": {},
      "source": [
        "## Step 4 — Dataset & DataLoader (Sliding Window)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "012f0bb2",
      "metadata": {},
      "source": [
        "\n",
        "We will:\n",
        "1) **Tokenize** the cleaned corpus with GPT-2 vocab.  \n",
        "2) **Window** the token stream into `(input, target)` pairs using a sliding window:\n",
        "   - `input = ids[i : i+context_size]`\n",
        "   - `target = ids[i+1 : i+context_size+1]`\n",
        "3) **Split** into train/test (90/10 on the token stream).  \n",
        "4) Wrap with **PyTorch `Dataset`** and **`DataLoader`** for batching.\n",
        "\n",
        "### Setup & load cleaned text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55a19701",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total tokens in cleaned text: 564244\n"
          ]
        }
      ],
      "source": [
        "import pathlib, torch\n",
        "\n",
        "CLEAN_PATH = pathlib.Path(\"data/text/combined_clean.txt\")\n",
        "clean_text = CLEAN_PATH.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "# Encode entire text\n",
        "enc_text = tokenizer.encode(clean_text, allowed_special={\"<|endoftext|>\"})\n",
        "print(\"Total tokens in cleaned text:\", len(enc_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "519abcdf",
      "metadata": {},
      "source": [
        "### A) Here we prepare a dataset of input–target sequences (using a sliding window approach).\n",
        "### Sliding-window dataset\n",
        "\n",
        "- We use `GPTDatasetV1` structure.  \n",
        "- We will allow passing **pre-encoded token IDs** (so we can split train/test on the token stream precisely).  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e2d35e5b",
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, data, tokenizer, context_size, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Accept raw text or already-encoded ids\n",
        "        if isinstance(data, str):\n",
        "            token_ids = tokenizer.encode(data, allowed_special={\"<|endoftext|>\"})\n",
        "        else:\n",
        "            token_ids = list(data)  # assume iterable of ints\n",
        "\n",
        "        assert len(token_ids) > context_size, \\\n",
        "            \"Token sequence must be longer than context_size+1.\"\n",
        "\n",
        "        # Sliding-window: step by `stride`\n",
        "        for i in range(0, len(token_ids) - context_size, stride):\n",
        "            x = token_ids[i : i + context_size]\n",
        "            y = token_ids[i + 1 : i + context_size + 1]\n",
        "            self.input_ids.append(torch.tensor(x, dtype=torch.long))\n",
        "            self.target_ids.append(torch.tensor(y, dtype=torch.long))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2adb94c5",
      "metadata": {},
      "source": [
        "### B) Here we implement a PyTorch DataLoader to batch the dataset for training.\n",
        "### Train/Test split on the token stream\n",
        "\n",
        "We split the encoded IDs (90/10) before windowing.  \n",
        "This ensures no leakage and preserves chronological order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "57d0f671",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train batches: 991 | Test batches: 110\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "context_size = 128  # Controls how many previous tokens the model can “see.”\n",
        "stride = 64         # Controls overlap between adjacent windows (smaller stride → more overlapping examples).\n",
        "batch_size = 8\n",
        "\n",
        "# Split 90/10 on the encoded token stream\n",
        "split_at = int(0.9 * len(enc_text))\n",
        "train_ids = enc_text[:split_at]\n",
        "test_ids  = enc_text[split_at:]\n",
        "\n",
        "# Build datasets (pass pre-encoded ids to reuse tokenizer once)\n",
        "train_ds = GPTDatasetV1(train_ids, tokenizer, context_size, stride)\n",
        "test_ds  = GPTDatasetV1(test_ids,  tokenizer, context_size, stride)\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_ds,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,      # better generalization\n",
        "    drop_last=True,\n",
        "    num_workers=0\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_ds,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,     # reproducible order\n",
        "    drop_last=False,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "print(f\"Train batches: {len(train_loader)} | Test batches: {len(test_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eeb2c3e6",
      "metadata": {},
      "source": [
        "**Check:** We want to reuse `iter(...)` and `next(...)` pattern to inspect `(inputs, targets)` pairs.  \n",
        "Each target is the next token of the corresponding input sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "81438e7b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch shapes -> inputs: torch.Size([8, 128]) targets: torch.Size([8, 128])\n",
            "\n",
            "First input IDs:\n",
            " tensor([ 8824, 18250, 12513,   198, 21947, 15168, 14435,  2700,  1377,   554,\n",
            "          257,  2829,  1339,   884,   355,   281,  2134, 19186,  2402,   257,\n",
            "         3084,    11,   262,  3487,  2700,   319,   262,  2134,   318,  4961,\n",
            "          475,   287,  6697,  4571,   284,   262, 29973,  2700,  5625,   319,\n",
            "          262,  2134,   357,   273,   262,  3463,   286,   262,  2134,   828,\n",
            "          326,   318,    11,   399,   796,   285,   308,   357,    59, 13812,\n",
            "         7635,   399,    28, 11296,   828,   810,   285,   318,  2347,    11,\n",
            "          290,   308,   318,   262, 29973,  2214,  4202,   357, 10755,   860,\n",
            "           13,  6659,   285,    14,    82,   319,  3668,   737,   994,  6870,\n",
            "          262,  2700,  5625,   416,   262,  3084,  1028,   262,  2134,   326,\n",
            "        15174,   340,   422, 27141,   832,   262,  3084,   290,  4433,   326,\n",
            "          262,  3084,   318,  2102,    11,   340, 28055,  1576,   284,  5203,\n",
            "          428,  3487,  2700,  1231,  7163,    13,   318,  2562])\n",
            "\n",
            "First target IDs:\n",
            " tensor([18250, 12513,   198, 21947, 15168, 14435,  2700,  1377,   554,   257,\n",
            "         2829,  1339,   884,   355,   281,  2134, 19186,  2402,   257,  3084,\n",
            "           11,   262,  3487,  2700,   319,   262,  2134,   318,  4961,   475,\n",
            "          287,  6697,  4571,   284,   262, 29973,  2700,  5625,   319,   262,\n",
            "         2134,   357,   273,   262,  3463,   286,   262,  2134,   828,   326,\n",
            "          318,    11,   399,   796,   285,   308,   357,    59, 13812,  7635,\n",
            "          399,    28, 11296,   828,   810,   285,   318,  2347,    11,   290,\n",
            "          308,   318,   262, 29973,  2214,  4202,   357, 10755,   860,    13,\n",
            "         6659,   285,    14,    82,   319,  3668,   737,   994,  6870,   262,\n",
            "         2700,  5625,   416,   262,  3084,  1028,   262,  2134,   326, 15174,\n",
            "          340,   422, 27141,   832,   262,  3084,   290,  4433,   326,   262,\n",
            "         3084,   318,  2102,    11,   340, 28055,  1576,   284,  5203,   428,\n",
            "         3487,  2700,  1231,  7163,    13,   318,  2562,   284])\n"
          ]
        }
      ],
      "source": [
        "data_iter = iter(train_loader)\n",
        "x, y = next(data_iter)\n",
        "print(\"Batch shapes -> inputs:\", x.shape, \"targets:\", y.shape)  # [B, T] each\n",
        "\n",
        "# Show first sequence pair\n",
        "print(\"\\nFirst input IDs:\\n\", x[0])\n",
        "print(\"\\nFirst target IDs:\\n\", y[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d8556c0",
      "metadata": {},
      "source": [
        "### Token & Position Embeddings.\n",
        "\n",
        "Here we embed tokens (`nn.Embedding(vocab_size, d_model)`),  \n",
        "and add position embeddings (`nn.Embedding(context_size, d_model)`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "bfcfb191",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input embeddings shape: torch.Size([8, 128, 256])\n"
          ]
        }
      ],
      "source": [
        "vocab_size = 50257          # GPT-2 vocab size\n",
        "d_model = 256               # embedding dimension (example)\n",
        "\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, d_model)\n",
        "pos_embedding_layer   = torch.nn.Embedding(context_size, d_model)\n",
        "\n",
        "# Get one batch\n",
        "data_iter = iter(train_loader)\n",
        "inputs, targets = next(data_iter)   # [B, T]\n",
        "\n",
        "# Token embeddings: [B, T, d_model]\n",
        "token_embeddings = token_embedding_layer(inputs)\n",
        "\n",
        "# Position indices [0..T-1], shape [T], then broadcast to [B, T]\n",
        "positions = torch.arange(inputs.size(1), dtype=torch.long)\n",
        "pos_embeddings = pos_embedding_layer(positions)       # [T, d_model]\n",
        "pos_embeddings = pos_embeddings.unsqueeze(0)          # [1, T, d_model]\n",
        "\n",
        "# Sum token + position embeddings -> final input embeddings\n",
        "input_embeddings = token_embeddings + pos_embeddings   # [B, T, d_model]\n",
        "print(\"Input embeddings shape:\", input_embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "960fe348",
      "metadata": {},
      "source": [
        "## Step 5 —"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "138cd51e",
      "metadata": {},
      "source": [
        "\n",
        "### A) Documents counts\n",
        "We’ll load the **raw combined** file and the **cleaned** file.  \n",
        "Doc counts are computed as:\n",
        "- **Raw**: number of `===== DOCUMENT START =====` markers.  \n",
        "- **Cleaned**: split on the `================================================================================` divider we used when concatenating docs (fallback to 1 if not found)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "14c27499",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Documents — raw: 37 | clean: 37\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "RAW_PATH = Path(\"data/text/combined_raw.txt\")\n",
        "CLEAN_PATH = Path(\"data/text/combined_clean.txt\")\n",
        "\n",
        "raw_text = RAW_PATH.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "clean_text = CLEAN_PATH.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "# Doc count in RAW (based on markers from Step 1)\n",
        "raw_docs = re.split(r\"(?m)^===== DOCUMENT START =====\\s*$\", raw_text)\n",
        "raw_docs = [d for d in raw_docs if d.strip()]\n",
        "n_docs_raw = len(raw_docs)\n",
        "\n",
        "# Doc count in CLEAN (based on 80 '=' chars divider used in Step 2)\n",
        "clean_docs = clean_text.split(\"=\" * 80)  # 80 '=' chars\n",
        "clean_docs = [d.strip() for d in clean_docs if d.strip()]\n",
        "n_docs_clean = max(1, len(clean_docs))   # fallback to 1 if no divider found\n",
        "\n",
        "print(f\"Documents — raw: {n_docs_raw} | clean: {n_docs_clean}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66914306",
      "metadata": {},
      "source": [
        "### A) Total tokens & average tokens per doc (BPE & Regex)\n",
        "\n",
        "We compute:\n",
        "- Total tokens (sum over all documents) - For raw and clean versions.\n",
        "- Average tokens per document - For raw and clean versions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbc6502c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "— BPE —\n",
            "Vocabulary size:        50257\n",
            "Total tokens (raw):     1500503\n",
            "Total tokens (clean):   564244\n",
            "Avg tokens/doc (raw):   40554\n",
            "Avg tokens/doc (clean): 15250\n",
            "\n",
            "— Regex —\n",
            "Vocabulary size:        26936\n",
            "Total tokens (raw):     978953\n",
            "Total tokens (clean):   441487\n",
            "Avg tokens/doc (raw):   26458\n",
            "Avg tokens/doc (clean): 11932\n"
          ]
        }
      ],
      "source": [
        "# Tokenize with existing tokenizers\n",
        "raw_bpe_ids   = tokenizer_gpt2.encode(raw_text,  allowed_special={\"<|endoftext|>\"})\n",
        "clean_bpe_ids = tokenizer_gpt2.encode(clean_text, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "raw_regex_ids   = tokenizer_v2.encode(raw_text)\n",
        "clean_regex_ids = tokenizer_v2.encode(clean_text)\n",
        "\n",
        "# Totals\n",
        "total_bpe_raw   = len(raw_bpe_ids)\n",
        "total_bpe_clean = len(clean_bpe_ids)\n",
        "total_regex_raw   = len(raw_regex_ids)\n",
        "total_regex_clean = len(clean_regex_ids)\n",
        "\n",
        "# Vocabulary sizes\n",
        "vocab_bpe   = tokenizer_gpt2.n_vocab\n",
        "vocab_regex = len(tokenizer_v2.tokens2ids)\n",
        "\n",
        "# Averages per document\n",
        "avg_bpe_per_doc_raw   = total_bpe_raw   / n_docs_raw\n",
        "avg_bpe_per_doc_clean = total_bpe_clean / n_docs_clean\n",
        "avg_regex_per_doc_raw   = total_regex_raw   / n_docs_raw\n",
        "avg_regex_per_doc_clean = total_regex_clean / n_docs_clean\n",
        "\n",
        "print(\"— BPE —\")\n",
        "print(f\"Vocabulary size:        {vocab_bpe}\")\n",
        "print(f\"Total tokens (raw):     {total_bpe_raw}\")\n",
        "print(f\"Total tokens (clean):   {total_bpe_clean}\")\n",
        "print(f\"Avg tokens/doc (raw):   {avg_bpe_per_doc_raw:.0f}\")\n",
        "print(f\"Avg tokens/doc (clean): {avg_bpe_per_doc_clean:.0f}\")\n",
        "\n",
        "print(\"\\n— Regex —\")\n",
        "print(f\"Vocabulary size:        {vocab_regex}\")\n",
        "print(f\"Total tokens (raw):     {total_regex_raw}\")\n",
        "print(f\"Total tokens (clean):   {total_regex_clean}\")\n",
        "print(f\"Avg tokens/doc (raw):   {avg_regex_per_doc_raw:.0f}\")\n",
        "print(f\"Avg tokens/doc (clean): {avg_regex_per_doc_clean:.0f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "326928d2",
      "metadata": {},
      "source": [
        "### B) Before vs after cleaning — how much text was removed\n",
        "\n",
        "We compute:\n",
        "- Removal stats (chars + tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "c0afd7f9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "— Characters —\n",
            "Chars (raw):   4824591\n",
            "Chars (clean): 2383984\n",
            "Removed chars: 2440607  (50.59% reduction)\n",
            "\n",
            "— Tokens (BPE) —\n",
            "Tokens (raw):   1500503\n",
            "Tokens (clean): 564244\n",
            "Removed tokens: 936259  (62.40% reduction)\n",
            "\n",
            "— Tokens (Regex) —\n",
            "Tokens (raw):   978953\n",
            "Tokens (clean): 441487\n",
            "Removed tokens: 537466  (54.90% reduction)\n"
          ]
        }
      ],
      "source": [
        "# Character-level change\n",
        "chars_raw   = len(raw_text)\n",
        "chars_clean = len(clean_text)\n",
        "chars_removed = chars_raw - chars_clean\n",
        "pct_chars_removed = (chars_removed / chars_raw * 100.0) if chars_raw > 0 else 0.0\n",
        "\n",
        "# Token-level change (BPE)\n",
        "bpe_removed = len(raw_bpe_ids) - len(clean_bpe_ids)\n",
        "pct_bpe_removed = (bpe_removed / len(raw_bpe_ids) * 100.0) if len(raw_bpe_ids) > 0 else 0.0\n",
        "\n",
        "# Token-level change (Regex)\n",
        "regex_removed = len(raw_regex_ids) - len(clean_regex_ids)\n",
        "pct_regex_removed = (regex_removed / len(raw_regex_ids) * 100.0) if len(raw_regex_ids) > 0 else 0.0\n",
        "\n",
        "print(\"— Characters —\")\n",
        "print(f\"Chars (raw):   {chars_raw}\")\n",
        "print(f\"Chars (clean): {chars_clean}\")\n",
        "print(f\"Removed chars: {chars_removed}  ({pct_chars_removed:.2f}% reduction)\")\n",
        "\n",
        "print(\"\\n— Tokens (BPE) —\")\n",
        "print(f\"Tokens (raw):   {len(raw_bpe_ids)}\")\n",
        "print(f\"Tokens (clean): {len(clean_bpe_ids)}\")\n",
        "print(f\"Removed tokens: {bpe_removed}  ({pct_bpe_removed:.2f}% reduction)\")\n",
        "\n",
        "print(\"\\n— Tokens (Regex) —\")\n",
        "print(f\"Tokens (raw):   {len(raw_regex_ids)}\")\n",
        "print(f\"Tokens (clean): {len(clean_regex_ids)}\")\n",
        "print(f\"Removed tokens: {regex_removed}  ({pct_regex_removed:.2f}% reduction)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4a1a175",
      "metadata": {},
      "source": [
        "# Secondly: End-To-End Training and Instruction Fine-Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d07a55c3",
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "import pathlib, re, logging, requests, torch, tiktoken, random, json\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "from pathlib import Path\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "gpt2 = False\n",
        "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d954448",
      "metadata": {},
      "source": [
        "### Step 1: Load the cleaned and raw text files\n",
        "Load both the raw and cleaned text files from disk to prepare the dataset for pretraining."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6c0ef388",
      "metadata": {},
      "outputs": [],
      "source": [
        "RAW_PATH = Path(\"data/text/combined_raw.txt\")\n",
        "CLEAN_PATH = Path(\"data/text/combined_clean.txt\")\n",
        "\n",
        "raw_text = RAW_PATH.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "clean_text = CLEAN_PATH.read_text(encoding=\"utf-8\", errors=\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df8bcee5",
      "metadata": {},
      "source": [
        "### Step 2: Re-create the custom regex-based tokenizer\n",
        "Tokenize the cleaned text using a regex-based tokenizer, build its vocabulary, and define a simple tokenizer class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "6ede5c03",
      "metadata": {},
      "outputs": [],
      "source": [
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', clean_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "\n",
        "vocab = sorted(list(set(preprocessed)))\n",
        "vocab.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "\n",
        "class SimpleTokenizerV2:\n",
        "    def __init__(self, vocab):\n",
        "        self.tokens2ids = {token: id for id, token in enumerate(vocab)}\n",
        "        self.ids2tokens = {id: token for token, id in self.tokens2ids.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        tokens = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "        tokens = [item.strip() for item in tokens if item.strip()]\n",
        "        tokens = [t if t in self.tokens2ids else \"<|unk|>\" for t in tokens]\n",
        "        return [self.tokens2ids[t] for t in tokens]\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.ids2tokens[i] for i in ids])\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c736643",
      "metadata": {},
      "source": [
        "## Part #1: Pre-Training "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18c96f12",
      "metadata": {},
      "source": [
        "### Step 3: Define GPT model architecture and helper functions\n",
        "This includes the Transformer blocks, attention layers, dataset loader, training loop, evaluation, and text generation utilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "84e7afbd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# This class creates causal multi-head self-attention module.\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec) # optional projection\n",
        "\n",
        "        return context_vec\n",
        "\n",
        "# This class implements the feed-forward network used in transformer blocks.\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "# This class defines a single transformer block with attention and feed-forward layers.\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"])\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = nn.LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = nn.LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        shortcut = x # Save input for residual connection\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        return x\n",
        "\n",
        "# This class defines the overall GPT model architecture.\n",
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = nn.LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits\n",
        "\n",
        "# This class creates a dataset for GPT training using sliding windows.\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, gpt2, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        if gpt2:\n",
        "          token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "        else:\n",
        "          token_ids = tokenizer.encode(txt)\n",
        "\n",
        "        # Use a sliding window to chunk the data into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]\n",
        "\n",
        "# This function creates a DataLoader for the GPT dataset.\n",
        "def create_dataloader_v1(txt, batch_size=4, max_length=32,\n",
        "                         stride=16, shuffle=True, drop_last=True, num_workers=0, tokenizer=None):\n",
        "    \n",
        "    dataset = GPTDatasetV1(txt, gpt2, tokenizer, max_length, stride)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "# This function calculates the cross-entropy loss for a single batch.\n",
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "    return loss\n",
        "\n",
        "# This function calculates the average loss over a data loader.\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches\n",
        "\n",
        "\n",
        "# This function generates text using the model in a simple greedy manner.\n",
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
        "    return idx\n",
        "\n",
        "# This function encodes text to token IDs.\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "  if gpt2:\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "  else:\n",
        "    encoded = tokenizer.encode(text)\n",
        "  encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
        "  return encoded_tensor\n",
        "\n",
        "# This function decodes token IDs back to text.\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "  flat = token_ids.squeeze(0) # remove batch dimension\n",
        "  return tokenizer.decode(flat.tolist())\n",
        "\n",
        "# This function trains the GPT model with periodic evaluation and sample generation.\n",
        "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                       eval_freq, eval_iter, start_context, tokenizer):\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward() # Calculate loss gradients\n",
        "            optimizer.step() # Update model weights using loss gradients\n",
        "            tokens_seen += input_batch.numel()\n",
        "            global_step += 1\n",
        "\n",
        "            # Optional evaluation step\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # Print a sample text after each epoch\n",
        "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen\n",
        "\n",
        "# This function evaluates the model on training and validation sets.\n",
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss\n",
        "\n",
        "# This function generates and prints a sample text from the model.\n",
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate_text_simple(model=model, idx=encoded, max_new_tokens=50, context_size=context_size)\n",
        "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
        "    model.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "902a3233",
      "metadata": {},
      "source": [
        "### Step 4: Define the text generation function\n",
        "Implements top-k sampling and temperature-based generation for more diverse outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "295869df",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        if top_k is not None:\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
        "\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "            logits = logits - logits.max(dim=-1, keepdim=True).values\n",
        "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
        "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
        "            break\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
        "        \n",
        "    return idx\n",
        "\n",
        "# This function plots training and validation losses over epochs and tokens seen.\n",
        "import matplotlib.pyplot as plt\n",
        "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
        "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
        "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.legend(loc=\"upper right\")\n",
        "    ax2 = ax1.twiny() #A\n",
        "    ax2.plot(tokens_seen, train_losses, alpha=0) #B\n",
        "    ax2.set_xlabel(\"Tokens seen\")\n",
        "    fig.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b272af8e",
      "metadata": {},
      "source": [
        "### Step 5: Pretrain GPT model using the custom regex tokenizer\n",
        "Train GPT on your cleaned text using the regex vocabulary, then save the trained model and optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc0ec1a7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep 1 (Step 000000): Train loss 9.816, Val loss 9.869\n",
            "Ep 1 (Step 000010): Train loss 8.255, Val loss 8.339\n",
            "Ep 1 (Step 000020): Train loss 7.870, Val loss 7.827\n",
            "Ep 1 (Step 000030): Train loss 7.302, Val loss 7.506\n",
            "Ep 1 (Step 000040): Train loss 6.873, Val loss 7.276\n",
            "Ep 1 (Step 000050): Train loss 6.845, Val loss 7.168\n",
            "Ep 1 (Step 000060): Train loss 6.760, Val loss 7.086\n",
            "Ep 1 (Step 000070): Train loss 6.750, Val loss 7.030\n",
            "Ep 1 (Step 000080): Train loss 6.805, Val loss 7.004\n",
            "Ep 1 (Step 000090): Train loss 6.900, Val loss 6.994\n",
            "Ep 1 (Step 000100): Train loss 6.788, Val loss 6.938\n",
            "Ep 1 (Step 000110): Train loss 6.471, Val loss 6.961\n",
            "Ep 1 (Step 000120): Train loss 6.770, Val loss 6.893\n",
            "Ep 1 (Step 000130): Train loss 6.716, Val loss 6.869\n",
            "Ep 1 (Step 000140): Train loss 6.396, Val loss 6.843\n",
            "Ep 1 (Step 000150): Train loss 6.407, Val loss 6.823\n",
            "Ep 1 (Step 000160): Train loss 6.580, Val loss 6.778\n",
            "Ep 1 (Step 000170): Train loss 6.481, Val loss 6.752\n",
            "Ep 1 (Step 000180): Train loss 6.208, Val loss 6.773\n",
            "Ep 1 (Step 000190): Train loss 6.154, Val loss 6.732\n",
            "large language models are the model, we use, and the model, and the model, and the model, and the model, and the model, and the model, and the model, we use, and the model, and the model, and the model, and\n",
            "Ep 2 (Step 000200): Train loss 6.153, Val loss 6.742\n",
            "Ep 2 (Step 000210): Train loss 6.337, Val loss 6.722\n",
            "Ep 2 (Step 000220): Train loss 6.203, Val loss 6.699\n",
            "Ep 2 (Step 000230): Train loss 6.074, Val loss 6.681\n",
            "Ep 2 (Step 000240): Train loss 6.093, Val loss 6.686\n",
            "Ep 2 (Step 000250): Train loss 6.375, Val loss 6.669\n",
            "Ep 2 (Step 000260): Train loss 6.322, Val loss 6.649\n",
            "Ep 2 (Step 000270): Train loss 6.019, Val loss 6.641\n",
            "Ep 2 (Step 000280): Train loss 6.228, Val loss 6.628\n",
            "Ep 2 (Step 000290): Train loss 5.943, Val loss 6.615\n",
            "Ep 2 (Step 000300): Train loss 6.086, Val loss 6.653\n",
            "Ep 2 (Step 000310): Train loss 6.020, Val loss 6.617\n",
            "Ep 2 (Step 000320): Train loss 5.983, Val loss 6.583\n",
            "Ep 2 (Step 000330): Train loss 5.679, Val loss 6.578\n",
            "Ep 2 (Step 000340): Train loss 5.880, Val loss 6.581\n",
            "Ep 2 (Step 000350): Train loss 5.982, Val loss 6.553\n",
            "Ep 2 (Step 000360): Train loss 5.934, Val loss 6.551\n",
            "Ep 2 (Step 000370): Train loss 5.707, Val loss 6.552\n",
            "Ep 2 (Step 000380): Train loss 5.798, Val loss 6.534\n",
            "large language models are not be used to the model to the model to the model is the model to the model to the model to the model to the model is the model is the model is the model is the model is the model to the model to the model is the\n",
            "Ep 3 (Step 000390): Train loss 5.826, Val loss 6.528\n",
            "Ep 3 (Step 000400): Train loss 5.801, Val loss 6.498\n",
            "Ep 3 (Step 000410): Train loss 5.900, Val loss 6.530\n",
            "Ep 3 (Step 000420): Train loss 5.959, Val loss 6.531\n",
            "Ep 3 (Step 000430): Train loss 5.918, Val loss 6.493\n",
            "Ep 3 (Step 000440): Train loss 5.824, Val loss 6.495\n",
            "Ep 3 (Step 000450): Train loss 5.927, Val loss 6.492\n",
            "Ep 3 (Step 000460): Train loss 5.489, Val loss 6.496\n",
            "Ep 3 (Step 000470): Train loss 5.886, Val loss 6.468\n",
            "Ep 3 (Step 000480): Train loss 5.672, Val loss 6.473\n",
            "Ep 3 (Step 000490): Train loss 5.688, Val loss 6.505\n",
            "Ep 3 (Step 000500): Train loss 5.716, Val loss 6.505\n",
            "Ep 3 (Step 000510): Train loss 5.670, Val loss 6.430\n",
            "Ep 3 (Step 000520): Train loss 5.732, Val loss 6.442\n",
            "Ep 3 (Step 000530): Train loss 5.825, Val loss 6.435\n",
            "Ep 3 (Step 000540): Train loss 5.451, Val loss 6.423\n",
            "Ep 3 (Step 000550): Train loss 5.308, Val loss 6.420\n",
            "Ep 3 (Step 000560): Train loss 5.413, Val loss 6.423\n",
            "Ep 3 (Step 000570): Train loss 5.645, Val loss 6.442\n",
            "Ep 3 (Step 000580): Train loss 5.509, Val loss 6.388\n",
            "large language models are not be used in the model, and the model, and the model, and the model, and the model, and the model, and the model, and the model, and the model, and the model, and the model, and the model\n",
            "Ep 4 (Step 000590): Train loss 5.772, Val loss 6.443\n",
            "Ep 4 (Step 000600): Train loss 5.548, Val loss 6.394\n",
            "Ep 4 (Step 000610): Train loss 5.542, Val loss 6.420\n",
            "Ep 4 (Step 000620): Train loss 5.269, Val loss 6.419\n",
            "Ep 4 (Step 000630): Train loss 5.189, Val loss 6.398\n",
            "Ep 4 (Step 000640): Train loss 5.596, Val loss 6.397\n",
            "Ep 4 (Step 000650): Train loss 5.821, Val loss 6.395\n",
            "Ep 4 (Step 000660): Train loss 5.476, Val loss 6.373\n",
            "Ep 4 (Step 000670): Train loss 5.461, Val loss 6.352\n",
            "Ep 4 (Step 000680): Train loss 5.706, Val loss 6.397\n",
            "Ep 4 (Step 000690): Train loss 5.503, Val loss 6.375\n",
            "Ep 4 (Step 000700): Train loss 5.782, Val loss 6.385\n",
            "Ep 4 (Step 000710): Train loss 5.069, Val loss 6.356\n",
            "Ep 4 (Step 000720): Train loss 5.427, Val loss 6.361\n",
            "Ep 4 (Step 000730): Train loss 5.552, Val loss 6.330\n",
            "Ep 4 (Step 000740): Train loss 5.209, Val loss 6.314\n",
            "Ep 4 (Step 000750): Train loss 5.458, Val loss 6.304\n",
            "Ep 4 (Step 000760): Train loss 5.140, Val loss 6.340\n",
            "Ep 4 (Step 000770): Train loss 5.454, Val loss 6.323\n",
            "large language models are the model is the model to the model to the model to the model to the model to the model, the model to the model to the model to the model to the model to the model to the model to the model to the model to the model\n",
            "Ep 5 (Step 000780): Train loss 5.338, Val loss 6.306\n",
            "Ep 5 (Step 000790): Train loss 5.526, Val loss 6.329\n",
            "Ep 5 (Step 000800): Train loss 5.058, Val loss 6.323\n",
            "Ep 5 (Step 000810): Train loss 5.527, Val loss 6.334\n",
            "Ep 5 (Step 000820): Train loss 5.303, Val loss 6.318\n",
            "Ep 5 (Step 000830): Train loss 5.453, Val loss 6.318\n",
            "Ep 5 (Step 000840): Train loss 5.339, Val loss 6.342\n",
            "Ep 5 (Step 000850): Train loss 5.626, Val loss 6.293\n",
            "Ep 5 (Step 000860): Train loss 5.132, Val loss 6.321\n",
            "Ep 5 (Step 000870): Train loss 5.499, Val loss 6.324\n",
            "Ep 5 (Step 000880): Train loss 4.929, Val loss 6.282\n",
            "Ep 5 (Step 000890): Train loss 5.089, Val loss 6.285\n",
            "Ep 5 (Step 000900): Train loss 5.011, Val loss 6.301\n",
            "Ep 5 (Step 000910): Train loss 5.186, Val loss 6.314\n",
            "Ep 5 (Step 000920): Train loss 5.291, Val loss 6.323\n",
            "Ep 5 (Step 000930): Train loss 4.915, Val loss 6.317\n",
            "Ep 5 (Step 000940): Train loss 4.925, Val loss 6.265\n",
            "Ep 5 (Step 000950): Train loss 5.024, Val loss 6.265\n",
            "Ep 5 (Step 000960): Train loss 4.762, Val loss 6.321\n",
            "large language models are not only the model to the model to the model to the model is the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "gpt2 = False  # custom tokenizer mode\n",
        "tokenizer_v2 = SimpleTokenizerV2(vocab)  # regex tokenizer\n",
        "vocab_size = len(tokenizer_v2.tokens2ids)\n",
        "\n",
        "# === Config ===\n",
        "GPT_CONFIG = {\n",
        "    \"vocab_size\": vocab_size,\n",
        "    \"context_length\": 1024,\n",
        "    \"emb_dim\": 768,\n",
        "    \"n_heads\": 12,\n",
        "    \"n_layers\": 12,\n",
        "    \"drop_rate\": 0.3,\n",
        "    \"qkv_bias\": False,\n",
        "}\n",
        "\n",
        "# === Dataset ===\n",
        "train_ratio = 0.9\n",
        "split = int(train_ratio * len(clean_text))\n",
        "train_data, val_data = clean_text[:split], clean_text[split:]\n",
        "\n",
        "train_loader = create_dataloader_v1(train_data, batch_size=2, max_length=GPT_CONFIG[\"context_length\"], stride=GPT_CONFIG[\"context_length\"], tokenizer=tokenizer_v2)\n",
        "val_loader   = create_dataloader_v1(val_data,   batch_size=2, max_length=GPT_CONFIG[\"context_length\"], stride=GPT_CONFIG[\"context_length\"], shuffle=False, tokenizer=tokenizer_v2)\n",
        "\n",
        "# === Model + training ===\n",
        "model_regex = GPTModel(GPT_CONFIG).to(device)\n",
        "optimizer = torch.optim.AdamW(model_regex.parameters(), lr=1e-4, weight_decay=0.1)\n",
        "\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model_regex, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=5, eval_freq=10, eval_iter=20,\n",
        "    start_context=\"large language models are\", tokenizer=tokenizer_v2)\n",
        "\n",
        "torch.save(model_regex.state_dict(), \"models/regrex_pretrained.pth\")\n",
        "epochs_tensor = torch.linspace(0, 5, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb96c885",
      "metadata": {},
      "source": [
        "### Step 6: Pretrain GPT model using the GPT-2 tokenizer\n",
        "Repeat the same pretraining pipeline, this time with the GPT-2 vocabulary and tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f937db1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep 1 (Step 000000): Train loss 10.641, Val loss 10.629\n",
            "Ep 1 (Step 000010): Train loss 9.263, Val loss 9.124\n",
            "Ep 1 (Step 000020): Train loss 8.434, Val loss 8.415\n",
            "Ep 1 (Step 000030): Train loss 8.011, Val loss 7.863\n",
            "Ep 1 (Step 000040): Train loss 7.823, Val loss 7.480\n",
            "Ep 1 (Step 000050): Train loss 7.295, Val loss 7.307\n",
            "Ep 1 (Step 000060): Train loss 7.102, Val loss 7.177\n",
            "Ep 1 (Step 000070): Train loss 7.179, Val loss 7.081\n",
            "Ep 1 (Step 000080): Train loss 7.242, Val loss 7.068\n",
            "Ep 1 (Step 000090): Train loss 7.141, Val loss 6.950\n",
            "Ep 1 (Step 000100): Train loss 6.616, Val loss 6.870\n",
            "Ep 1 (Step 000110): Train loss 6.558, Val loss 6.825\n",
            "Ep 1 (Step 000120): Train loss 6.779, Val loss 6.745\n",
            "Ep 1 (Step 000130): Train loss 6.478, Val loss 6.685\n",
            "Ep 1 (Step 000140): Train loss 6.534, Val loss 6.589\n",
            "Ep 1 (Step 000150): Train loss 6.613, Val loss 6.518\n",
            "Ep 1 (Step 000160): Train loss 6.316, Val loss 6.460\n",
            "Ep 1 (Step 000170): Train loss 6.332, Val loss 6.474\n",
            "Ep 1 (Step 000180): Train loss 6.376, Val loss 6.456\n",
            "Ep 1 (Step 000190): Train loss 6.201, Val loss 6.456\n",
            "Ep 1 (Step 000200): Train loss 5.764, Val loss 6.374\n",
            "Ep 1 (Step 000210): Train loss 6.216, Val loss 6.354\n",
            "Ep 1 (Step 000220): Train loss 6.080, Val loss 6.339\n",
            "Ep 1 (Step 000230): Train loss 6.091, Val loss 6.325\n",
            "Ep 1 (Step 000240): Train loss 6.195, Val loss 6.285\n",
            "large language models are the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model\n",
            "Ep 2 (Step 000250): Train loss 6.101, Val loss 6.295\n",
            "Ep 2 (Step 000260): Train loss 6.173, Val loss 6.266\n",
            "Ep 2 (Step 000270): Train loss 6.060, Val loss 6.266\n",
            "Ep 2 (Step 000280): Train loss 5.878, Val loss 6.206\n",
            "Ep 2 (Step 000290): Train loss 6.018, Val loss 6.212\n",
            "Ep 2 (Step 000300): Train loss 6.204, Val loss 6.171\n",
            "Ep 2 (Step 000310): Train loss 6.258, Val loss 6.158\n",
            "Ep 2 (Step 000320): Train loss 5.890, Val loss 6.124\n",
            "Ep 2 (Step 000330): Train loss 5.814, Val loss 6.105\n",
            "Ep 2 (Step 000340): Train loss 6.264, Val loss 6.114\n",
            "Ep 2 (Step 000350): Train loss 5.815, Val loss 6.126\n",
            "Ep 2 (Step 000360): Train loss 5.763, Val loss 6.092\n",
            "Ep 2 (Step 000370): Train loss 5.745, Val loss 6.060\n",
            "Ep 2 (Step 000380): Train loss 6.120, Val loss 6.005\n",
            "Ep 2 (Step 000390): Train loss 5.887, Val loss 6.013\n",
            "Ep 2 (Step 000400): Train loss 5.578, Val loss 5.984\n",
            "Ep 2 (Step 000410): Train loss 5.919, Val loss 5.962\n",
            "Ep 2 (Step 000420): Train loss 6.120, Val loss 5.970\n",
            "Ep 2 (Step 000430): Train loss 5.871, Val loss 5.910\n",
            "Ep 2 (Step 000440): Train loss 5.715, Val loss 5.935\n",
            "Ep 2 (Step 000450): Train loss 5.616, Val loss 5.879\n",
            "Ep 2 (Step 000460): Train loss 5.464, Val loss 5.870\n",
            "Ep 2 (Step 000470): Train loss 5.418, Val loss 5.862\n",
            "Ep 2 (Step 000480): Train loss 5.775, Val loss 5.893\n",
            "Ep 2 (Step 000490): Train loss 5.445, Val loss 5.873\n",
            "large language models are the model, and the model, and the model, and the model, and the model, and the model, and the model, and the model, and the model, and the model, and the model, and the model, and the model\n",
            "Ep 3 (Step 000500): Train loss 5.532, Val loss 5.868\n",
            "Ep 3 (Step 000510): Train loss 5.517, Val loss 5.843\n",
            "Ep 3 (Step 000520): Train loss 5.822, Val loss 5.820\n",
            "Ep 3 (Step 000530): Train loss 5.971, Val loss 5.774\n",
            "Ep 3 (Step 000540): Train loss 5.648, Val loss 5.762\n",
            "Ep 3 (Step 000550): Train loss 5.351, Val loss 5.786\n",
            "Ep 3 (Step 000560): Train loss 5.612, Val loss 5.765\n",
            "Ep 3 (Step 000570): Train loss 5.075, Val loss 5.758\n",
            "Ep 3 (Step 000580): Train loss 5.472, Val loss 5.731\n",
            "Ep 3 (Step 000590): Train loss 5.327, Val loss 5.731\n",
            "Ep 3 (Step 000600): Train loss 5.432, Val loss 5.704\n",
            "Ep 3 (Step 000610): Train loss 5.602, Val loss 5.703\n",
            "Ep 3 (Step 000620): Train loss 5.189, Val loss 5.691\n",
            "Ep 3 (Step 000630): Train loss 5.301, Val loss 5.702\n",
            "Ep 3 (Step 000640): Train loss 5.157, Val loss 5.737\n",
            "Ep 3 (Step 000650): Train loss 5.646, Val loss 5.677\n",
            "Ep 3 (Step 000660): Train loss 5.463, Val loss 5.698\n",
            "Ep 3 (Step 000670): Train loss 5.404, Val loss 5.687\n",
            "Ep 3 (Step 000680): Train loss 5.344, Val loss 5.655\n",
            "Ep 3 (Step 000690): Train loss 5.237, Val loss 5.641\n",
            "Ep 3 (Step 000700): Train loss 5.211, Val loss 5.626\n",
            "Ep 3 (Step 000710): Train loss 5.305, Val loss 5.663\n",
            "Ep 3 (Step 000720): Train loss 5.019, Val loss 5.651\n",
            "Ep 3 (Step 000730): Train loss 5.123, Val loss 5.665\n",
            "Ep 3 (Step 000740): Train loss 5.090, Val loss 5.632\n",
            "large language models are the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model\n",
            "Ep 4 (Step 000750): Train loss 5.394, Val loss 5.601\n",
            "Ep 4 (Step 000760): Train loss 5.451, Val loss 5.606\n",
            "Ep 4 (Step 000770): Train loss 5.253, Val loss 5.600\n",
            "Ep 4 (Step 000780): Train loss 5.077, Val loss 5.605\n",
            "Ep 4 (Step 000790): Train loss 5.482, Val loss 5.581\n",
            "Ep 4 (Step 000800): Train loss 5.219, Val loss 5.597\n",
            "Ep 4 (Step 000810): Train loss 5.064, Val loss 5.575\n",
            "Ep 4 (Step 000820): Train loss 5.327, Val loss 5.569\n",
            "Ep 4 (Step 000830): Train loss 5.191, Val loss 5.577\n",
            "Ep 4 (Step 000840): Train loss 5.421, Val loss 5.558\n",
            "Ep 4 (Step 000850): Train loss 5.117, Val loss 5.553\n",
            "Ep 4 (Step 000860): Train loss 5.003, Val loss 5.545\n",
            "Ep 4 (Step 000870): Train loss 5.117, Val loss 5.565\n",
            "Ep 4 (Step 000880): Train loss 5.232, Val loss 5.572\n",
            "Ep 4 (Step 000890): Train loss 5.309, Val loss 5.568\n",
            "Ep 4 (Step 000900): Train loss 5.059, Val loss 5.587\n",
            "Ep 4 (Step 000910): Train loss 5.010, Val loss 5.544\n",
            "Ep 4 (Step 000920): Train loss 4.500, Val loss 5.517\n",
            "Ep 4 (Step 000930): Train loss 4.809, Val loss 5.486\n",
            "Ep 4 (Step 000940): Train loss 5.166, Val loss 5.520\n",
            "Ep 4 (Step 000950): Train loss 4.968, Val loss 5.493\n",
            "Ep 4 (Step 000960): Train loss 4.900, Val loss 5.450\n",
            "Ep 4 (Step 000970): Train loss 4.510, Val loss 5.451\n",
            "Ep 4 (Step 000980): Train loss 4.945, Val loss 5.469\n",
            "Ep 4 (Step 000990): Train loss 4.841, Val loss 5.475\n",
            "large language models are not the model is a single-V2-V2-V2-V2-V2-V2-V2-V2-V2-V2-V2-V2-V2-V2-V\n",
            "Ep 5 (Step 001000): Train loss 4.941, Val loss 5.482\n",
            "Ep 5 (Step 001010): Train loss 5.107, Val loss 5.491\n",
            "Ep 5 (Step 001020): Train loss 4.734, Val loss 5.465\n",
            "Ep 5 (Step 001030): Train loss 4.467, Val loss 5.424\n",
            "Ep 5 (Step 001040): Train loss 5.080, Val loss 5.437\n",
            "Ep 5 (Step 001050): Train loss 5.111, Val loss 5.428\n",
            "Ep 5 (Step 001060): Train loss 4.953, Val loss 5.466\n",
            "Ep 5 (Step 001070): Train loss 4.937, Val loss 5.465\n",
            "Ep 5 (Step 001080): Train loss 4.940, Val loss 5.459\n",
            "Ep 5 (Step 001090): Train loss 5.058, Val loss 5.428\n",
            "Ep 5 (Step 001100): Train loss 4.709, Val loss 5.424\n",
            "Ep 5 (Step 001110): Train loss 4.736, Val loss 5.411\n",
            "Ep 5 (Step 001120): Train loss 4.721, Val loss 5.397\n",
            "Ep 5 (Step 001130): Train loss 4.719, Val loss 5.419\n",
            "Ep 5 (Step 001140): Train loss 4.845, Val loss 5.420\n",
            "Ep 5 (Step 001150): Train loss 4.874, Val loss 5.408\n",
            "Ep 5 (Step 001160): Train loss 4.548, Val loss 5.406\n",
            "Ep 5 (Step 001170): Train loss 4.416, Val loss 5.377\n",
            "Ep 5 (Step 001180): Train loss 4.986, Val loss 5.400\n",
            "Ep 5 (Step 001190): Train loss 4.501, Val loss 5.407\n",
            "Ep 5 (Step 001200): Train loss 4.646, Val loss 5.406\n",
            "Ep 5 (Step 001210): Train loss 4.829, Val loss 5.393\n",
            "Ep 5 (Step 001220): Train loss 4.485, Val loss 5.392\n",
            "Ep 5 (Step 001230): Train loss 4.700, Val loss 5.414\n",
            "large language models are not be used to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the\n",
            "Ep 6 (Step 001240): Train loss 4.940, Val loss 5.386\n",
            "Ep 6 (Step 001250): Train loss 4.560, Val loss 5.404\n",
            "Ep 6 (Step 001260): Train loss 4.887, Val loss 5.437\n",
            "Ep 6 (Step 001270): Train loss 4.575, Val loss 5.368\n",
            "Ep 6 (Step 001280): Train loss 4.602, Val loss 5.365\n",
            "Ep 6 (Step 001290): Train loss 4.622, Val loss 5.364\n",
            "Ep 6 (Step 001300): Train loss 4.579, Val loss 5.340\n",
            "Ep 6 (Step 001310): Train loss 4.714, Val loss 5.345\n",
            "Ep 6 (Step 001320): Train loss 4.764, Val loss 5.355\n",
            "Ep 6 (Step 001330): Train loss 4.289, Val loss 5.346\n",
            "Ep 6 (Step 001340): Train loss 4.439, Val loss 5.300\n",
            "Ep 6 (Step 001350): Train loss 4.738, Val loss 5.333\n",
            "Ep 6 (Step 001360): Train loss 4.727, Val loss 5.333\n",
            "Ep 6 (Step 001370): Train loss 4.408, Val loss 5.335\n",
            "Ep 6 (Step 001380): Train loss 4.810, Val loss 5.330\n",
            "Ep 6 (Step 001390): Train loss 4.542, Val loss 5.349\n",
            "Ep 6 (Step 001400): Train loss 4.773, Val loss 5.338\n",
            "Ep 6 (Step 001410): Train loss 4.334, Val loss 5.331\n",
            "Ep 6 (Step 001420): Train loss 4.714, Val loss 5.366\n",
            "Ep 6 (Step 001430): Train loss 4.566, Val loss 5.318\n",
            "Ep 6 (Step 001440): Train loss 4.505, Val loss 5.309\n",
            "Ep 6 (Step 001450): Train loss 4.971, Val loss 5.290\n",
            "Ep 6 (Step 001460): Train loss 4.689, Val loss 5.338\n",
            "Ep 6 (Step 001470): Train loss 4.478, Val loss 5.317\n",
            "Ep 6 (Step 001480): Train loss 4.129, Val loss 5.325\n",
            "large language models are not the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model. We use of the model to the model to the model to the model to\n",
            "Ep 7 (Step 001490): Train loss 4.100, Val loss 5.327\n",
            "Ep 7 (Step 001500): Train loss 4.508, Val loss 5.299\n",
            "Ep 7 (Step 001510): Train loss 4.765, Val loss 5.300\n",
            "Ep 7 (Step 001520): Train loss 4.211, Val loss 5.272\n",
            "Ep 7 (Step 001530): Train loss 4.636, Val loss 5.296\n",
            "Ep 7 (Step 001540): Train loss 4.358, Val loss 5.358\n",
            "Ep 7 (Step 001550): Train loss 4.352, Val loss 5.292\n",
            "Ep 7 (Step 001560): Train loss 4.451, Val loss 5.308\n",
            "Ep 7 (Step 001570): Train loss 4.830, Val loss 5.293\n",
            "Ep 7 (Step 001580): Train loss 4.482, Val loss 5.283\n",
            "Ep 7 (Step 001590): Train loss 4.214, Val loss 5.266\n",
            "Ep 7 (Step 001600): Train loss 4.279, Val loss 5.294\n",
            "Ep 7 (Step 001610): Train loss 3.920, Val loss 5.288\n",
            "Ep 7 (Step 001620): Train loss 4.565, Val loss 5.284\n",
            "Ep 7 (Step 001630): Train loss 4.584, Val loss 5.290\n",
            "Ep 7 (Step 001640): Train loss 4.434, Val loss 5.263\n",
            "Ep 7 (Step 001650): Train loss 4.498, Val loss 5.286\n",
            "Ep 7 (Step 001660): Train loss 4.307, Val loss 5.268\n",
            "Ep 7 (Step 001670): Train loss 4.299, Val loss 5.276\n",
            "Ep 7 (Step 001680): Train loss 4.368, Val loss 5.286\n",
            "Ep 7 (Step 001690): Train loss 4.390, Val loss 5.287\n",
            "Ep 7 (Step 001700): Train loss 4.262, Val loss 5.281\n",
            "Ep 7 (Step 001710): Train loss 4.266, Val loss 5.260\n",
            "Ep 7 (Step 001720): Train loss 4.213, Val loss 5.269\n",
            "Ep 7 (Step 001730): Train loss 4.218, Val loss 5.248\n",
            "large language models are not only a single-shot setting. We also perform a single-shot setting, we have been a model to the model to the model, we use the model to the model, we use the model to the model to the model to the\n",
            "Ep 8 (Step 001740): Train loss 4.255, Val loss 5.240\n",
            "Ep 8 (Step 001750): Train loss 4.373, Val loss 5.238\n",
            "Ep 8 (Step 001760): Train loss 4.283, Val loss 5.234\n",
            "Ep 8 (Step 001770): Train loss 4.302, Val loss 5.240\n",
            "Ep 8 (Step 001780): Train loss 4.437, Val loss 5.267\n",
            "Ep 8 (Step 001790): Train loss 4.149, Val loss 5.281\n",
            "Ep 8 (Step 001800): Train loss 4.160, Val loss 5.246\n",
            "Ep 8 (Step 001810): Train loss 4.185, Val loss 5.257\n",
            "Ep 8 (Step 001820): Train loss 4.463, Val loss 5.264\n",
            "Ep 8 (Step 001830): Train loss 4.002, Val loss 5.256\n",
            "Ep 8 (Step 001840): Train loss 4.090, Val loss 5.239\n",
            "Ep 8 (Step 001850): Train loss 4.038, Val loss 5.251\n",
            "Ep 8 (Step 001860): Train loss 4.334, Val loss 5.254\n",
            "Ep 8 (Step 001870): Train loss 4.462, Val loss 5.225\n",
            "Ep 8 (Step 001880): Train loss 4.408, Val loss 5.257\n",
            "Ep 8 (Step 001890): Train loss 4.211, Val loss 5.235\n",
            "Ep 8 (Step 001900): Train loss 4.015, Val loss 5.260\n",
            "Ep 8 (Step 001910): Train loss 4.638, Val loss 5.272\n",
            "Ep 8 (Step 001920): Train loss 4.256, Val loss 5.273\n",
            "Ep 8 (Step 001930): Train loss 4.033, Val loss 5.262\n",
            "Ep 8 (Step 001940): Train loss 4.251, Val loss 5.265\n",
            "Ep 8 (Step 001950): Train loss 3.932, Val loss 5.274\n",
            "Ep 8 (Step 001960): Train loss 3.965, Val loss 5.213\n",
            "Ep 8 (Step 001970): Train loss 4.151, Val loss 5.233\n",
            "Ep 8 (Step 001980): Train loss 4.134, Val loss 5.221\n",
            "large language models are not be used to the model. We also find that the model to the model, we use the model to the model, we use the model to the model, we use the model to the model to the model to the model, we use\n",
            "Ep 9 (Step 001990): Train loss 4.112, Val loss 5.232\n",
            "Ep 9 (Step 002000): Train loss 3.947, Val loss 5.240\n",
            "Ep 9 (Step 002010): Train loss 4.129, Val loss 5.240\n",
            "Ep 9 (Step 002020): Train loss 4.343, Val loss 5.237\n",
            "Ep 9 (Step 002030): Train loss 4.187, Val loss 5.251\n",
            "Ep 9 (Step 002040): Train loss 4.319, Val loss 5.276\n",
            "Ep 9 (Step 002050): Train loss 4.028, Val loss 5.242\n",
            "Ep 9 (Step 002060): Train loss 3.981, Val loss 5.243\n",
            "Ep 9 (Step 002070): Train loss 4.071, Val loss 5.245\n",
            "Ep 9 (Step 002080): Train loss 4.140, Val loss 5.249\n",
            "Ep 9 (Step 002090): Train loss 3.855, Val loss 5.232\n",
            "Ep 9 (Step 002100): Train loss 4.046, Val loss 5.234\n",
            "Ep 9 (Step 002110): Train loss 3.883, Val loss 5.223\n",
            "Ep 9 (Step 002120): Train loss 3.902, Val loss 5.232\n",
            "Ep 9 (Step 002130): Train loss 3.947, Val loss 5.233\n",
            "Ep 9 (Step 002140): Train loss 4.091, Val loss 5.248\n",
            "Ep 9 (Step 002150): Train loss 4.065, Val loss 5.250\n",
            "Ep 9 (Step 002160): Train loss 4.012, Val loss 5.199\n",
            "Ep 9 (Step 002170): Train loss 3.774, Val loss 5.203\n",
            "Ep 9 (Step 002180): Train loss 4.141, Val loss 5.215\n",
            "Ep 9 (Step 002190): Train loss 3.773, Val loss 5.231\n",
            "Ep 9 (Step 002200): Train loss 3.919, Val loss 5.178\n",
            "Ep 9 (Step 002210): Train loss 4.188, Val loss 5.197\n",
            "Ep 9 (Step 002220): Train loss 3.934, Val loss 5.219\n",
            "Ep 9 (Step 002230): Train loss 3.976, Val loss 5.200\n",
            "large language models are not the model is a model is a more than the model is a model is a model is a model is a model is a model is a model is a model is a model is a model is a model is a model is a model is a\n",
            "Ep 10 (Step 002240): Train loss 3.826, Val loss 5.184\n",
            "Ep 10 (Step 002250): Train loss 3.962, Val loss 5.211\n",
            "Ep 10 (Step 002260): Train loss 4.084, Val loss 5.207\n",
            "Ep 10 (Step 002270): Train loss 4.130, Val loss 5.206\n",
            "Ep 10 (Step 002280): Train loss 3.932, Val loss 5.223\n",
            "Ep 10 (Step 002290): Train loss 3.875, Val loss 5.222\n",
            "Ep 10 (Step 002300): Train loss 3.572, Val loss 5.195\n",
            "Ep 10 (Step 002310): Train loss 4.055, Val loss 5.203\n",
            "Ep 10 (Step 002320): Train loss 3.864, Val loss 5.222\n",
            "Ep 10 (Step 002330): Train loss 3.699, Val loss 5.208\n",
            "Ep 10 (Step 002340): Train loss 3.722, Val loss 5.234\n",
            "Ep 10 (Step 002350): Train loss 3.816, Val loss 5.231\n",
            "Ep 10 (Step 002360): Train loss 3.996, Val loss 5.217\n",
            "Ep 10 (Step 002370): Train loss 4.001, Val loss 5.216\n",
            "Ep 10 (Step 002380): Train loss 3.580, Val loss 5.191\n",
            "Ep 10 (Step 002390): Train loss 3.928, Val loss 5.193\n",
            "Ep 10 (Step 002400): Train loss 4.158, Val loss 5.207\n",
            "Ep 10 (Step 002410): Train loss 3.903, Val loss 5.212\n",
            "Ep 10 (Step 002420): Train loss 3.549, Val loss 5.180\n",
            "Ep 10 (Step 002430): Train loss 3.782, Val loss 5.203\n",
            "Ep 10 (Step 002440): Train loss 3.815, Val loss 5.203\n",
            "Ep 10 (Step 002450): Train loss 3.513, Val loss 5.211\n",
            "Ep 10 (Step 002460): Train loss 3.791, Val loss 5.196\n",
            "Ep 10 (Step 002470): Train loss 3.731, Val loss 5.175\n",
            "large language models are not only the model to the model to the model to the model to the model. In this work is a model is a model is a model to be a model to the model to the model to the model to be more than the model to\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "gpt2 = True\n",
        "tokenizer_gpt2 = tiktoken.get_encoding(\"gpt2\")\n",
        "vocab_size = tokenizer_gpt2.n_vocab\n",
        "GPT_CONFIG[\"vocab_size\"] = vocab_size\n",
        "\n",
        "# Recreate model & loaders (same data)\n",
        "train_loader = create_dataloader_v1(train_data, batch_size=2, max_length=GPT_CONFIG[\"context_length\"], stride=GPT_CONFIG[\"context_length\"], tokenizer=tokenizer_gpt2)\n",
        "val_loader   = create_dataloader_v1(val_data,   batch_size=2, max_length=GPT_CONFIG[\"context_length\"], stride=GPT_CONFIG[\"context_length\"], shuffle=False, tokenizer=tokenizer_gpt2)\n",
        "\n",
        "model_gpt2 = GPTModel(GPT_CONFIG).to(device)\n",
        "optimizer = torch.optim.AdamW(model_gpt2.parameters(), lr=1e-4, weight_decay=0.1)\n",
        "\n",
        "train_losses_gpt2, val_losses_gpt2, tokens_seen_gpt2 = train_model_simple(\n",
        "    model_gpt2, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=10, eval_freq=10, eval_iter=20,\n",
        "    start_context=\"large language models are\", tokenizer=tokenizer_gpt2)\n",
        "\n",
        "torch.save(model_gpt2.state_dict(), \"models/gpt2_pretrained.pth\")\n",
        "epochs_tensor = torch.linspace(0, 10, len(train_losses_gpt2))\n",
        "plot_losses(epochs_tensor, tokens_seen_gpt2, train_losses_gpt2, val_losses_gpt2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd024798",
      "metadata": {},
      "source": [
        "### Step 7: Load the pretrained Regex and GPT-2 models from saved checkpoints\n",
        "\n",
        "This step restores the previously trained models (`gpt2_pretrained.pth` and `regex_pretrained.pth`) so they can be used later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abe81339",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🟦 Regex tokenizer model loaded.\n",
            "🟨 GPT-2 tokenizer model loaded.\n"
          ]
        }
      ],
      "source": [
        "GPT_CONFIG = {\n",
        "    \"vocab_size\": 1000,  # placeholder, will be updated\n",
        "    \"context_length\": 1024,\n",
        "    \"emb_dim\": 768,\n",
        "    \"n_heads\": 12,\n",
        "    \"n_layers\": 12,\n",
        "    \"drop_rate\": 0.3,\n",
        "    \"qkv_bias\": False}\n",
        "\n",
        "# ---- Load Regex tokenizer model ----\n",
        "gpt2 = False\n",
        "tokenizer_v2 = SimpleTokenizerV2(vocab)\n",
        "vocab_size = len(tokenizer_v2.tokens2ids)\n",
        "GPT_CONFIG[\"vocab_size\"] = vocab_size\n",
        "\n",
        "model_regex = GPTModel(GPT_CONFIG).to(device)\n",
        "model_regex.load_state_dict(torch.load(\"models/regex_pretrained.pth\", map_location=device))\n",
        "model_regex.eval()\n",
        "print(\"🟦 Regex tokenizer model loaded.\")\n",
        "\n",
        "# ---- Load GPT-2 tokenizer model ----\n",
        "gpt2 = True\n",
        "tokenizer_gpt2 = tiktoken.get_encoding(\"gpt2\")\n",
        "vocab_size = tokenizer_gpt2.n_vocab\n",
        "GPT_CONFIG[\"vocab_size\"] = vocab_size\n",
        "\n",
        "model_gpt2 = GPTModel(GPT_CONFIG).to(device)\n",
        "model_gpt2.load_state_dict(torch.load(\"models/gpt2_pretrained.pth\", map_location=device))\n",
        "model_gpt2.eval()\n",
        "print(\"🟨 GPT-2 tokenizer model loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26a45134",
      "metadata": {},
      "source": [
        "### Step 8: Generate sample text using both trained models (Regex & GPT-2)\n",
        "\n",
        "Here we use the same `generate()` function to compare how both models (regex-tokenizer and GPT-2 tokenizer) generate text.  \n",
        " - Adjust `top_k` and `temperature` for creativity control."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "cc9770ce",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🟦 Regex tokenizer model output:\n",
            " large language models are then can generate two stages, with only on top with no prompt with their models from the performance and reasoning in the model in Figure 7B. • We report a new size. We find the model’s ability to produce an average, and use in an the\n",
            "\n",
            "🟨 GPT-2 tokenizer model output:\n",
            " large language models are the performance of the entire and text-tuned models in-Pro, including MLLMs, which is designed for all the models with a large margin, as possible with human evaluation models. During LLMs of two versions of large model, we\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "start_text = \"large language models are\"\n",
        "\n",
        "# === Regex tokenizer model ===\n",
        "gpt2 = False\n",
        "vocab_size = len(tokenizer_v2.tokens2ids)\n",
        "GPT_CONFIG[\"vocab_size\"] = vocab_size\n",
        "token_ids = generate(\n",
        "    model=model_regex,\n",
        "    idx=text_to_token_ids(start_text, tokenizer_v2).to(device),\n",
        "    max_new_tokens=50,\n",
        "    context_size=GPT_CONFIG[\"context_length\"],\n",
        "    top_k=25,\n",
        "    temperature=1.4)\n",
        "print(\"🟦 Regex tokenizer model output:\\n\", token_ids_to_text(token_ids, tokenizer_v2))\n",
        "\n",
        "# === GPT-2 tokenizer model ===\n",
        "gpt2 = True\n",
        "vocab_size = tokenizer_gpt2.n_vocab\n",
        "GPT_CONFIG[\"vocab_size\"] = vocab_size\n",
        "token_ids = generate(\n",
        "    model=model_gpt2,\n",
        "    idx=text_to_token_ids(start_text, tokenizer_gpt2).to(device),\n",
        "    max_new_tokens=50,\n",
        "    context_size=GPT_CONFIG[\"context_length\"],\n",
        "    top_k=25,\n",
        "    temperature=1.4)\n",
        "print(\"\\n🟨 GPT-2 tokenizer model output:\\n\", token_ids_to_text(token_ids, tokenizer_gpt2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e60626fa",
      "metadata": {},
      "source": [
        "## Part 2: Instruction–Response Pair Generation\n",
        "\n",
        "Here we automates the creation of an instruction-following dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f60d123",
      "metadata": {},
      "source": [
        "### Step 1: Split Documents text into Subsections \n",
        "\n",
        " - Prepare Source Documents: We split firstly the text into document units.\n",
        " - The cleaned text already uses (`=` * 80) as a delimiter.  \n",
        "\n",
        "Each document may contain multiple paragraphs or sections, and using only full documents limits the number of possible instruction–response pairs.  \n",
        "\n",
        "The `split_into_subsections()` function increases dataset richness by dividing each document into smaller **subsections** based on the following criteria:\n",
        "\n",
        "- **Splitting rule:** It separates text wherever there is **one or more newline characters (`\\n`)**.  \n",
        "- **Filtering rule:** It keeps only those subsections containing **at least 80 words** (`min_words=80`) to ensure that each segment is substantial enough."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "b22b5562",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detected 37 documents.\n",
            "Total sections: 1618\n"
          ]
        }
      ],
      "source": [
        "def split_into_subsections(docs, min_words=80): \n",
        "    # Previous configuration: min_words=117 was choosen manually to have = 2028 pairs\n",
        "    # New configuration: min_words=80 to have more sections for training = 4840 pairs\n",
        "    sections = []\n",
        "    for d in docs:\n",
        "        parts = re.split(r'\\n{1,}', d)\n",
        "        for p in parts:\n",
        "            if len(p.split()) >= min_words:\n",
        "                sections.append(p.strip())\n",
        "    return sections\n",
        "\n",
        "docs = [d.strip() for d in clean_text.split(\"=\" * 80) if d.strip()]\n",
        "print(f\"Detected {len(docs)} documents.\")\n",
        "\n",
        "sections = split_into_subsections(docs)\n",
        "print(f\"Total sections: {len(sections)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e330447d",
      "metadata": {},
      "source": [
        "### Step 2: Loads a pretrained instruction-tuned model\n",
        "   - Uses the free and open-source **`Qwen/Qwen2.5-7B-Instruct`** model from Hugging Face.  \n",
        "   - Automatically initializes a text-generation pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "078f6d3d",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/nabeel/anaconda3/envs/LLM/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "2025-11-16 15:32:50.593442: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu126 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.72it/s]\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Device set to use cuda:2\n"
          ]
        }
      ],
      "source": [
        "import re, random, json\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# ===== Load Mistral-7B-Instruct model =====\n",
        "# model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "device = \"cuda:2\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token  # prevent padding warnings\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(\"cuda:2\")\n",
        "\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=2,  # explicitly pick cuda:2\n",
        "    torch_dtype=torch.float16,\n",
        "    max_new_tokens=256,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    batch_size=12)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0d67bf4",
      "metadata": {},
      "source": [
        "### Step 3: Creates multiple types of instruction–input pairs per paragraph\n",
        "  - Each paragraph is lightly cleaned and truncated to stay within the **1024-token context window**.\n",
        "  - For each text chunk, three types of tasks are created:\n",
        "     - **Summarization** — e.g., “Summarize the following passage in 1–3 sentences.”\n",
        "     - **Explanation** — e.g., “Explain the main mechanism or method described.”\n",
        "     - **Q&A** — e.g., “What is [Key Term] in this context?” (automatically picks a capitalized term from the text).\n",
        "   - Each instruction and paragraph are wrapped in **Alpaca-style format**:\n",
        "     ```\n",
        "     Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "     ### Instruction:\n",
        "     [Instruction text]\n",
        "\n",
        "     ### Input:\n",
        "     [Paragraph text]\n",
        "\n",
        "     ### Response:\n",
        "     ```\n",
        "\n",
        "### Step 4: Generates real model responses automatically\n",
        "   - The Mistral model reads each formatted prompt and writes an appropriate answer.  \n",
        "   - The generated response is extracted from the output and stored as `\"Response\"`.\n",
        "\n",
        "### Step 5: Assembles and saves a full JSON dataset\n",
        "   - The result is a list of dictionaries:\n",
        "     ```json\n",
        "     {\n",
        "       \"Instruction\": \"...\",\n",
        "       \"Input\": \"...\",\n",
        "       \"Response\": \"...\"\n",
        "     }\n",
        "     ```\n",
        "   - Saved to `instruction_data_from_pdfs.json`, ready for supervised fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating instruction–response pairs:   1%|          | 28/4860 [01:57<5:20:38,  3.98s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "Generating instruction–response pairs: 100%|█████████▉| 4840/4860 [4:20:06<01:04,  3.22s/it]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated 4840 instruction–response pairs.\n",
            "Saved → instruction_data_from_pdfs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ===== Generate instruction–response pairs =====\n",
        "def make_pairs_from_paragraphs(paragraphs, target=2000, seed=123, max_chars=1500):\n",
        "    random.seed(seed)\n",
        "    random.shuffle(paragraphs)\n",
        "\n",
        "    pairs = []\n",
        "    progress = tqdm(total=target, desc=\"Generating instruction–response pairs\")\n",
        "\n",
        "    for p in paragraphs:\n",
        "        text = re.sub(r\"\\s+\", \" \", p).strip()\n",
        "        if len(text.split()) < 40:\n",
        "            continue\n",
        "\n",
        "        short_text = text[:max_chars]\n",
        "\n",
        "        # Define 3 instruction types\n",
        "        tasks = [\n",
        "            (\"Summarize the following passage in 1–3 sentences.\", short_text),\n",
        "            (\"Explain the main mechanism or method described.\", short_text)\n",
        "        ]\n",
        "\n",
        "        # Try to create a context-specific Q&A\n",
        "        tokens = re.findall(r\"[A-Za-z][A-Za-z\\-]+\", text)\n",
        "        topic = next((t for t in tokens if t[0].isupper() and len(t) > 3), None)\n",
        "        if topic:\n",
        "            tasks.append((f\"What is {topic} in this context?\", short_text))\n",
        "\n",
        "        # Batch prompts for efficiency\n",
        "        prompts = [\n",
        "            f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n\"\n",
        "            f\"### Instruction:\\n{inst}\\n\\n### Input:\\n{inp}\\n\\n### Response:\\n\"\n",
        "            for inst, inp in tasks\n",
        "        ]\n",
        "\n",
        "        outputs = generator(prompts)\n",
        "\n",
        "        for prompt, out in zip(prompts, outputs):\n",
        "            response = out[0][\"generated_text\"].split(\"### Response:\")[-1].strip()\n",
        "            instruction = re.search(r\"### Instruction:\\n(.*?)\\n\\n### Input:\", prompt, re.S).group(1)\n",
        "            input_text = re.search(r\"### Input:\\n(.*)\\n\\n### Response:\", prompt, re.S).group(1)\n",
        "\n",
        "            pairs.append({\n",
        "                \"Instruction\": instruction,\n",
        "                \"Input\": input_text,\n",
        "                \"Response\": response\n",
        "            })\n",
        "\n",
        "            progress.update(1)\n",
        "            if len(pairs) >= target:\n",
        "                progress.close()\n",
        "                return pairs[:target]\n",
        "\n",
        "    progress.close()\n",
        "    return pairs[:target]\n",
        "\n",
        "# debug_subset = sections[:15]  # use only first 15 sections\n",
        "# pairs = make_pairs_from_paragraphs(debug_subset, target=10)\n",
        "\n",
        "target0 = 4860 \n",
        "# In a previous configuration, I set min_words=117 to get exactly 2028 pairs. \n",
        "# Now with min_words=80, I can get 4860 pairs.\n",
        "\n",
        "pairs = make_pairs_from_paragraphs(sections, target=target0)\n",
        "print(f\"Generated {len(pairs)} instruction–response pairs.\")\n",
        "\n",
        "# ===== Save =====\n",
        "with open(\"data\\instruction_data_from_pdfs.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(pairs, f, ensure_ascii=False, indent=2)\n",
        "print(\"Saved → data\\instruction_data_from_pdfs.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ee2dd33",
      "metadata": {},
      "source": [
        "### Step 6: Load the instruction dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0aa6302e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 4840 instruction–response pairs.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "with open(\"data\\instruction_data_from_pdfs.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "print(f\"Loaded {len(data)} instruction–response pairs.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecf6d6db",
      "metadata": {},
      "source": [
        "### Step 7: Performs a Train/Validation split\n",
        "   - Randomly shuffles and splits the dataset into:\n",
        "     - *4340 training samples*\n",
        "     - *500 validation samples*\n",
        "  - In a previous configuration, I made *1528 training samples* and *500 validation samples*, and the performance was less compared to this new configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "718ae127",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train: 4340 | val: 500 | total: 4840\n"
          ]
        }
      ],
      "source": [
        "pairs = data.copy()\n",
        "random.shuffle(pairs)\n",
        "val_set = pairs[:500]\n",
        "train_set = pairs[500:]\n",
        "\n",
        "print(f\"train: {len(train_set)} | val: {len(val_set)} | total: {len(pairs)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cba3f46d",
      "metadata": {},
      "source": [
        "## Part 3 — Fine-Tuning The Pretrained Models\n",
        "Here we fine-tune both pretrained models (Regex and GPT-2 versions) on the generated instruction dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5136f12",
      "metadata": {},
      "source": [
        "### Step 1: Prepare datasets and dataloaders\n",
        "\n",
        "- `InstructionDataset()` and `format_input()` are used to prepare the dataset for instruction fine-tuning.  \n",
        "    - They convert each JSON entry (with keys `\"Instruction\"`, `\"Input\"`, and `\"Response\"`) into an **Alpaca-style prompt–response format** suitable for LLM models.\n",
        "    - **`format_input()`:** Formats each record into the Alpaca-style text.\n",
        "    - **`InstructionDataset()`:** loads the pre-generated *Instruction–Input–Response* triplets and tokenizes them into input–target pairs.\n",
        "\n",
        "- We use **`custom_collate_fn()`** to pad and align input–target sequences within each batch.  \n",
        "    - Padding tokens (`<|endoftext|>`) are ignored in the loss function using `ignore_index = -100`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "3aeaf59f",
      "metadata": {},
      "outputs": [],
      "source": [
        "class InstructionDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.data = data\n",
        "        self.encoded_texts = []\n",
        "        for entry in data:\n",
        "            instruction_plus_input = format_input(entry)\n",
        "            response_text = f\"\\n\\n### Response:\\n{entry['Response']}\"\n",
        "            full_text = instruction_plus_input + response_text\n",
        "            self.encoded_texts.append(tokenizer.encode(full_text))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.encoded_texts[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "def format_input(entry):\n",
        "    instruction_text = (\n",
        "        f\"Below is an instruction that describes a task. \"\n",
        "        f\"Write a response that appropriately completes the request.\"\n",
        "        f\"\\n\\n### Instruction:\\n{entry['Instruction']}\"\n",
        "    )\n",
        "    input_text = f\"\\n\\n### Input:\\n{entry['Input']}\" if entry[\"Input\"] else \"\"\n",
        "    return instruction_text + input_text\n",
        "\n",
        "def custom_collate_fn(batch, pad_token_id, ignore_index=-100, allowed_max_length=None, device=\"cpu\"):\n",
        "    # Find the longest sequence in the batch\n",
        "    batch_max_length = max(len(item)+1 for item in batch)\n",
        "    inputs_lst, targets_lst = [], []\n",
        "    for item in batch:\n",
        "        new_item = item.copy()\n",
        "        # Add an <|endoftext|> token\n",
        "        new_item += [pad_token_id]\n",
        "        # Pad sequences to max_length\n",
        "        padded = (\n",
        "            new_item + [pad_token_id] *\n",
        "            (batch_max_length - len(new_item)))\n",
        "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
        "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
        "        # New: Replace all but the first padding tokens in targets by ignore_index\n",
        "        mask = targets == pad_token_id\n",
        "        indices = torch.nonzero(mask).squeeze()\n",
        "        if indices.numel() > 1:\n",
        "            targets[indices[1:]] = ignore_index\n",
        "\n",
        "        # New: Optionally truncate to maximum sequence length\n",
        "        if allowed_max_length is not None:\n",
        "            inputs = inputs[:allowed_max_length]\n",
        "            targets = targets[:allowed_max_length]\n",
        "\n",
        "        inputs_lst.append(inputs)\n",
        "        targets_lst.append(targets)\n",
        "\n",
        "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
        "    targets_tensor = torch.stack(targets_lst).to(device)\n",
        "    return inputs_tensor, targets_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "6cde3f9e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🟦 Regex loaders ready.\n",
            "🟨 GPT-2 loaders ready.\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from functools import partial\n",
        "\n",
        "# === Regex tokenizer model ===\n",
        "regex_pad_token = \"<|endoftext|>\" # \"<|endoftext|>\" is the second last token.\n",
        "regex_pad_id = tokenizer_v2.tokens2ids.get(regex_pad_token, len(tokenizer_v2.tokens2ids) - 1)\n",
        "customized_collate_regex = partial(custom_collate_fn, pad_token_id=regex_pad_id, device=device, allowed_max_length=1024)\n",
        "\n",
        "train_dataset_regex = InstructionDataset(train_set, tokenizer_v2)\n",
        "val_dataset_regex = InstructionDataset(val_set, tokenizer_v2)\n",
        "\n",
        "train_loader_regex = DataLoader(train_dataset_regex, batch_size=8, collate_fn=customized_collate_regex, shuffle=True)\n",
        "val_loader_regex = DataLoader(val_dataset_regex, batch_size=8, collate_fn=customized_collate_regex, shuffle=False)\n",
        "print(\"🟦 Regex loaders ready.\")\n",
        "\n",
        "# === GPT-2 tokenizer model ===\n",
        "gpt2_pad_id = 50256  # <|endoftext|> in GPT-2\n",
        "customized_collate_gpt2 = partial(custom_collate_fn, pad_token_id=gpt2_pad_id, device=device, allowed_max_length=1024)\n",
        "\n",
        "train_dataset_gpt2 = InstructionDataset(train_set, tokenizer_gpt2)\n",
        "val_dataset_gpt2 = InstructionDataset(val_set, tokenizer_gpt2)\n",
        "\n",
        "train_loader_gpt2 = DataLoader(train_dataset_gpt2, batch_size=8, collate_fn=customized_collate_gpt2, shuffle=True)\n",
        "val_loader_gpt2 = DataLoader(val_dataset_gpt2, batch_size=8, collate_fn=customized_collate_gpt2, shuffle=False)\n",
        "print(\"🟨 GPT-2 loaders ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3a2a968",
      "metadata": {},
      "source": [
        "### Step 2: Fine-Tuning the Regex Model\n",
        "\n",
        "- Saves the fine-tuned checkpoint as **`regex_finetuned.pth`**.  \n",
        "- Plots both **training** and **validation loss curves** across epochs.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "35f03270",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep 1 (Step 000000): Train loss 5.905, Val loss 5.877\n",
            "Ep 1 (Step 000050): Train loss 4.965, Val loss 5.016\n",
            "Ep 1 (Step 000100): Train loss 4.876, Val loss 4.870\n",
            "Ep 1 (Step 000150): Train loss 4.776, Val loss 4.810\n",
            "Ep 1 (Step 000200): Train loss 4.698, Val loss 4.774\n",
            "Ep 1 (Step 000250): Train loss 4.639, Val loss 4.726\n",
            "Ep 1 (Step 000300): Train loss 4.674, Val loss 4.694\n",
            "Ep 1 (Step 000350): Train loss 4.632, Val loss 4.672\n",
            "Ep 1 (Step 000400): Train loss 4.553, Val loss 4.631\n",
            "Ep 1 (Step 000450): Train loss 4.522, Val loss 4.610\n",
            "Ep 1 (Step 000500): Train loss 4.543, Val loss 4.583\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to the model to the model to the model to the model. ### Response : The model' s performance of the model' s performance of the model' s performance of the model to the model' s performance of the model is the model' s performance\n",
            "Ep 2 (Step 000550): Train loss 4.470, Val loss 4.567\n",
            "Ep 2 (Step 000600): Train loss 4.468, Val loss 4.546\n",
            "Ep 2 (Step 000650): Train loss 4.498, Val loss 4.533\n",
            "Ep 2 (Step 000700): Train loss 4.478, Val loss 4.525\n",
            "Ep 2 (Step 000750): Train loss 4.478, Val loss 4.493\n",
            "Ep 2 (Step 000800): Train loss 4.357, Val loss 4.474\n",
            "Ep 2 (Step 000850): Train loss 4.354, Val loss 4.460\n",
            "Ep 2 (Step 000900): Train loss 4.308, Val loss 4.443\n",
            "Ep 2 (Step 000950): Train loss 4.337, Val loss 4.410\n",
            "Ep 2 (Step 001000): Train loss 4.306, Val loss 4.407\n",
            "Ep 2 (Step 001050): Train loss 4.289, Val loss 4.388\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model\n",
            "Ep 3 (Step 001100): Train loss 4.237, Val loss 4.371\n",
            "Ep 3 (Step 001150): Train loss 4.209, Val loss 4.359\n",
            "Ep 3 (Step 001200): Train loss 4.156, Val loss 4.339\n",
            "Ep 3 (Step 001250): Train loss 4.216, Val loss 4.339\n",
            "Ep 3 (Step 001300): Train loss 4.189, Val loss 4.324\n",
            "Ep 3 (Step 001350): Train loss 4.163, Val loss 4.311\n",
            "Ep 3 (Step 001400): Train loss 4.133, Val loss 4.280\n",
            "Ep 3 (Step 001450): Train loss 4.086, Val loss 4.266\n",
            "Ep 3 (Step 001500): Train loss 4.058, Val loss 4.260\n",
            "Ep 3 (Step 001550): Train loss 4.117, Val loss 4.241\n",
            "Ep 3 (Step 001600): Train loss 4.076, Val loss 4.246\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to the model to the model to the model. ### Response : The model' s performance of the model' s performance of the model' s performance of the model' s performance of the model' s performance of the model' s performance of the model\n",
            "Ep 4 (Step 001650): Train loss 4.077, Val loss 4.215\n",
            "Ep 4 (Step 001700): Train loss 4.047, Val loss 4.207\n",
            "Ep 4 (Step 001750): Train loss 4.014, Val loss 4.191\n",
            "Ep 4 (Step 001800): Train loss 4.043, Val loss 4.189\n",
            "Ep 4 (Step 001850): Train loss 3.920, Val loss 4.180\n",
            "Ep 4 (Step 001900): Train loss 3.972, Val loss 4.154\n",
            "Ep 4 (Step 001950): Train loss 3.972, Val loss 4.148\n",
            "Ep 4 (Step 002000): Train loss 3.957, Val loss 4.130\n",
            "Ep 4 (Step 002050): Train loss 3.928, Val loss 4.110\n",
            "Ep 4 (Step 002100): Train loss 3.865, Val loss 4.100\n",
            "Ep 4 (Step 002150): Train loss 3.897, Val loss 4.082\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to the model performance of the model performance of the model. ### Response : The model is trained on the model to the model to the model. Specifically, the model is the model to the model to the model to the model to the model. This approach\n",
            "Ep 5 (Step 002200): Train loss 3.832, Val loss 4.069\n",
            "Ep 5 (Step 002250): Train loss 3.824, Val loss 4.054\n",
            "Ep 5 (Step 002300): Train loss 3.843, Val loss 4.027\n",
            "Ep 5 (Step 002350): Train loss 3.821, Val loss 4.021\n",
            "Ep 5 (Step 002400): Train loss 3.790, Val loss 4.015\n",
            "Ep 5 (Step 002450): Train loss 3.798, Val loss 3.992\n",
            "Ep 5 (Step 002500): Train loss 3.723, Val loss 3.982\n",
            "Ep 5 (Step 002550): Train loss 3.796, Val loss 3.968\n",
            "Ep 5 (Step 002600): Train loss 3.709, Val loss 3.968\n",
            "Ep 5 (Step 002650): Train loss 3.713, Val loss 3.956\n",
            "Ep 5 (Step 002700): Train loss 3.684, Val loss 3.954\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to better performance on the model performance on the model performance. ### Response : The passage discusses the model, which is trained on the model performance of the model performance on the model performance of the performance of the model. The model is trained on the model performance\n",
            "Ep 6 (Step 002750): Train loss 3.692, Val loss 3.944\n",
            "Ep 6 (Step 002800): Train loss 3.686, Val loss 3.911\n",
            "Ep 6 (Step 002850): Train loss 3.655, Val loss 3.922\n",
            "Ep 6 (Step 002900): Train loss 3.640, Val loss 3.892\n",
            "Ep 6 (Step 002950): Train loss 3.603, Val loss 3.888\n",
            "Ep 6 (Step 003000): Train loss 3.635, Val loss 3.878\n",
            "Ep 6 (Step 003050): Train loss 3.621, Val loss 3.878\n",
            "Ep 6 (Step 003100): Train loss 3.572, Val loss 3.853\n",
            "Ep 6 (Step 003150): Train loss 3.548, Val loss 3.846\n",
            "Ep 6 (Step 003200): Train loss 3.525, Val loss 3.847\n",
            "Ep 6 (Step 003250): Train loss 3.545, Val loss 3.817\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to the model performance on the model performance on the model. ### Response : The passage discusses the performance of the performance of the performance of the performance of the model, which is trained on the performance of the model. The model is trained on the model,\n",
            "Ep 7 (Step 003300): Train loss 3.536, Val loss 3.828\n",
            "Ep 7 (Step 003350): Train loss 3.556, Val loss 3.820\n",
            "Ep 7 (Step 003400): Train loss 3.543, Val loss 3.809\n",
            "Ep 7 (Step 003450): Train loss 3.507, Val loss 3.805\n",
            "Ep 7 (Step 003500): Train loss 3.504, Val loss 3.783\n",
            "Ep 7 (Step 003550): Train loss 3.438, Val loss 3.783\n",
            "Ep 7 (Step 003600): Train loss 3.441, Val loss 3.767\n",
            "Ep 7 (Step 003650): Train loss 3.476, Val loss 3.751\n",
            "Ep 7 (Step 003700): Train loss 3.403, Val loss 3.760\n",
            "Ep 7 (Step 003750): Train loss 3.431, Val loss 3.737\n",
            "Ep 7 (Step 003800): Train loss 3.371, Val loss 3.717\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to the model performance on the performance of the model size. ### Response : The study compares the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the model,\n",
            "Ep 8 (Step 003850): Train loss 3.316, Val loss 3.686\n",
            "Ep 8 (Step 003900): Train loss 3.369, Val loss 3.708\n",
            "Ep 8 (Step 003950): Train loss 3.370, Val loss 3.718\n",
            "Ep 8 (Step 004000): Train loss 3.360, Val loss 3.692\n",
            "Ep 8 (Step 004050): Train loss 3.370, Val loss 3.689\n",
            "Ep 8 (Step 004100): Train loss 3.415, Val loss 3.682\n",
            "Ep 8 (Step 004150): Train loss 3.293, Val loss 3.633\n",
            "Ep 8 (Step 004200): Train loss 3.408, Val loss 3.643\n",
            "Ep 8 (Step 004250): Train loss 3.297, Val loss 3.657\n",
            "Ep 8 (Step 004300): Train loss 3.256, Val loss 3.629\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to the model size, and the model size increases from the model size. ### Response : The study compares the performance on a model, which is trained on a larger model, which is trained on a larger model, which is trained on a larger model.\n",
            "Ep 9 (Step 004350): Train loss 3.312, Val loss 3.622\n",
            "Ep 9 (Step 004400): Train loss 3.262, Val loss 3.632\n",
            "Ep 9 (Step 004450): Train loss 3.272, Val loss 3.624\n",
            "Ep 9 (Step 004500): Train loss 3.293, Val loss 3.598\n",
            "Ep 9 (Step 004550): Train loss 3.200, Val loss 3.586\n",
            "Ep 9 (Step 004600): Train loss 3.263, Val loss 3.594\n",
            "Ep 9 (Step 004650): Train loss 3.228, Val loss 3.577\n",
            "Ep 9 (Step 004700): Train loss 3.205, Val loss 3.564\n",
            "Ep 9 (Step 004750): Train loss 3.194, Val loss 3.567\n",
            "Ep 9 (Step 004800): Train loss 3.153, Val loss 3.547\n",
            "Ep 9 (Step 004850): Train loss 3.153, Val loss 3.522\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to the model can be more than the training data. ### Response : The study compares the performance on the 7B model, which is trained on the 7B model, with 1. 5 billion parameters, and 1.( Note : The original text was slightly adjusted\n",
            "Ep 10 (Step 004900): Train loss 3.109, Val loss 3.525\n",
            "Ep 10 (Step 004950): Train loss 3.146, Val loss 3.518\n",
            "Ep 10 (Step 005000): Train loss 3.104, Val loss 3.513\n",
            "Ep 10 (Step 005050): Train loss 3.014, Val loss 3.507\n",
            "Ep 10 (Step 005100): Train loss 3.106, Val loss 3.491\n",
            "Ep 10 (Step 005150): Train loss 3.043, Val loss 3.486\n",
            "Ep 10 (Step 005200): Train loss 3.057, Val loss 3.488\n",
            "Ep 10 (Step 005250): Train loss 3.054, Val loss 3.477\n",
            "Ep 10 (Step 005300): Train loss 3.001, Val loss 3.466\n",
            "Ep 10 (Step 005350): Train loss 3.001, Val loss 3.455\n",
            "Ep 10 (Step 005400): Train loss 3.052, Val loss 3.448\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to the model size increases from the training data, which is more than the model size increases from the training data. ### Response : The study compares the scaling laws for training data contamination, which is trained on a larger model, which is trained on a larger\n",
            "Ep 11 (Step 005450): Train loss 3.044, Val loss 3.449\n",
            "Ep 11 (Step 005500): Train loss 2.998, Val loss 3.428\n",
            "Ep 11 (Step 005550): Train loss 2.909, Val loss 3.429\n",
            "Ep 11 (Step 005600): Train loss 2.967, Val loss 3.428\n",
            "Ep 11 (Step 005650): Train loss 2.962, Val loss 3.425\n",
            "Ep 11 (Step 005700): Train loss 2.983, Val loss 3.420\n",
            "Ep 11 (Step 005750): Train loss 2.988, Val loss 3.398\n",
            "Ep 11 (Step 005800): Train loss 2.903, Val loss 3.381\n",
            "Ep 11 (Step 005850): Train loss 2.903, Val loss 3.378\n",
            "Ep 11 (Step 005900): Train loss 2.899, Val loss 3.379\n",
            "Ep 11 (Step 005950): Train loss 2.881, Val loss 3.355\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to the model size increases from the training data. ### Response : The study compares the performance of the performance of the larger models, which is trained on a smaller model, with a larger model, and a smaller model, with a larger batch size of 8\n",
            "Ep 12 (Step 006000): Train loss 2.905, Val loss 3.366\n",
            "Ep 12 (Step 006050): Train loss 2.905, Val loss 3.358\n",
            "Ep 12 (Step 006100): Train loss 2.867, Val loss 3.354\n",
            "Ep 12 (Step 006150): Train loss 2.847, Val loss 3.330\n",
            "Ep 12 (Step 006200): Train loss 2.850, Val loss 3.336\n",
            "Ep 12 (Step 006250): Train loss 2.852, Val loss 3.317\n",
            "Ep 12 (Step 006300): Train loss 2.809, Val loss 3.323\n",
            "Ep 12 (Step 006350): Train loss 2.896, Val loss 3.327\n",
            "Ep 12 (Step 006400): Train loss 2.796, Val loss 3.335\n",
            "Ep 12 (Step 006450): Train loss 2.750, Val loss 3.298\n",
            "Ep 12 (Step 006500): Train loss 2.755, Val loss 3.287\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to the model size increases from the training data. ### Response : The study compares the scaling laws for training data scaling laws, which are trained on a smaller model, which is trained on a larger model size of 8 billion parameters, with 1 billion parameters,\n",
            "Ep 13 (Step 006550): Train loss 2.740, Val loss 3.270\n",
            "Ep 13 (Step 006600): Train loss 2.732, Val loss 3.268\n",
            "Ep 13 (Step 006650): Train loss 2.722, Val loss 3.272\n",
            "Ep 13 (Step 006700): Train loss 2.749, Val loss 3.277\n",
            "Ep 13 (Step 006750): Train loss 2.761, Val loss 3.263\n",
            "Ep 13 (Step 006800): Train loss 2.709, Val loss 3.261\n",
            "Ep 13 (Step 006850): Train loss 2.699, Val loss 3.252\n",
            "Ep 13 (Step 006900): Train loss 2.710, Val loss 3.263\n",
            "Ep 13 (Step 006950): Train loss 2.645, Val loss 3.253\n",
            "Ep 13 (Step 007000): Train loss 2.647, Val loss 3.221\n",
            "Ep 13 (Step 007050): Train loss 2.613, Val loss 3.218\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to the model size increases from 8B model size. ### Response : The study compares the scaling laws for training data scaling laws, particularly for the scaling model size, with a larger model size of 1. 8B model size increases from 1. 1. 1.\n",
            "Ep 14 (Step 007100): Train loss 2.641, Val loss 3.202\n",
            "Ep 14 (Step 007150): Train loss 2.658, Val loss 3.197\n",
            "Ep 14 (Step 007200): Train loss 2.644, Val loss 3.216\n",
            "Ep 14 (Step 007250): Train loss 2.638, Val loss 3.204\n",
            "Ep 14 (Step 007300): Train loss 2.629, Val loss 3.194\n",
            "Ep 14 (Step 007350): Train loss 2.533, Val loss 3.170\n",
            "Ep 14 (Step 007400): Train loss 2.618, Val loss 3.190\n",
            "Ep 14 (Step 007450): Train loss 2.559, Val loss 3.164\n",
            "Ep 14 (Step 007500): Train loss 2.547, Val loss 3.149\n",
            "Ep 14 (Step 007550): Train loss 2.561, Val loss 3.156\n",
            "Ep 14 (Step 007600): Train loss 2.487, Val loss 3.147\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to 540B model size, and is better than the training compute budget, despite having a smaller model size increases from 8B to 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 1\n",
            "Ep 15 (Step 007650): Train loss 2.551, Val loss 3.120\n",
            "Ep 15 (Step 007700): Train loss 2.524, Val loss 3.136\n",
            "Ep 15 (Step 007750): Train loss 2.524, Val loss 3.121\n",
            "Ep 15 (Step 007800): Train loss 2.505, Val loss 3.099\n",
            "Ep 15 (Step 007850): Train loss 2.533, Val loss 3.111\n",
            "Ep 15 (Step 007900): Train loss 2.503, Val loss 3.116\n",
            "Ep 15 (Step 007950): Train loss 2.473, Val loss 3.108\n",
            "Ep 15 (Step 008000): Train loss 2.473, Val loss 3.083\n",
            "Ep 15 (Step 008050): Train loss 2.463, Val loss 3.094\n",
            "Ep 15 (Step 008100): Train loss 2.454, Val loss 3.091\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to the training data mixture of the training data, the model is more computationally efficient. ### Response : The study compares the scaling model on a larger model on a larger model, showing that the larger model size increases, and the scaling factor model size, and\n",
            "Ep 16 (Step 008150): Train loss 2.469, Val loss 3.066\n",
            "Ep 16 (Step 008200): Train loss 2.421, Val loss 3.065\n",
            "Ep 16 (Step 008250): Train loss 2.405, Val loss 3.065\n",
            "Ep 16 (Step 008300): Train loss 2.366, Val loss 3.049\n",
            "Ep 16 (Step 008350): Train loss 2.441, Val loss 3.051\n",
            "Ep 16 (Step 008400): Train loss 2.394, Val loss 3.042\n",
            "Ep 16 (Step 008450): Train loss 2.379, Val loss 3.048\n",
            "Ep 16 (Step 008500): Train loss 2.391, Val loss 3.036\n",
            "Ep 16 (Step 008550): Train loss 2.350, Val loss 3.038\n",
            "Ep 16 (Step 008600): Train loss 2.340, Val loss 3.015\n",
            "Ep 16 (Step 008650): Train loss 2.335, Val loss 3.011\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to the larger models trained on a smaller model, and the larger model trained on a smaller model, and a larger model trained on 2. 0. 0. 0. 0. 0. 1. 0, 1. 0 series, and 1. 0\n",
            "Ep 17 (Step 008700): Train loss 2.387, Val loss 2.989\n",
            "Ep 17 (Step 008750): Train loss 2.310, Val loss 2.996\n",
            "Ep 17 (Step 008800): Train loss 2.266, Val loss 2.998\n",
            "Ep 17 (Step 008850): Train loss 2.287, Val loss 2.989\n",
            "Ep 17 (Step 008900): Train loss 2.302, Val loss 2.980\n",
            "Ep 17 (Step 008950): Train loss 2.266, Val loss 2.990\n",
            "Ep 17 (Step 009000): Train loss 2.290, Val loss 2.969\n",
            "Ep 17 (Step 009050): Train loss 2.273, Val loss 2.976\n",
            "Ep 17 (Step 009100): Train loss 2.250, Val loss 2.976\n",
            "Ep 17 (Step 009150): Train loss 2.247, Val loss 2.965\n",
            "Ep 17 (Step 009200): Train loss 2.230, Val loss 2.941\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to the training data mixture of the training data, the model size increases from 8B to 2. 0. 1. 1. 1. 0. 1. 1. 0 Pro model size, and 1. 1. 0 Pro outperforms 1. 0. 0\n",
            "Ep 18 (Step 009250): Train loss 2.250, Val loss 2.938\n",
            "Ep 18 (Step 009300): Train loss 2.217, Val loss 2.927\n",
            "Ep 18 (Step 009350): Train loss 2.223, Val loss 2.928\n",
            "Ep 18 (Step 009400): Train loss 2.167, Val loss 2.916\n",
            "Ep 18 (Step 009450): Train loss 2.176, Val loss 2.929\n",
            "Ep 18 (Step 009500): Train loss 2.158, Val loss 2.897\n",
            "Ep 18 (Step 009550): Train loss 2.150, Val loss 2.891\n",
            "Ep 18 (Step 009600): Train loss 2.215, Val loss 2.906\n",
            "Ep 18 (Step 009650): Train loss 2.133, Val loss 2.895\n",
            "Ep 18 (Step 009700): Train loss 2.125, Val loss 2.884\n",
            "Ep 18 (Step 009750): Train loss 2.153, Val loss 2.891\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to the larger models of the larger models, even when scaling up to 1. 7B models are more than the 7B model size. ### Response : The study compares the scaling laws for larger models with larger models, showing that larger models like 8B parameters, showing\n",
            "Ep 19 (Step 009800): Train loss 2.077, Val loss 2.891\n",
            "Ep 19 (Step 009850): Train loss 2.172, Val loss 2.888\n",
            "Ep 19 (Step 009900): Train loss 2.126, Val loss 2.878\n",
            "Ep 19 (Step 009950): Train loss 2.098, Val loss 2.851\n",
            "Ep 19 (Step 010000): Train loss 2.133, Val loss 2.863\n",
            "Ep 19 (Step 010050): Train loss 2.104, Val loss 2.861\n",
            "Ep 19 (Step 010100): Train loss 2.075, Val loss 2.863\n",
            "Ep 19 (Step 010150): Train loss 2.056, Val loss 2.843\n",
            "Ep 19 (Step 010200): Train loss 2.073, Val loss 2.846\n",
            "Ep 19 (Step 010250): Train loss 2.028, Val loss 2.833\n",
            "Ep 19 (Step 010300): Train loss 2.030, Val loss 2.815\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to a similar trend observed in Table 2. ### Response : The study compares the scaling laws for training compute budget, showing that the optimal model size increases, and training loss can be trained on a larger batch size of 8 × 1022 FLOPs. The training loss\n",
            "Ep 20 (Step 010350): Train loss 2.016, Val loss 2.817\n",
            "Ep 20 (Step 010400): Train loss 2.032, Val loss 2.812\n",
            "Ep 20 (Step 010450): Train loss 2.008, Val loss 2.785\n",
            "Ep 20 (Step 010500): Train loss 2.026, Val loss 2.782\n",
            "Ep 20 (Step 010550): Train loss 1.955, Val loss 2.776\n",
            "Ep 20 (Step 010600): Train loss 1.976, Val loss 2.784\n",
            "Ep 20 (Step 010650): Train loss 1.994, Val loss 2.794\n",
            "Ep 20 (Step 010700): Train loss 1.984, Val loss 2.788\n",
            "Ep 20 (Step 010750): Train loss 1.979, Val loss 2.788\n",
            "Ep 20 (Step 010800): Train loss 1.902, Val loss 2.772\n",
            "Ep 20 (Step 010850): Train loss 1.965, Val loss 2.775\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to 540B, there is a slight drop in performance for the 540B model, although the 540B model size is not yet it is due to the larger model size. ### Response : The study compares the scaling laws for model size, showing that larger models with larger\n",
            "Ep 21 (Step 010900): Train loss 1.911, Val loss 2.766\n",
            "Ep 21 (Step 010950): Train loss 1.938, Val loss 2.756\n",
            "Ep 21 (Step 011000): Train loss 1.871, Val loss 2.765\n",
            "Ep 21 (Step 011050): Train loss 1.895, Val loss 2.760\n",
            "Ep 21 (Step 011100): Train loss 1.917, Val loss 2.760\n",
            "Ep 21 (Step 011150): Train loss 1.914, Val loss 2.759\n",
            "Ep 21 (Step 011200): Train loss 1.898, Val loss 2.755\n",
            "Ep 21 (Step 011250): Train loss 1.864, Val loss 2.737\n",
            "Ep 21 (Step 011300): Train loss 1.933, Val loss 2.737\n",
            "Ep 21 (Step 011350): Train loss 1.861, Val loss 2.705\n",
            "Ep 21 (Step 011400): Train loss 1.817, Val loss 2.696\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to be trained with a larger model, although it is more computationally expensive for the training loss is not trained on downstream tasks. ### Response : The study found that the scaling laws for training loss and model sizes are trained on a larger model, with a larger\n",
            "Ep 22 (Step 011450): Train loss 1.814, Val loss 2.705\n",
            "Ep 22 (Step 011500): Train loss 1.825, Val loss 2.701\n",
            "Ep 22 (Step 011550): Train loss 1.829, Val loss 2.702\n",
            "Ep 22 (Step 011600): Train loss 1.760, Val loss 2.682\n",
            "Ep 22 (Step 011650): Train loss 1.830, Val loss 2.689\n",
            "Ep 22 (Step 011700): Train loss 1.808, Val loss 2.683\n",
            "Ep 22 (Step 011750): Train loss 1.809, Val loss 2.676\n",
            "Ep 22 (Step 011800): Train loss 1.790, Val loss 2.668\n",
            "Ep 22 (Step 011850): Train loss 1.812, Val loss 2.673\n",
            "Ep 22 (Step 011900): Train loss 1.808, Val loss 2.660\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to be attributed to the training data, it is not always lead to a smaller model for downstream tasks but also potentially due to the larger model size. ### Response : The study compares the scaling laws for language models like Chinchilla and 540B, showing that larger models\n",
            "Ep 23 (Step 011950): Train loss 1.769, Val loss 2.660\n",
            "Ep 23 (Step 012000): Train loss 1.771, Val loss 2.650\n",
            "Ep 23 (Step 012050): Train loss 1.752, Val loss 2.652\n",
            "Ep 23 (Step 012100): Train loss 1.747, Val loss 2.641\n",
            "Ep 23 (Step 012150): Train loss 1.752, Val loss 2.639\n",
            "Ep 23 (Step 012200): Train loss 1.713, Val loss 2.634\n",
            "Ep 23 (Step 012250): Train loss 1.714, Val loss 2.631\n",
            "Ep 23 (Step 012300): Train loss 1.677, Val loss 2.638\n",
            "Ep 23 (Step 012350): Train loss 1.686, Val loss 2.620\n",
            "Ep 23 (Step 012400): Train loss 1.679, Val loss 2.631\n",
            "Ep 23 (Step 012450): Train loss 1.681, Val loss 2.608\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to be attributed to a similar trend of model size. ### Response : The study compares the scaling laws for different model sizes( 8B and 1. 8B, showing that 1. 8B and 1. 7B parameters) with a 1. 3B parameter model, showing\n",
            "Ep 24 (Step 012500): Train loss 1.648, Val loss 2.601\n",
            "Ep 24 (Step 012550): Train loss 1.688, Val loss 2.610\n",
            "Ep 24 (Step 012600): Train loss 1.675, Val loss 2.578\n",
            "Ep 24 (Step 012650): Train loss 1.696, Val loss 2.598\n",
            "Ep 24 (Step 012700): Train loss 1.662, Val loss 2.594\n",
            "Ep 24 (Step 012750): Train loss 1.648, Val loss 2.593\n",
            "Ep 24 (Step 012800): Train loss 1.628, Val loss 2.593\n",
            "Ep 24 (Step 012850): Train loss 1.626, Val loss 2.573\n",
            "Ep 24 (Step 012900): Train loss 1.652, Val loss 2.580\n",
            "Ep 24 (Step 012950): Train loss 1.624, Val loss 2.574\n",
            "Ep 24 (Step 013000): Train loss 1.574, Val loss 2.573\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to be trained on downstream tasks( e. g., 8B and 3), as a similar model size. ### Response : The study compares Llama 2 with Llama 2 and 8B model size on various benchmarks, showing that larger models like Mixtral and Mixtral outperforms\n",
            "Ep 25 (Step 013050): Train loss 1.606, Val loss 2.556\n",
            "Ep 25 (Step 013100): Train loss 1.587, Val loss 2.567\n",
            "Ep 25 (Step 013150): Train loss 1.597, Val loss 2.554\n",
            "Ep 25 (Step 013200): Train loss 1.567, Val loss 2.540\n",
            "Ep 25 (Step 013250): Train loss 1.585, Val loss 2.551\n",
            "Ep 25 (Step 013300): Train loss 1.579, Val loss 2.545\n",
            "Ep 25 (Step 013350): Train loss 1.592, Val loss 2.553\n",
            "Ep 25 (Step 013400): Train loss 1.534, Val loss 2.522\n",
            "Ep 25 (Step 013450): Train loss 1.538, Val loss 2.535\n",
            "Ep 25 (Step 013500): Train loss 1.512, Val loss 2.508\n",
            "Ep 25 (Step 013550): Train loss 1.545, Val loss 2.511\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to be attributed to the training loss in the same model size. ### Response : The study compares Llama 2 models with larger models with larger Llama 1 models like 8B and 8B and 8B models, showing that larger models outperform larger sizes, showing that larger models like\n",
            "Ep 26 (Step 013600): Train loss 1.483, Val loss 2.507\n",
            "Ep 26 (Step 013650): Train loss 1.551, Val loss 2.505\n",
            "Ep 26 (Step 013700): Train loss 1.501, Val loss 2.503\n",
            "Ep 26 (Step 013750): Train loss 1.508, Val loss 2.501\n",
            "Ep 26 (Step 013800): Train loss 1.520, Val loss 2.495\n",
            "Ep 26 (Step 013850): Train loss 1.475, Val loss 2.514\n",
            "Ep 26 (Step 013900): Train loss 1.500, Val loss 2.495\n",
            "Ep 26 (Step 013950): Train loss 1.451, Val loss 2.480\n",
            "Ep 26 (Step 014000): Train loss 1.470, Val loss 2.483\n",
            "Ep 26 (Step 014050): Train loss 1.469, Val loss 2.492\n",
            "Ep 26 (Step 014100): Train loss 1.437, Val loss 2.483\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to be attributed to the optimal model size. ### Response : The study compares Llama 1 models with similar sizes of similar sizes( 8B, 8B, 8B, and 8B, 8B, 8B, 8B, 8B, 8B, 8B, and 8B parameters),\n",
            "Ep 27 (Step 014150): Train loss 1.483, Val loss 2.475\n",
            "Ep 27 (Step 014200): Train loss 1.458, Val loss 2.467\n",
            "Ep 27 (Step 014250): Train loss 1.422, Val loss 2.460\n",
            "Ep 27 (Step 014300): Train loss 1.410, Val loss 2.454\n",
            "Ep 27 (Step 014350): Train loss 1.401, Val loss 2.450\n",
            "Ep 27 (Step 014400): Train loss 1.446, Val loss 2.443\n",
            "Ep 27 (Step 014450): Train loss 1.417, Val loss 2.446\n",
            "Ep 27 (Step 014500): Train loss 1.369, Val loss 2.448\n",
            "Ep 27 (Step 014550): Train loss 1.399, Val loss 2.454\n",
            "Ep 27 (Step 014600): Train loss 1.419, Val loss 2.454\n",
            "Ep 27 (Step 014650): Train loss 1.395, Val loss 2.438\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to be attributed to the optimal model size, the optimal model size does not show that the optimal model size does not show that the optimal model size does not have the optimal model size. ### Response : The study compares the performance of smaller models with larger sizes\n",
            "Ep 28 (Step 014700): Train loss 1.332, Val loss 2.434\n",
            "Ep 28 (Step 014750): Train loss 1.403, Val loss 2.431\n",
            "Ep 28 (Step 014800): Train loss 1.405, Val loss 2.432\n",
            "Ep 28 (Step 014850): Train loss 1.328, Val loss 2.413\n",
            "Ep 28 (Step 014900): Train loss 1.355, Val loss 2.422\n",
            "Ep 28 (Step 014950): Train loss 1.368, Val loss 2.416\n",
            "Ep 28 (Step 015000): Train loss 1.313, Val loss 2.407\n",
            "Ep 28 (Step 015050): Train loss 1.327, Val loss 2.412\n",
            "Ep 28 (Step 015100): Train loss 1.319, Val loss 2.413\n",
            "Ep 28 (Step 015150): Train loss 1.323, Val loss 2.401\n",
            "Ep 28 (Step 015200): Train loss 1.273, Val loss 2.401\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to be attributed to a similar model size, and that the 540B model size is trained for a similar size of the 8B model. ### Response : The study compares the performance of smaller models with larger models like 8B and 8B, showing that larger models trained on\n",
            "Ep 29 (Step 015250): Train loss 1.326, Val loss 2.394\n",
            "Ep 29 (Step 015300): Train loss 1.240, Val loss 2.393\n",
            "Ep 29 (Step 015350): Train loss 1.287, Val loss 2.396\n",
            "Ep 29 (Step 015400): Train loss 1.291, Val loss 2.384\n",
            "Ep 29 (Step 015450): Train loss 1.252, Val loss 2.380\n",
            "Ep 29 (Step 015500): Train loss 1.291, Val loss 2.385\n",
            "Ep 29 (Step 015550): Train loss 1.277, Val loss 2.379\n",
            "Ep 29 (Step 015600): Train loss 1.251, Val loss 2.358\n",
            "Ep 29 (Step 015650): Train loss 1.237, Val loss 2.365\n",
            "Ep 29 (Step 015700): Train loss 1.267, Val loss 2.369\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to be attributed to the optimal model size, and that it is more computationally optimal model size. ### Response : The study found that scaling laws exhibit optimal performance varies between model size and optimal model size, showing that larger models trained on 2. 8B and 1\n",
            "Ep 30 (Step 015750): Train loss 1.229, Val loss 2.345\n",
            "Ep 30 (Step 015800): Train loss 1.239, Val loss 2.369\n",
            "Ep 30 (Step 015850): Train loss 1.258, Val loss 2.369\n",
            "Ep 30 (Step 015900): Train loss 1.244, Val loss 2.358\n",
            "Ep 30 (Step 015950): Train loss 1.232, Val loss 2.353\n",
            "Ep 30 (Step 016000): Train loss 1.216, Val loss 2.343\n",
            "Ep 30 (Step 016050): Train loss 1.189, Val loss 2.346\n",
            "Ep 30 (Step 016100): Train loss 1.249, Val loss 2.349\n",
            "Ep 30 (Step 016150): Train loss 1.212, Val loss 2.347\n",
            "Ep 30 (Step 016200): Train loss 1.193, Val loss 2.331\n",
            "Ep 30 (Step 016250): Train loss 1.186, Val loss 2.326\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to be attributed to the training loss and training loss, as we can observe that the 8B model size is not the optimal model size, and the 8B model size is trained on 8B and 1. 8B total. 8B, the 8B model, the 8B model\n",
            "Ep 31 (Step 016300): Train loss 1.186, Val loss 2.324\n",
            "Ep 31 (Step 016350): Train loss 1.174, Val loss 2.325\n",
            "Ep 31 (Step 016400): Train loss 1.165, Val loss 2.326\n",
            "Ep 31 (Step 016450): Train loss 1.146, Val loss 2.316\n",
            "Ep 31 (Step 016500): Train loss 1.141, Val loss 2.327\n",
            "Ep 31 (Step 016550): Train loss 1.132, Val loss 2.318\n",
            "Ep 31 (Step 016600): Train loss 1.180, Val loss 2.332\n",
            "Ep 31 (Step 016650): Train loss 1.156, Val loss 2.309\n",
            "Ep 31 (Step 016700): Train loss 1.139, Val loss 2.299\n",
            "Ep 31 (Step 016750): Train loss 1.151, Val loss 2.308\n",
            "Ep 31 (Step 016800): Train loss 1.151, Val loss 2.306\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to be trained on downstream tasks( e. g., 8B and 3), they show that it is not yet it is not trained on a very similar scale, it is not the same scale model size. ### Response : The study compares Llama 2\n",
            "Ep 32 (Step 016850): Train loss 1.071, Val loss 2.288\n",
            "Ep 32 (Step 016900): Train loss 1.115, Val loss 2.298\n",
            "Ep 32 (Step 016950): Train loss 1.098, Val loss 2.295\n",
            "Ep 32 (Step 017000): Train loss 1.098, Val loss 2.286\n",
            "Ep 32 (Step 017050): Train loss 1.106, Val loss 2.286\n",
            "Ep 32 (Step 017100): Train loss 1.061, Val loss 2.288\n",
            "Ep 32 (Step 017150): Train loss 1.074, Val loss 2.291\n",
            "Ep 32 (Step 017200): Train loss 1.093, Val loss 2.296\n",
            "Ep 32 (Step 017250): Train loss 1.063, Val loss 2.278\n",
            "Ep 32 (Step 017300): Train loss 1.101, Val loss 2.267\n",
            "Ep 32 (Step 017350): Train loss 1.075, Val loss 2.271\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to be in most evaluation metrics can be attributed to the scaling law, the 7B model, despite having smaller 8B model size. ### Response : The study compares the scaling laws for training loss and model sizes, showing that increasing model size, and 1. 7B\n",
            "Ep 33 (Step 017400): Train loss 1.029, Val loss 2.252\n",
            "Ep 33 (Step 017450): Train loss 1.058, Val loss 2.258\n",
            "Ep 33 (Step 017500): Train loss 1.059, Val loss 2.261\n",
            "Ep 33 (Step 017550): Train loss 1.023, Val loss 2.267\n",
            "Ep 33 (Step 017600): Train loss 1.022, Val loss 2.256\n",
            "Ep 33 (Step 017650): Train loss 1.057, Val loss 2.260\n",
            "Ep 33 (Step 017700): Train loss 1.084, Val loss 2.263\n",
            "Ep 33 (Step 017750): Train loss 1.027, Val loss 2.247\n",
            "Ep 33 (Step 017800): Train loss 1.056, Val loss 2.249\n",
            "Ep 33 (Step 017850): Train loss 1.012, Val loss 2.254\n",
            "Ep 33 (Step 017900): Train loss 1.042, Val loss 2.235\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to be attributed to a similar model size, the fact that it is more likely close to the optimal model size. ### Response : The study found that increasing parameter sizes( 8B and 1. 8B parameters) can outperform larger models with larger sizes( 1.\n",
            "Ep 34 (Step 017950): Train loss 0.974, Val loss 2.242\n",
            "Ep 34 (Step 018000): Train loss 1.016, Val loss 2.240\n",
            "Ep 34 (Step 018050): Train loss 1.033, Val loss 2.246\n",
            "Ep 34 (Step 018100): Train loss 0.983, Val loss 2.231\n",
            "Ep 34 (Step 018150): Train loss 0.976, Val loss 2.225\n",
            "Ep 34 (Step 018200): Train loss 0.985, Val loss 2.247\n",
            "Ep 34 (Step 018250): Train loss 0.945, Val loss 2.218\n",
            "Ep 34 (Step 018300): Train loss 0.970, Val loss 2.240\n",
            "Ep 34 (Step 018350): Train loss 0.957, Val loss 2.238\n",
            "Ep 34 (Step 018400): Train loss 0.941, Val loss 2.215\n",
            "Ep 34 (Step 018450): Train loss 0.960, Val loss 2.210\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to be in the training data. ### Response : The study compares Llama 2 and 8B models with 8B and 8B models, showing that larger models outperform larger models of similar size, showing that larger models of similar size, and 8B, and 8B parameters, showing\n",
            "Ep 35 (Step 018500): Train loss 0.934, Val loss 2.218\n",
            "Ep 35 (Step 018550): Train loss 0.946, Val loss 2.214\n",
            "Ep 35 (Step 018600): Train loss 0.953, Val loss 2.213\n",
            "Ep 35 (Step 018650): Train loss 0.927, Val loss 2.215\n",
            "Ep 35 (Step 018700): Train loss 0.924, Val loss 2.220\n",
            "Ep 35 (Step 018750): Train loss 0.914, Val loss 2.212\n",
            "Ep 35 (Step 018800): Train loss 0.928, Val loss 2.200\n",
            "Ep 35 (Step 018850): Train loss 0.956, Val loss 2.216\n",
            "Ep 35 (Step 018900): Train loss 0.920, Val loss 2.194\n",
            "Ep 35 (Step 018950): Train loss 0.931, Val loss 2.190\n",
            "Ep 35 (Step 019000): Train loss 0.913, Val loss 2.196\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The study compares Llama 2 models with similar sizes( 8B and 8B) and 8B models, showing that larger models trained on downstream tasks, showing that larger models of similar size, showing that larger models trained on downstream tasks. The study shows\n",
            "Ep 36 (Step 019050): Train loss 0.892, Val loss 2.182\n",
            "Ep 36 (Step 019100): Train loss 0.919, Val loss 2.199\n",
            "Ep 36 (Step 019150): Train loss 0.879, Val loss 2.199\n",
            "Ep 36 (Step 019200): Train loss 0.913, Val loss 2.191\n",
            "Ep 36 (Step 019250): Train loss 0.887, Val loss 2.208\n",
            "Ep 36 (Step 019300): Train loss 0.853, Val loss 2.189\n",
            "Ep 36 (Step 019350): Train loss 0.850, Val loss 2.182\n",
            "Ep 36 (Step 019400): Train loss 0.836, Val loss 2.188\n",
            "Ep 36 (Step 019450): Train loss 0.882, Val loss 2.174\n",
            "Ep 36 (Step 019500): Train loss 0.872, Val loss 2.182\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The study compares Llama 1 model size for various models, showing that larger models like Llama 1. 5 and 1 models, showing that while larger sizes are trained on a fixed version of the 1. 3B model, showing that while larger models\n",
            "Ep 37 (Step 019550): Train loss 0.855, Val loss 2.170\n",
            "Ep 37 (Step 019600): Train loss 0.851, Val loss 2.170\n",
            "Ep 37 (Step 019650): Train loss 0.837, Val loss 2.149\n",
            "Ep 37 (Step 019700): Train loss 0.855, Val loss 2.172\n",
            "Ep 37 (Step 019750): Train loss 0.827, Val loss 2.153\n",
            "Ep 37 (Step 019800): Train loss 0.847, Val loss 2.159\n",
            "Ep 37 (Step 019850): Train loss 0.813, Val loss 2.152\n",
            "Ep 37 (Step 019900): Train loss 0.821, Val loss 2.156\n",
            "Ep 37 (Step 019950): Train loss 0.840, Val loss 2.172\n",
            "Ep 37 (Step 020000): Train loss 0.788, Val loss 2.144\n",
            "Ep 37 (Step 020050): Train loss 0.829, Val loss 2.142\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to obtain strong performance, the smaller model size does not come with more downstream tasks. ### Response : The study shows that training loss with larger models trained on a larger dataset, showing that larger models of similar size, and the 8B model, and 8B model\n",
            "Ep 38 (Step 020100): Train loss 0.788, Val loss 2.145\n",
            "Ep 38 (Step 020150): Train loss 0.789, Val loss 2.164\n",
            "Ep 38 (Step 020200): Train loss 0.813, Val loss 2.156\n",
            "Ep 38 (Step 020250): Train loss 0.792, Val loss 2.158\n",
            "Ep 38 (Step 020300): Train loss 0.800, Val loss 2.159\n",
            "Ep 38 (Step 020350): Train loss 0.771, Val loss 2.139\n",
            "Ep 38 (Step 020400): Train loss 0.762, Val loss 2.138\n",
            "Ep 38 (Step 020450): Train loss 0.774, Val loss 2.139\n",
            "Ep 38 (Step 020500): Train loss 0.784, Val loss 2.152\n",
            "Ep 38 (Step 020550): Train loss 0.765, Val loss 2.128\n",
            "Ep 38 (Step 020600): Train loss 0.744, Val loss 2.128\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The study compares the optimal model size for optimal model sizes( N and 1. 8B parameters), showing that increasing model size increases, and training loss becomes more focused on downstream tasks, showing that scaling laws can sometimes exhibit optimal performance.\n",
            "Ep 39 (Step 020650): Train loss 0.739, Val loss 2.133\n",
            "Ep 39 (Step 020700): Train loss 0.756, Val loss 2.126\n",
            "Ep 39 (Step 020750): Train loss 0.757, Val loss 2.148\n",
            "Ep 39 (Step 020800): Train loss 0.756, Val loss 2.142\n",
            "Ep 39 (Step 020850): Train loss 0.727, Val loss 2.128\n",
            "Ep 39 (Step 020900): Train loss 0.735, Val loss 2.132\n",
            "Ep 39 (Step 020950): Train loss 0.746, Val loss 2.116\n",
            "Ep 39 (Step 021000): Train loss 0.728, Val loss 2.120\n",
            "Ep 39 (Step 021050): Train loss 0.717, Val loss 2.130\n",
            "Ep 39 (Step 021100): Train loss 0.732, Val loss 2.124\n",
            "Ep 39 (Step 021150): Train loss 0.724, Val loss 2.115\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The study compares the optimal model size for various models, showing that increasing model size, and training loss and downstream metrics, and downstream tasks. Additionally, the study observed that larger models trained for downstream tasks suggests that larger model size can be\n",
            "Ep 40 (Step 021200): Train loss 0.728, Val loss 2.115\n",
            "Ep 40 (Step 021250): Train loss 0.690, Val loss 2.116\n",
            "Ep 40 (Step 021300): Train loss 0.696, Val loss 2.117\n",
            "Ep 40 (Step 021350): Train loss 0.686, Val loss 2.107\n",
            "Ep 40 (Step 021400): Train loss 0.707, Val loss 2.120\n",
            "Ep 40 (Step 021450): Train loss 0.709, Val loss 2.118\n",
            "Ep 40 (Step 021500): Train loss 0.709, Val loss 2.109\n",
            "Ep 40 (Step 021550): Train loss 0.684, Val loss 2.104\n",
            "Ep 40 (Step 021600): Train loss 0.709, Val loss 2.102\n",
            "Ep 40 (Step 021650): Train loss 0.685, Val loss 2.108\n",
            "Ep 40 (Step 021700): Train loss 0.687, Val loss 2.099\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The study compares the optimal model size for various models, showing that increasing model size, and training loss and that larger models trained on downstream tasks is consistent with smaller, showing that larger models can achieve better performance on downstream tasks. To validate\n",
            "Ep 41 (Step 021750): Train loss 0.669, Val loss 2.102\n",
            "Ep 41 (Step 021800): Train loss 0.668, Val loss 2.102\n",
            "Ep 41 (Step 021850): Train loss 0.690, Val loss 2.112\n",
            "Ep 41 (Step 021900): Train loss 0.660, Val loss 2.103\n",
            "Ep 41 (Step 021950): Train loss 0.658, Val loss 2.113\n",
            "Ep 41 (Step 022000): Train loss 0.668, Val loss 2.097\n",
            "Ep 41 (Step 022050): Train loss 0.650, Val loss 2.103\n",
            "Ep 41 (Step 022100): Train loss 0.659, Val loss 2.094\n",
            "Ep 41 (Step 022150): Train loss 0.662, Val loss 2.105\n",
            "Ep 41 (Step 022200): Train loss 0.629, Val loss 2.087\n",
            "Ep 41 (Step 022250): Train loss 0.643, Val loss 2.090\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The study compares the scaling laws for different model sizes( 8B and 8B models) with varying sizes( N. 3B parameters trained on 9. 1B model size, showing that performance is more pronounced on downstream tasks but slightly underperforms similar to model\n",
            "Ep 42 (Step 022300): Train loss 0.680, Val loss 2.120\n",
            "Ep 42 (Step 022350): Train loss 0.628, Val loss 2.105\n",
            "Ep 42 (Step 022400): Train loss 0.645, Val loss 2.115\n",
            "Ep 42 (Step 022450): Train loss 0.634, Val loss 2.107\n",
            "Ep 42 (Step 022500): Train loss 0.636, Val loss 2.103\n",
            "Ep 42 (Step 022550): Train loss 0.639, Val loss 2.109\n",
            "Ep 42 (Step 022600): Train loss 0.642, Val loss 2.094\n",
            "Ep 42 (Step 022650): Train loss 0.612, Val loss 2.099\n",
            "Ep 42 (Step 022700): Train loss 0.616, Val loss 2.076\n",
            "Ep 42 (Step 022750): Train loss 0.598, Val loss 2.074\n",
            "Ep 42 (Step 022800): Train loss 0.595, Val loss 2.088\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The study compares the optimal model size for optimal model sizes and fixed sizes( 1. 5B parameters), showing that larger models trained on 9 × 28 × 28. 9 × 28 × 28 × 28. 5B and 1 × 28.\n",
            "Ep 43 (Step 022850): Train loss 0.578, Val loss 2.092\n",
            "Ep 43 (Step 022900): Train loss 0.591, Val loss 2.092\n",
            "Ep 43 (Step 022950): Train loss 0.572, Val loss 2.086\n",
            "Ep 43 (Step 023000): Train loss 0.599, Val loss 2.089\n",
            "Ep 43 (Step 023050): Train loss 0.615, Val loss 2.081\n",
            "Ep 43 (Step 023100): Train loss 0.565, Val loss 2.077\n",
            "Ep 43 (Step 023150): Train loss 0.607, Val loss 2.081\n",
            "Ep 43 (Step 023200): Train loss 0.593, Val loss 2.074\n",
            "Ep 43 (Step 023250): Train loss 0.586, Val loss 2.076\n",
            "Ep 43 (Step 023300): Train loss 0.581, Val loss 2.078\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The study compares the optimal model size for optimal model sizes and fixed size, showing that optimal model size( 1. 8B and 9 points) loss generally model size( 1 × 28. 3B variant) suggests that optimal model size( 2\n",
            "Ep 44 (Step 023350): Train loss 0.576, Val loss 2.080\n",
            "Ep 44 (Step 023400): Train loss 0.578, Val loss 2.093\n",
            "Ep 44 (Step 023450): Train loss 0.555, Val loss 2.083\n",
            "Ep 44 (Step 023500): Train loss 0.549, Val loss 2.090\n",
            "Ep 44 (Step 023550): Train loss 0.552, Val loss 2.089\n",
            "Ep 44 (Step 023600): Train loss 0.575, Val loss 2.087\n",
            "Ep 44 (Step 023650): Train loss 0.549, Val loss 2.084\n",
            "Ep 44 (Step 023700): Train loss 0.562, Val loss 2.082\n",
            "Ep 44 (Step 023750): Train loss 0.547, Val loss 2.081\n",
            "Ep 44 (Step 023800): Train loss 0.536, Val loss 2.077\n",
            "Ep 44 (Step 023850): Train loss 0.545, Val loss 2.079\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The study compares Llama 2 and Llama 2 models of varying sizes( 8B and 8B parameters) with varying parameter sizes, showing that scaling laws often outperforms larger models of similar size and model sizes( 8B, 62B, and 540B) when trained\n",
            "Ep 45 (Step 023900): Train loss 0.550, Val loss 2.071\n",
            "Ep 45 (Step 023950): Train loss 0.538, Val loss 2.083\n",
            "Ep 45 (Step 024000): Train loss 0.549, Val loss 2.071\n",
            "Ep 45 (Step 024050): Train loss 0.546, Val loss 2.071\n",
            "Ep 45 (Step 024100): Train loss 0.522, Val loss 2.071\n",
            "Ep 45 (Step 024150): Train loss 0.524, Val loss 2.076\n",
            "Ep 45 (Step 024200): Train loss 0.511, Val loss 2.060\n",
            "Ep 45 (Step 024250): Train loss 0.525, Val loss 2.061\n",
            "Ep 45 (Step 024300): Train loss 0.505, Val loss 2.073\n",
            "Ep 45 (Step 024350): Train loss 0.524, Val loss 2.062\n",
            "Ep 45 (Step 024400): Train loss 0.515, Val loss 2.067\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The study compares different model sizes( 8B, 8B, 8B, and 8B, 62B) models of varying sizes( with 6. 9 × 28 and 9. 8B, respectively) and showing that scaling laws can be attributed to a loss\n",
            "Ep 46 (Step 024450): Train loss 0.497, Val loss 2.071\n",
            "Ep 46 (Step 024500): Train loss 0.473, Val loss 2.077\n",
            "Ep 46 (Step 024550): Train loss 0.513, Val loss 2.077\n",
            "Ep 46 (Step 024600): Train loss 0.495, Val loss 2.075\n",
            "Ep 46 (Step 024650): Train loss 0.511, Val loss 2.065\n",
            "Ep 46 (Step 024700): Train loss 0.495, Val loss 2.064\n",
            "Ep 46 (Step 024750): Train loss 0.506, Val loss 2.062\n",
            "Ep 46 (Step 024800): Train loss 0.505, Val loss 2.067\n",
            "Ep 46 (Step 024850): Train loss 0.488, Val loss 2.078\n",
            "Ep 46 (Step 024900): Train loss 0.477, Val loss 2.060\n",
            "Ep 46 (Step 024950): Train loss 0.489, Val loss 2.068\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The study compares Llama 2 models to their performance to varying sizes( 8B and 8B parameters) and optimal model sizes, showing that increasing training loss scales leads to a similar number of parameters( 1. 3B model size increases, and 1.\n",
            "Ep 47 (Step 025000): Train loss 0.473, Val loss 2.064\n",
            "Ep 47 (Step 025050): Train loss 0.460, Val loss 2.080\n",
            "Ep 47 (Step 025100): Train loss 0.471, Val loss 2.084\n",
            "Ep 47 (Step 025150): Train loss 0.469, Val loss 2.071\n",
            "Ep 47 (Step 025200): Train loss 0.482, Val loss 2.084\n",
            "Ep 47 (Step 025250): Train loss 0.450, Val loss 2.063\n",
            "Ep 47 (Step 025300): Train loss 0.472, Val loss 2.072\n",
            "Ep 47 (Step 025350): Train loss 0.474, Val loss 2.082\n",
            "Ep 47 (Step 025400): Train loss 0.466, Val loss 2.078\n",
            "Ep 47 (Step 025450): Train loss 0.458, Val loss 2.066\n",
            "Ep 47 (Step 025500): Train loss 0.459, Val loss 2.081\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The study reports that increasing downstream metrics for optimal model sizes( 6. 7B parameters) and training loss and downstream metrics, while scaling laws exhibit optimal performance loss and optimal downstream performance, they can be attributed to a 1. 5 billion parameter\n",
            "Ep 48 (Step 025550): Train loss 0.443, Val loss 2.071\n",
            "Ep 48 (Step 025600): Train loss 0.444, Val loss 2.067\n",
            "Ep 48 (Step 025650): Train loss 0.445, Val loss 2.069\n",
            "Ep 48 (Step 025700): Train loss 0.434, Val loss 2.069\n",
            "Ep 48 (Step 025750): Train loss 0.436, Val loss 2.063\n",
            "Ep 48 (Step 025800): Train loss 0.436, Val loss 2.073\n",
            "Ep 48 (Step 025850): Train loss 0.424, Val loss 2.062\n",
            "Ep 48 (Step 025900): Train loss 0.432, Val loss 2.074\n",
            "Ep 48 (Step 025950): Train loss 0.430, Val loss 2.075\n",
            "Ep 48 (Step 026000): Train loss 0.418, Val loss 2.070\n",
            "Ep 48 (Step 026050): Train loss 0.412, Val loss 2.054\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The study compares Llama 2 models of similar sizes( 8B and 8B) models trained on downstream tasks, showing that scaling laws tend to outperform their performance on downstream tasks due to scaling laws and suggests that model size may not be fine-tuned for specific\n",
            "Ep 49 (Step 026100): Train loss 0.418, Val loss 2.059\n",
            "Ep 49 (Step 026150): Train loss 0.401, Val loss 2.067\n",
            "Ep 49 (Step 026200): Train loss 0.408, Val loss 2.067\n",
            "Ep 49 (Step 026250): Train loss 0.409, Val loss 2.073\n",
            "Ep 49 (Step 026300): Train loss 0.411, Val loss 2.073\n",
            "Ep 49 (Step 026350): Train loss 0.409, Val loss 2.068\n",
            "Ep 49 (Step 026400): Train loss 0.422, Val loss 2.078\n",
            "Ep 49 (Step 026450): Train loss 0.406, Val loss 2.068\n",
            "Ep 49 (Step 026500): Train loss 0.424, Val loss 2.083\n",
            "Ep 49 (Step 026550): Train loss 0.404, Val loss 2.070\n",
            "Ep 49 (Step 026600): Train loss 0.416, Val loss 2.074\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The study compares the optimal model size for optimal model sizes( N and 1 × 1022 FLOPs) based on downstream metrics, showing a 1 × 1022 FLOPs model size, and a logarithmic ratio for training loss and downstream tasks. The optimal model\n",
            "Ep 50 (Step 026650): Train loss 0.391, Val loss 2.070\n",
            "Ep 50 (Step 026700): Train loss 0.391, Val loss 2.073\n",
            "Ep 50 (Step 026750): Train loss 0.390, Val loss 2.068\n",
            "Ep 50 (Step 026800): Train loss 0.380, Val loss 2.073\n",
            "Ep 50 (Step 026850): Train loss 0.403, Val loss 2.077\n",
            "Ep 50 (Step 026900): Train loss 0.402, Val loss 2.077\n",
            "Ep 50 (Step 026950): Train loss 0.380, Val loss 2.066\n",
            "Ep 50 (Step 027000): Train loss 0.379, Val loss 2.080\n",
            "Ep 50 (Step 027050): Train loss 0.373, Val loss 2.057\n",
            "Ep 50 (Step 027100): Train loss 0.387, Val loss 2.055\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The study compares different model sizes( 1 × 28 × 28 × 28 × 1022 FLOPs) and fixed training loss, showing a 1. 5 × 28 and a 1. 3B parameter model trained for 9 × 16 × 28. 1 ×\n",
            "Ep 51 (Step 027150): Train loss 0.378, Val loss 2.063\n",
            "Ep 51 (Step 027200): Train loss 0.371, Val loss 2.074\n",
            "Ep 51 (Step 027250): Train loss 0.378, Val loss 2.075\n",
            "Ep 51 (Step 027300): Train loss 0.358, Val loss 2.084\n",
            "Ep 51 (Step 027350): Train loss 0.360, Val loss 2.081\n",
            "Ep 51 (Step 027400): Train loss 0.361, Val loss 2.084\n",
            "Ep 51 (Step 027450): Train loss 0.371, Val loss 2.083\n",
            "Ep 51 (Step 027500): Train loss 0.344, Val loss 2.081\n",
            "Ep 51 (Step 027550): Train loss 0.338, Val loss 2.076\n",
            "Ep 51 (Step 027600): Train loss 0.348, Val loss 2.071\n",
            "Ep 51 (Step 027650): Train loss 0.359, Val loss 2.066\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The 6. 5B model, shows that while larger models trained for a fixed number of parameters, there is a 1. 5 billion parameter model, the 9. The 6 billion parameter model is trained on 9 billion tokens, and 9 billion\n",
            "Ep 52 (Step 027700): Train loss 0.358, Val loss 2.061\n",
            "Ep 52 (Step 027750): Train loss 0.340, Val loss 2.070\n",
            "Ep 52 (Step 027800): Train loss 0.347, Val loss 2.072\n",
            "Ep 52 (Step 027850): Train loss 0.330, Val loss 2.077\n",
            "Ep 52 (Step 027900): Train loss 0.335, Val loss 2.068\n",
            "Ep 52 (Step 027950): Train loss 0.343, Val loss 2.080\n",
            "Ep 52 (Step 028000): Train loss 0.329, Val loss 2.082\n",
            "Ep 52 (Step 028050): Train loss 0.315, Val loss 2.080\n",
            "Ep 52 (Step 028100): Train loss 0.345, Val loss 2.078\n",
            "Ep 52 (Step 028150): Train loss 0.328, Val loss 2.064\n",
            "Ep 52 (Step 028200): Train loss 0.318, Val loss 2.061\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The study observed that the optimal model size influences the optimal model size, and downstream tasks exhibit optimal performance when training data is not significantly smaller, but the optimal model size and to a similar number of parameters, the optimal model size increases to\n",
            "Ep 53 (Step 028250): Train loss 0.317, Val loss 2.081\n",
            "Ep 53 (Step 028300): Train loss 0.326, Val loss 2.090\n",
            "Ep 53 (Step 028350): Train loss 0.330, Val loss 2.092\n",
            "Ep 53 (Step 028400): Train loss 0.325, Val loss 2.080\n",
            "Ep 53 (Step 028450): Train loss 0.315, Val loss 2.083\n",
            "Ep 53 (Step 028500): Train loss 0.312, Val loss 2.084\n",
            "Ep 53 (Step 028550): Train loss 0.315, Val loss 2.078\n",
            "Ep 53 (Step 028600): Train loss 0.322, Val loss 2.084\n",
            "Ep 53 (Step 028650): Train loss 0.307, Val loss 2.078\n",
            "Ep 53 (Step 028700): Train loss 0.309, Val loss 2.083\n",
            "Ep 53 (Step 028750): Train loss 0.311, Val loss 2.066\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The study compares different model sizes( 1 × 1022 FLOPs) and fixed computational budgets, with results in Table 6. It shows that scaling performance scales with model size, and 1 × <|unk|> FLOPs trained on downstream tasks, while a 1 billion\n",
            "Ep 54 (Step 028800): Train loss 0.294, Val loss 2.089\n",
            "Ep 54 (Step 028850): Train loss 0.296, Val loss 2.086\n",
            "Ep 54 (Step 028900): Train loss 0.304, Val loss 2.085\n",
            "Ep 54 (Step 028950): Train loss 0.310, Val loss 2.092\n",
            "Ep 54 (Step 029000): Train loss 0.300, Val loss 2.093\n",
            "Ep 54 (Step 029050): Train loss 0.309, Val loss 2.102\n",
            "Ep 54 (Step 029100): Train loss 0.296, Val loss 2.091\n",
            "Ep 54 (Step 029150): Train loss 0.290, Val loss 2.096\n",
            "Ep 54 (Step 029200): Train loss 0.283, Val loss 2.089\n",
            "Ep 54 (Step 029250): Train loss 0.285, Val loss 2.101\n",
            "Ep 54 (Step 029300): Train loss 0.289, Val loss 2.089\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The study compares the optimal model size for various models, showing that having 1. 5 billion parameters( 1. 2) outperform Llama 2 models of these optimal sizes. The study finds that most of these models, showing that scaling laws often\n",
            "Ep 55 (Step 029350): Train loss 0.292, Val loss 2.084\n",
            "Ep 55 (Step 029400): Train loss 0.288, Val loss 2.091\n",
            "Ep 55 (Step 029450): Train loss 0.283, Val loss 2.100\n",
            "Ep 55 (Step 029500): Train loss 0.283, Val loss 2.087\n",
            "Ep 55 (Step 029550): Train loss 0.270, Val loss 2.088\n",
            "Ep 55 (Step 029600): Train loss 0.284, Val loss 2.099\n",
            "Ep 55 (Step 029650): Train loss 0.288, Val loss 2.094\n",
            "Ep 55 (Step 029700): Train loss 0.290, Val loss 2.102\n",
            "Ep 55 (Step 029750): Train loss 0.273, Val loss 2.098\n",
            "Ep 55 (Step 029800): Train loss 0.266, Val loss 2.081\n",
            "Ep 55 (Step 029850): Train loss 0.274, Val loss 2.087\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The study compares different model sizes( 1 × Downstream vs. 5B and 8B, 62B) models of varying sizes, showing that scaling laws tend to loss and optimal learning rate for various downstream tasks. The study reports that optimal model sizes are\n",
            "Ep 56 (Step 029900): Train loss 0.281, Val loss 2.104\n",
            "Ep 56 (Step 029950): Train loss 0.273, Val loss 2.101\n",
            "Ep 56 (Step 030000): Train loss 0.243, Val loss 2.097\n",
            "Ep 56 (Step 030050): Train loss 0.258, Val loss 2.117\n",
            "Ep 56 (Step 030100): Train loss 0.261, Val loss 2.120\n",
            "Ep 56 (Step 030150): Train loss 0.261, Val loss 2.102\n",
            "Ep 56 (Step 030200): Train loss 0.250, Val loss 2.098\n",
            "Ep 56 (Step 030250): Train loss 0.256, Val loss 2.102\n",
            "Ep 56 (Step 030300): Train loss 0.249, Val loss 2.098\n",
            "Ep 56 (Step 030350): Train loss 0.256, Val loss 2.093\n",
            "Ep 56 (Step 030400): Train loss 0.249, Val loss 2.098\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The study compares different model sizes( 1 × 28 × 28 and 1 × 1022 FLOPs) and optimal model sizes, showing that a 1 × <|unk|> loss( 9. 9 points) loss is closest to model size, and a 1.\n",
            "Ep 57 (Step 030450): Train loss 0.248, Val loss 2.106\n",
            "Ep 57 (Step 030500): Train loss 0.252, Val loss 2.114\n",
            "Ep 57 (Step 030550): Train loss 0.244, Val loss 2.116\n",
            "Ep 57 (Step 030600): Train loss 0.252, Val loss 2.121\n",
            "Ep 57 (Step 030650): Train loss 0.244, Val loss 2.119\n",
            "Ep 57 (Step 030700): Train loss 0.246, Val loss 2.115\n",
            "Ep 57 (Step 030750): Train loss 0.235, Val loss 2.110\n",
            "Ep 57 (Step 030800): Train loss 0.238, Val loss 2.110\n",
            "Ep 57 (Step 030850): Train loss 0.238, Val loss 2.101\n",
            "Ep 57 (Step 030900): Train loss 0.233, Val loss 2.098\n",
            "Ep 57 (Step 030950): Train loss 0.235, Val loss 2.114\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The study compares the optimal model size for 9 × 28 and fixed accuracy in Table 9. Downstream metrics, showing that the 1 model size generally outperforms the 9 English 8B model and 9 models in accuracy, with model size ranges from 8B to\n",
            "Ep 58 (Step 031000): Train loss 0.229, Val loss 2.109\n",
            "Ep 58 (Step 031050): Train loss 0.231, Val loss 2.123\n",
            "Ep 58 (Step 031100): Train loss 0.220, Val loss 2.115\n",
            "Ep 58 (Step 031150): Train loss 0.229, Val loss 2.121\n",
            "Ep 58 (Step 031200): Train loss 0.227, Val loss 2.121\n",
            "Ep 58 (Step 031250): Train loss 0.225, Val loss 2.122\n",
            "Ep 58 (Step 031300): Train loss 0.225, Val loss 2.120\n",
            "Ep 58 (Step 031350): Train loss 0.211, Val loss 2.113\n",
            "Ep 58 (Step 031400): Train loss 0.224, Val loss 2.115\n",
            "Ep 58 (Step 031450): Train loss 0.222, Val loss 2.125\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The results in Table 6. Downstream metrics suggest that the 1. 5B model, showing that the optimal model size model size generally outperforms larger models but slightly underperforms the optimal downstream performance, indicating that the optimal model size. This suggests that while\n",
            "Ep 59 (Step 031500): Train loss 0.231, Val loss 2.128\n",
            "Ep 59 (Step 031550): Train loss 0.215, Val loss 2.128\n",
            "Ep 59 (Step 031600): Train loss 0.218, Val loss 2.133\n",
            "Ep 59 (Step 031650): Train loss 0.206, Val loss 2.125\n",
            "Ep 59 (Step 031700): Train loss 0.230, Val loss 2.138\n",
            "Ep 59 (Step 031750): Train loss 0.212, Val loss 2.129\n",
            "Ep 59 (Step 031800): Train loss 0.223, Val loss 2.150\n",
            "Ep 59 (Step 031850): Train loss 0.210, Val loss 2.140\n",
            "Ep 59 (Step 031900): Train loss 0.218, Val loss 2.142\n",
            "Ep 59 (Step 031950): Train loss 0.205, Val loss 2.138\n",
            "Ep 59 (Step 032000): Train loss 0.202, Val loss 2.122\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The results in Table 6. Downstream model shows that the 8B model outperforms the best downstream metrics, showing that the optimal model size does not exhibit downstream performance while the optimal model size remains a useful for downstream tasks. This suggests that the training\n",
            "Ep 60 (Step 032050): Train loss 0.206, Val loss 2.133\n",
            "Ep 60 (Step 032100): Train loss 0.201, Val loss 2.140\n",
            "Ep 60 (Step 032150): Train loss 0.194, Val loss 2.144\n",
            "Ep 60 (Step 032200): Train loss 0.190, Val loss 2.137\n",
            "Ep 60 (Step 032250): Train loss 0.206, Val loss 2.156\n",
            "Ep 60 (Step 032300): Train loss 0.200, Val loss 2.139\n",
            "Ep 60 (Step 032350): Train loss 0.184, Val loss 2.139\n",
            "Ep 60 (Step 032400): Train loss 0.203, Val loss 2.148\n",
            "Ep 60 (Step 032450): Train loss 0.192, Val loss 2.147\n",
            "Ep 60 (Step 032500): Train loss 0.193, Val loss 2.141\n",
            "Ep 60 (Step 032550): Train loss 0.202, Val loss 2.148\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The study compares the optimal model size for various models, showing that performance is trained on downstream tasks, showing that a 1 billion parameter model size is trained on downstream tasks due to a fixed computational budget, and that the 1 model size is\n",
            "🟦 saved → regex_finetuned.pth\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "gpt2 = False\n",
        "model_regex = model_regex.to(device)\n",
        "model_regex.train()\n",
        "\n",
        "optimizer_regex = torch.optim.AdamW(model_regex.parameters(), lr=5e-5, weight_decay=0.1)\n",
        "\n",
        "torch.manual_seed(123)\n",
        "train_losses_rx, val_losses_rx, tokens_rx = train_model_simple(\n",
        "    model_regex, train_loader_regex, val_loader_regex, optimizer_regex, device,\n",
        "    num_epochs=60, eval_freq=50, eval_iter=20,\n",
        "    start_context=format_input(val_set[0]), tokenizer=tokenizer_v2)\n",
        "\n",
        "torch.save(model_regex.state_dict(), \"regex_finetuned.pth\")\n",
        "print(\"🟦 saved → regex_finetuned.pth\")\n",
        "\n",
        "epochs_tensor = torch.linspace(0, 60, len(train_losses_rx))\n",
        "plot_losses(epochs_tensor, tokens_rx, train_losses_rx, val_losses_rx)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2be15b38",
      "metadata": {},
      "source": [
        "### Step 3: Fine-Tuning the GPT-2 Model\n",
        "\n",
        "- Saves the fine-tuned checkpoint as **`gpt2_finetuned.pth`**.  \n",
        "- Plots both **training** and **validation loss curves** across epochs.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10cfb3f0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep 1 (Step 000000): Train loss 5.379, Val loss 5.359\n",
            "Ep 1 (Step 000050): Train loss 4.358, Val loss 4.399\n",
            "Ep 1 (Step 000100): Train loss 4.300, Val loss 4.273\n",
            "Ep 1 (Step 000150): Train loss 4.165, Val loss 4.225\n",
            "Ep 1 (Step 000200): Train loss 4.155, Val loss 4.188\n",
            "Ep 1 (Step 000250): Train loss 3.994, Val loss 4.149\n",
            "Ep 1 (Step 000300): Train loss 4.184, Val loss 4.113\n",
            "Ep 1 (Step 000350): Train loss 4.024, Val loss 4.092\n",
            "Ep 1 (Step 000400): Train loss 3.958, Val loss 4.051\n",
            "Ep 1 (Step 000450): Train loss 3.905, Val loss 4.048\n",
            "Ep 1 (Step 000500): Train loss 3.946, Val loss 4.009\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to the performance on the performance on the performance on the performance on the performance of the performance on the performance on the performance of the performance on the performance on the performance on the performance on the model, we also outperforms the performance of the performance of the\n",
            "Ep 2 (Step 000550): Train loss 3.877, Val loss 3.997\n",
            "Ep 2 (Step 000600): Train loss 3.886, Val loss 3.970\n",
            "Ep 2 (Step 000650): Train loss 3.945, Val loss 3.964\n",
            "Ep 2 (Step 000700): Train loss 3.901, Val loss 3.954\n",
            "Ep 2 (Step 000750): Train loss 3.887, Val loss 3.920\n",
            "Ep 2 (Step 000800): Train loss 3.829, Val loss 3.930\n",
            "Ep 2 (Step 000850): Train loss 3.816, Val loss 3.915\n",
            "Ep 2 (Step 000900): Train loss 3.729, Val loss 3.885\n",
            "Ep 2 (Step 000950): Train loss 3.776, Val loss 3.847\n",
            "Ep 2 (Step 001000): Train loss 3.744, Val loss 3.843\n",
            "Ep 2 (Step 001050): Train loss 3.742, Val loss 3.831\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to the model performance of the model performance of the performance on the performance on the performance of the performance of the model performance of the performance of the performance of the model performance of the performance of the performance of the performance of the performance of the performance of the\n",
            "Ep 3 (Step 001100): Train loss 3.629, Val loss 3.818\n",
            "Ep 3 (Step 001150): Train loss 3.649, Val loss 3.807\n",
            "Ep 3 (Step 001200): Train loss 3.610, Val loss 3.791\n",
            "Ep 3 (Step 001250): Train loss 3.626, Val loss 3.796\n",
            "Ep 3 (Step 001300): Train loss 3.624, Val loss 3.764\n",
            "Ep 3 (Step 001350): Train loss 3.601, Val loss 3.771\n",
            "Ep 3 (Step 001400): Train loss 3.507, Val loss 3.738\n",
            "Ep 3 (Step 001450): Train loss 3.546, Val loss 3.727\n",
            "Ep 3 (Step 001500): Train loss 3.514, Val loss 3.722\n",
            "Ep 3 (Step 001550): Train loss 3.553, Val loss 3.707\n",
            "Ep 3 (Step 001600): Train loss 3.506, Val loss 3.700\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to the model performance of the model performance of the model.  ### Response: The main mechanism described in the performance of the performance of the performance of the performance of the performance of the performance of the model performance of the model. The performance of\n",
            "Ep 4 (Step 001650): Train loss 3.552, Val loss 3.690\n",
            "Ep 4 (Step 001700): Train loss 3.484, Val loss 3.689\n",
            "Ep 4 (Step 001750): Train loss 3.468, Val loss 3.671\n",
            "Ep 4 (Step 001800): Train loss 3.505, Val loss 3.666\n",
            "Ep 4 (Step 001850): Train loss 3.399, Val loss 3.646\n",
            "Ep 4 (Step 001900): Train loss 3.424, Val loss 3.631\n",
            "Ep 4 (Step 001950): Train loss 3.405, Val loss 3.614\n",
            "Ep 4 (Step 002000): Train loss 3.411, Val loss 3.593\n",
            "Ep 4 (Step 002050): Train loss 3.405, Val loss 3.612\n",
            "Ep 4 (Step 002100): Train loss 3.320, Val loss 3.605\n",
            "Ep 4 (Step 002150): Train loss 3.361, Val loss 3.568\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to the model performance of the model.  ### Response: The study involves using a model size of the model performance on the model. The model is trained on the model, which is trained on the model performance and model performance on the model.\n",
            "Ep 5 (Step 002200): Train loss 3.268, Val loss 3.546\n",
            "Ep 5 (Step 002250): Train loss 3.307, Val loss 3.564\n",
            "Ep 5 (Step 002300): Train loss 3.337, Val loss 3.544\n",
            "Ep 5 (Step 002350): Train loss 3.314, Val loss 3.529\n",
            "Ep 5 (Step 002400): Train loss 3.287, Val loss 3.552\n",
            "Ep 5 (Step 002450): Train loss 3.304, Val loss 3.506\n",
            "Ep 5 (Step 002500): Train loss 3.208, Val loss 3.490\n",
            "Ep 5 (Step 002550): Train loss 3.262, Val loss 3.490\n",
            "Ep 5 (Step 002600): Train loss 3.170, Val loss 3.484\n",
            "Ep 5 (Step 002650): Train loss 3.220, Val loss 3.480\n",
            "Ep 5 (Step 002700): Train loss 3.140, Val loss 3.472\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to the model performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the\n",
            "Ep 6 (Step 002750): Train loss 3.195, Val loss 3.466\n",
            "Ep 6 (Step 002800): Train loss 3.197, Val loss 3.447\n",
            "Ep 6 (Step 002850): Train loss 3.191, Val loss 3.467\n",
            "Ep 6 (Step 002900): Train loss 3.155, Val loss 3.431\n",
            "Ep 6 (Step 002950): Train loss 3.121, Val loss 3.428\n",
            "Ep 6 (Step 003000): Train loss 3.158, Val loss 3.427\n",
            "Ep 6 (Step 003050): Train loss 3.104, Val loss 3.405\n",
            "Ep 6 (Step 003100): Train loss 3.093, Val loss 3.405\n",
            "Ep 6 (Step 003150): Train loss 3.066, Val loss 3.383\n",
            "Ep 6 (Step 003200): Train loss 3.027, Val loss 3.389\n",
            "Ep 6 (Step 003250): Train loss 3.098, Val loss 3.381\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to the model is not only on the model.  ### Response: The study compares the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance\n",
            "Ep 7 (Step 003300): Train loss 3.078, Val loss 3.380\n",
            "Ep 7 (Step 003350): Train loss 3.074, Val loss 3.360\n",
            "Ep 7 (Step 003400): Train loss 3.062, Val loss 3.354\n",
            "Ep 7 (Step 003450): Train loss 3.015, Val loss 3.353\n",
            "Ep 7 (Step 003500): Train loss 3.066, Val loss 3.361\n",
            "Ep 7 (Step 003550): Train loss 2.965, Val loss 3.341\n",
            "Ep 7 (Step 003600): Train loss 2.974, Val loss 3.326\n",
            "Ep 7 (Step 003650): Train loss 3.013, Val loss 3.334\n",
            "Ep 7 (Step 003700): Train loss 2.924, Val loss 3.318\n",
            "Ep 7 (Step 003750): Train loss 2.936, Val loss 3.300\n",
            "Ep 7 (Step 003800): Train loss 2.924, Val loss 3.298\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to the model.  ### Response: The study compares to the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the\n",
            "Ep 8 (Step 003850): Train loss 2.851, Val loss 3.259\n",
            "Ep 8 (Step 003900): Train loss 2.868, Val loss 3.265\n",
            "Ep 8 (Step 003950): Train loss 2.908, Val loss 3.282\n",
            "Ep 8 (Step 004000): Train loss 2.935, Val loss 3.281\n",
            "Ep 8 (Step 004050): Train loss 2.893, Val loss 3.254\n",
            "Ep 8 (Step 004100): Train loss 2.954, Val loss 3.254\n",
            "Ep 8 (Step 004150): Train loss 2.845, Val loss 3.235\n",
            "Ep 8 (Step 004200): Train loss 2.962, Val loss 3.235\n",
            "Ep 8 (Step 004250): Train loss 2.849, Val loss 3.229\n",
            "Ep 8 (Step 004300): Train loss 2.807, Val loss 3.228\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to the model performance on the performance of the performance of the performance of the performance of the smaller models.  ### Response: The study compares the performance of the performance of the performance of the performance of the performance improvements in performance improvements, and the\n",
            "Ep 9 (Step 004350): Train loss 2.875, Val loss 3.213\n",
            "Ep 9 (Step 004400): Train loss 2.814, Val loss 3.218\n",
            "Ep 9 (Step 004450): Train loss 2.838, Val loss 3.219\n",
            "Ep 9 (Step 004500): Train loss 2.865, Val loss 3.195\n",
            "Ep 9 (Step 004550): Train loss 2.777, Val loss 3.198\n",
            "Ep 9 (Step 004600): Train loss 2.813, Val loss 3.184\n",
            "Ep 9 (Step 004650): Train loss 2.817, Val loss 3.173\n",
            "Ep 9 (Step 004700): Train loss 2.791, Val loss 3.176\n",
            "Ep 9 (Step 004750): Train loss 2.787, Val loss 3.171\n",
            "Ep 9 (Step 004800): Train loss 2.724, Val loss 3.139\n",
            "Ep 9 (Step 004850): Train loss 2.746, Val loss 3.131\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to the smaller models.  ### Response: The study compares the performance of the performance of the performance of the performance of smaller models, with larger models, with larger models, and outperforms larger models, with larger models, with larger models.\n",
            "Ep 10 (Step 004900): Train loss 2.709, Val loss 3.129\n",
            "Ep 10 (Step 004950): Train loss 2.726, Val loss 3.129\n",
            "Ep 10 (Step 005000): Train loss 2.689, Val loss 3.124\n",
            "Ep 10 (Step 005050): Train loss 2.612, Val loss 3.126\n",
            "Ep 10 (Step 005100): Train loss 2.670, Val loss 3.115\n",
            "Ep 10 (Step 005150): Train loss 2.625, Val loss 3.112\n",
            "Ep 10 (Step 005200): Train loss 2.647, Val loss 3.100\n",
            "Ep 10 (Step 005250): Train loss 2.643, Val loss 3.095\n",
            "Ep 10 (Step 005300): Train loss 2.600, Val loss 3.096\n",
            "Ep 10 (Step 005350): Train loss 2.626, Val loss 3.084\n",
            "Ep 10 (Step 005400): Train loss 2.673, Val loss 3.083\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to the model size increases.  ### Response: The study compares to the performance of the scaling laws for training data contamination, with larger models, and the performance of smaller models, with larger models, and larger models, with larger models trained on\n",
            "Ep 11 (Step 005450): Train loss 2.646, Val loss 3.082\n",
            "Ep 11 (Step 005500): Train loss 2.623, Val loss 3.070\n",
            "Ep 11 (Step 005550): Train loss 2.536, Val loss 3.097\n",
            "Ep 11 (Step 005600): Train loss 2.580, Val loss 3.062\n",
            "Ep 11 (Step 005650): Train loss 2.578, Val loss 3.061\n",
            "Ep 11 (Step 005700): Train loss 2.604, Val loss 3.053\n",
            "Ep 11 (Step 005750): Train loss 2.620, Val loss 3.044\n",
            "Ep 11 (Step 005800): Train loss 2.528, Val loss 3.029\n",
            "Ep 11 (Step 005850): Train loss 2.538, Val loss 3.038\n",
            "Ep 11 (Step 005900): Train loss 2.556, Val loss 3.045\n",
            "Ep 11 (Step 005950): Train loss 2.534, Val loss 3.021\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to the model size increases in performance.  ### Response: The study compares to larger models trained on a smaller models trained on a smaller models trained on a smaller models trained on a smaller models trained on a smaller models trained on a smaller models.\n",
            "Ep 12 (Step 006000): Train loss 2.524, Val loss 3.015\n",
            "Ep 12 (Step 006050): Train loss 2.549, Val loss 3.016\n",
            "Ep 12 (Step 006100): Train loss 2.512, Val loss 3.005\n",
            "Ep 12 (Step 006150): Train loss 2.474, Val loss 2.980\n",
            "Ep 12 (Step 006200): Train loss 2.516, Val loss 2.997\n",
            "Ep 12 (Step 006250): Train loss 2.487, Val loss 2.978\n",
            "Ep 12 (Step 006300): Train loss 2.441, Val loss 2.990\n",
            "Ep 12 (Step 006350): Train loss 2.541, Val loss 2.985\n",
            "Ep 12 (Step 006400): Train loss 2.436, Val loss 2.978\n",
            "Ep 12 (Step 006450): Train loss 2.407, Val loss 2.978\n",
            "Ep 12 (Step 006500): Train loss 2.390, Val loss 2.954\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to the model size increases.  ### Response: The study compares the performance of smaller models on a smaller model size of smaller models on a smaller model size of smaller models, with larger models trained on a smaller model size of smaller models. The\n",
            "Ep 13 (Step 006550): Train loss 2.423, Val loss 2.956\n",
            "Ep 13 (Step 006600): Train loss 2.382, Val loss 2.933\n",
            "Ep 13 (Step 006650): Train loss 2.360, Val loss 2.947\n",
            "Ep 13 (Step 006700): Train loss 2.377, Val loss 2.931\n",
            "Ep 13 (Step 006750): Train loss 2.403, Val loss 2.937\n",
            "Ep 13 (Step 006800): Train loss 2.344, Val loss 2.918\n",
            "Ep 13 (Step 006850): Train loss 2.341, Val loss 2.915\n",
            "Ep 13 (Step 006900): Train loss 2.343, Val loss 2.921\n",
            "Ep 13 (Step 006950): Train loss 2.318, Val loss 2.921\n",
            "Ep 13 (Step 007000): Train loss 2.320, Val loss 2.908\n",
            "Ep 13 (Step 007050): Train loss 2.292, Val loss 2.902\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to the smaller model size increases, and is more than the smaller models.  ### Response: The study compares to larger models on a larger model size of smaller models, showing that larger models outperforms larger models on larger sizes, showing that larger\n",
            "Ep 14 (Step 007100): Train loss 2.337, Val loss 2.894\n",
            "Ep 14 (Step 007150): Train loss 2.356, Val loss 2.898\n",
            "Ep 14 (Step 007200): Train loss 2.316, Val loss 2.891\n",
            "Ep 14 (Step 007250): Train loss 2.335, Val loss 2.894\n",
            "Ep 14 (Step 007300): Train loss 2.314, Val loss 2.878\n",
            "Ep 14 (Step 007350): Train loss 2.237, Val loss 2.866\n",
            "Ep 14 (Step 007400): Train loss 2.281, Val loss 2.869\n",
            "Ep 14 (Step 007450): Train loss 2.259, Val loss 2.861\n",
            "Ep 14 (Step 007500): Train loss 2.252, Val loss 2.852\n",
            "Ep 14 (Step 007550): Train loss 2.274, Val loss 2.852\n",
            "Ep 14 (Step 007600): Train loss 2.195, Val loss 2.843\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to train smaller models, and larger models trained on larger models.  ### Response: The study compares the performance of larger models on a 1.5 billion parameter model, showing that larger models trained on a 1.5 billion parameter model, and\n",
            "Ep 15 (Step 007650): Train loss 2.234, Val loss 2.830\n",
            "Ep 15 (Step 007700): Train loss 2.215, Val loss 2.832\n",
            "Ep 15 (Step 007750): Train loss 2.221, Val loss 2.826\n",
            "Ep 15 (Step 007800): Train loss 2.195, Val loss 2.812\n",
            "Ep 15 (Step 007850): Train loss 2.255, Val loss 2.820\n",
            "Ep 15 (Step 007900): Train loss 2.239, Val loss 2.835\n",
            "Ep 15 (Step 007950): Train loss 2.193, Val loss 2.820\n",
            "Ep 15 (Step 008000): Train loss 2.195, Val loss 2.801\n",
            "Ep 15 (Step 008050): Train loss 2.188, Val loss 2.814\n",
            "Ep 15 (Step 008100): Train loss 2.146, Val loss 2.805\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to train a smaller model.  ### Response: The study compares the performance of larger models on a larger model size of smaller models on a 1.5 billion parameter model, showing that larger models trained on a smaller model size of 1.5\n",
            "Ep 16 (Step 008150): Train loss 2.163, Val loss 2.784\n",
            "Ep 16 (Step 008200): Train loss 2.133, Val loss 2.789\n",
            "Ep 16 (Step 008250): Train loss 2.129, Val loss 2.790\n",
            "Ep 16 (Step 008300): Train loss 2.090, Val loss 2.774\n",
            "Ep 16 (Step 008350): Train loss 2.190, Val loss 2.789\n",
            "Ep 16 (Step 008400): Train loss 2.122, Val loss 2.774\n",
            "Ep 16 (Step 008450): Train loss 2.090, Val loss 2.764\n",
            "Ep 16 (Step 008500): Train loss 2.119, Val loss 2.760\n",
            "Ep 16 (Step 008550): Train loss 2.072, Val loss 2.765\n",
            "Ep 16 (Step 008600): Train loss 2.084, Val loss 2.761\n",
            "Ep 16 (Step 008650): Train loss 2.054, Val loss 2.735\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to be more likely to be more likely to be more likely to be more likely due to the smaller models.  ### Response: The study compares the performance of smaller models on larger models on a larger models on smaller models on a smaller models on\n",
            "Ep 17 (Step 008700): Train loss 2.121, Val loss 2.736\n",
            "Ep 17 (Step 008750): Train loss 2.038, Val loss 2.733\n",
            "Ep 17 (Step 008800): Train loss 1.972, Val loss 2.727\n",
            "Ep 17 (Step 008850): Train loss 2.070, Val loss 2.746\n",
            "Ep 17 (Step 008900): Train loss 2.083, Val loss 2.738\n",
            "Ep 17 (Step 008950): Train loss 1.998, Val loss 2.727\n",
            "Ep 17 (Step 009000): Train loss 2.042, Val loss 2.719\n",
            "Ep 17 (Step 009050): Train loss 2.015, Val loss 2.718\n",
            "Ep 17 (Step 009100): Train loss 1.973, Val loss 2.725\n",
            "Ep 17 (Step 009150): Train loss 2.003, Val loss 2.704\n",
            "Ep 17 (Step 009200): Train loss 2.004, Val loss 2.695\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to be more efficient.  ### Response: The study compares the performance of smaller models on a 1.5 billion parameter model, showing that larger models outperform larger models, showing that larger models like GPT-3 and LLaMA-\n",
            "Ep 18 (Step 009250): Train loss 2.004, Val loss 2.685\n",
            "Ep 18 (Step 009300): Train loss 1.983, Val loss 2.681\n",
            "Ep 18 (Step 009350): Train loss 1.994, Val loss 2.698\n",
            "Ep 18 (Step 009400): Train loss 1.958, Val loss 2.685\n",
            "Ep 18 (Step 009450): Train loss 1.933, Val loss 2.695\n",
            "Ep 18 (Step 009500): Train loss 1.936, Val loss 2.668\n",
            "Ep 18 (Step 009550): Train loss 1.934, Val loss 2.678\n",
            "Ep 18 (Step 009600): Train loss 1.979, Val loss 2.670\n",
            "Ep 18 (Step 009650): Train loss 1.899, Val loss 2.657\n",
            "Ep 18 (Step 009700): Train loss 1.928, Val loss 2.663\n",
            "Ep 18 (Step 009750): Train loss 1.926, Val loss 2.645\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to train smaller models.  ### Response: The study compares LLaMA-2B and LLaMA-2B, showing that larger models outperforms larger models like GPT-3 and LLaMA-3B, despite being\n",
            "Ep 19 (Step 009800): Train loss 1.821, Val loss 2.638\n",
            "Ep 19 (Step 009850): Train loss 1.923, Val loss 2.647\n",
            "Ep 19 (Step 009900): Train loss 1.888, Val loss 2.642\n",
            "Ep 19 (Step 009950): Train loss 1.863, Val loss 2.634\n",
            "Ep 19 (Step 010000): Train loss 1.913, Val loss 2.634\n",
            "Ep 19 (Step 010050): Train loss 1.902, Val loss 2.648\n",
            "Ep 19 (Step 010100): Train loss 1.852, Val loss 2.624\n",
            "Ep 19 (Step 010150): Train loss 1.829, Val loss 2.624\n",
            "Ep 19 (Step 010200): Train loss 1.846, Val loss 2.618\n",
            "Ep 19 (Step 010250): Train loss 1.804, Val loss 2.602\n",
            "Ep 19 (Step 010300): Train loss 1.806, Val loss 2.590\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to be more than the smaller model.  ### Response: The study compares to a larger model (1B model (1B) with 1.5B parameters, showing that larger models, showing that larger models smaller models smaller models can be\n",
            "Ep 20 (Step 010350): Train loss 1.792, Val loss 2.597\n",
            "Ep 20 (Step 010400): Train loss 1.809, Val loss 2.601\n",
            "Ep 20 (Step 010450): Train loss 1.819, Val loss 2.589\n",
            "Ep 20 (Step 010500): Train loss 1.825, Val loss 2.593\n",
            "Ep 20 (Step 010550): Train loss 1.720, Val loss 2.583\n",
            "Ep 20 (Step 010600): Train loss 1.778, Val loss 2.574\n",
            "Ep 20 (Step 010650): Train loss 1.801, Val loss 2.587\n",
            "Ep 20 (Step 010700): Train loss 1.787, Val loss 2.591\n",
            "Ep 20 (Step 010750): Train loss 1.785, Val loss 2.581\n",
            "Ep 20 (Step 010800): Train loss 1.713, Val loss 2.571\n",
            "Ep 20 (Step 010850): Train loss 1.744, Val loss 2.570\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to be more efficient.  ### Response: The study compares the performance of smaller models on various benchmarks like MMLU and BBH, showing that larger models like GPT-3, despite being smaller size increases, with larger models like G\n",
            "Ep 21 (Step 010900): Train loss 1.719, Val loss 2.564\n",
            "Ep 21 (Step 010950): Train loss 1.735, Val loss 2.562\n",
            "Ep 21 (Step 011000): Train loss 1.699, Val loss 2.555\n",
            "Ep 21 (Step 011050): Train loss 1.700, Val loss 2.559\n",
            "Ep 21 (Step 011100): Train loss 1.702, Val loss 2.552\n",
            "Ep 21 (Step 011150): Train loss 1.731, Val loss 2.556\n",
            "Ep 21 (Step 011200): Train loss 1.706, Val loss 2.547\n",
            "Ep 21 (Step 011250): Train loss 1.683, Val loss 2.541\n",
            "Ep 21 (Step 011300): Train loss 1.737, Val loss 2.535\n",
            "Ep 21 (Step 011350): Train loss 1.667, Val loss 2.530\n",
            "Ep 21 (Step 011400): Train loss 1.635, Val loss 2.523\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to be more efficient.  ### Response: The study examines the performance of smaller models on various benchmarks, showing that larger models with larger models like GPT-3 and Chinchilla, showing that larger models like GPT-3 and more\n",
            "Ep 22 (Step 011450): Train loss 1.641, Val loss 2.518\n",
            "Ep 22 (Step 011500): Train loss 1.654, Val loss 2.518\n",
            "Ep 22 (Step 011550): Train loss 1.633, Val loss 2.506\n",
            "Ep 22 (Step 011600): Train loss 1.598, Val loss 2.494\n",
            "Ep 22 (Step 011650): Train loss 1.657, Val loss 2.509\n",
            "Ep 22 (Step 011700): Train loss 1.638, Val loss 2.514\n",
            "Ep 22 (Step 011750): Train loss 1.644, Val loss 2.502\n",
            "Ep 22 (Step 011800): Train loss 1.631, Val loss 2.492\n",
            "Ep 22 (Step 011850): Train loss 1.653, Val loss 2.512\n",
            "Ep 22 (Step 011900): Train loss 1.646, Val loss 2.500\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to be more efficient.  ### Response: The study compares different language models with varying parameter sizes (1B and FLOPs) with 1.5 billion parameters, showing that larger models trained on various benchmarks, showing better performance improvements in larger\n",
            "Ep 23 (Step 011950): Train loss 1.606, Val loss 2.492\n",
            "Ep 23 (Step 012000): Train loss 1.606, Val loss 2.462\n",
            "Ep 23 (Step 012050): Train loss 1.579, Val loss 2.483\n",
            "Ep 23 (Step 012100): Train loss 1.583, Val loss 2.473\n",
            "Ep 23 (Step 012150): Train loss 1.575, Val loss 2.469\n",
            "Ep 23 (Step 012200): Train loss 1.548, Val loss 2.472\n",
            "Ep 23 (Step 012250): Train loss 1.567, Val loss 2.470\n",
            "Ep 23 (Step 012300): Train loss 1.529, Val loss 2.463\n",
            "Ep 23 (Step 012350): Train loss 1.543, Val loss 2.474\n",
            "Ep 23 (Step 012400): Train loss 1.539, Val loss 2.465\n",
            "Ep 23 (Step 012450): Train loss 1.540, Val loss 2.455\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to train a model, and that it is not be trained on.  ### Response: The study compares the performance of smaller models on various benchmarks with larger models like GPT-3 and Chinchilla, showing that larger models like GPT\n",
            "Ep 24 (Step 012500): Train loss 1.515, Val loss 2.456\n",
            "Ep 24 (Step 012550): Train loss 1.529, Val loss 2.455\n",
            "Ep 24 (Step 012600): Train loss 1.542, Val loss 2.439\n",
            "Ep 24 (Step 012650): Train loss 1.528, Val loss 2.451\n",
            "Ep 24 (Step 012700): Train loss 1.486, Val loss 2.432\n",
            "Ep 24 (Step 012750): Train loss 1.505, Val loss 2.435\n",
            "Ep 24 (Step 012800): Train loss 1.474, Val loss 2.425\n",
            "Ep 24 (Step 012850): Train loss 1.512, Val loss 2.448\n",
            "Ep 24 (Step 012900): Train loss 1.496, Val loss 2.436\n",
            "Ep 24 (Step 012950): Train loss 1.474, Val loss 2.439\n",
            "Ep 24 (Step 013000): Train loss 1.412, Val loss 2.418\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to be close to the optimal model size.  ### Response: The study compares the performance of smaller models with larger models of smaller models with larger sizes, showing that larger models trained on larger models like GPT-3 and PaLM, despite\n",
            "Ep 25 (Step 013050): Train loss 1.458, Val loss 2.411\n",
            "Ep 25 (Step 013100): Train loss 1.445, Val loss 2.407\n",
            "Ep 25 (Step 013150): Train loss 1.443, Val loss 2.422\n",
            "Ep 25 (Step 013200): Train loss 1.435, Val loss 2.401\n",
            "Ep 25 (Step 013250): Train loss 1.443, Val loss 2.403\n",
            "Ep 25 (Step 013300): Train loss 1.421, Val loss 2.388\n",
            "Ep 25 (Step 013350): Train loss 1.431, Val loss 2.387\n",
            "Ep 25 (Step 013400): Train loss 1.417, Val loss 2.399\n",
            "Ep 25 (Step 013450): Train loss 1.409, Val loss 2.378\n",
            "Ep 25 (Step 013500): Train loss 1.395, Val loss 2.379\n",
            "Ep 25 (Step 013550): Train loss 1.396, Val loss 2.372\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to be close to the optimal model size for downstream tasks.  ### Response: The study compares different model sizes of varying sizes (1B, 1.5B, and 1.5B, 1.5B, and 1.5\n",
            "Ep 26 (Step 013600): Train loss 1.356, Val loss 2.381\n",
            "Ep 26 (Step 013650): Train loss 1.428, Val loss 2.385\n",
            "Ep 26 (Step 013700): Train loss 1.353, Val loss 2.370\n",
            "Ep 26 (Step 013750): Train loss 1.393, Val loss 2.381\n",
            "Ep 26 (Step 013800): Train loss 1.381, Val loss 2.365\n",
            "Ep 26 (Step 013850): Train loss 1.344, Val loss 2.367\n",
            "Ep 26 (Step 013900): Train loss 1.373, Val loss 2.373\n",
            "Ep 26 (Step 013950): Train loss 1.330, Val loss 2.362\n",
            "Ep 26 (Step 014000): Train loss 1.340, Val loss 2.355\n",
            "Ep 26 (Step 014050): Train loss 1.350, Val loss 2.352\n",
            "Ep 26 (Step 014100): Train loss 1.299, Val loss 2.351\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to be close-parameter (see Figure 6.1).  ### Response: The study compares the performance of different language models on various benchmarks including language modeling benchmarks, showing that larger models like GPT-3 and Chinchilla, despite\n",
            "Ep 27 (Step 014150): Train loss 1.360, Val loss 2.347\n",
            "Ep 27 (Step 014200): Train loss 1.348, Val loss 2.366\n",
            "Ep 27 (Step 014250): Train loss 1.294, Val loss 2.350\n",
            "Ep 27 (Step 014300): Train loss 1.307, Val loss 2.340\n",
            "Ep 27 (Step 014350): Train loss 1.290, Val loss 2.334\n",
            "Ep 27 (Step 014400): Train loss 1.316, Val loss 2.330\n",
            "Ep 27 (Step 014450): Train loss 1.307, Val loss 2.339\n",
            "Ep 27 (Step 014500): Train loss 1.258, Val loss 2.330\n",
            "Ep 27 (Step 014550): Train loss 1.274, Val loss 2.327\n",
            "Ep 27 (Step 014600): Train loss 1.288, Val loss 2.326\n",
            "Ep 27 (Step 014650): Train loss 1.280, Val loss 2.319\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to train a model, and that larger model, the smaller model is more efficient.  ### Response: The study compares model sizes of 1.5 and 1.0 Pro, showing that larger models with larger size, with 1.0B\n",
            "Ep 28 (Step 014700): Train loss 1.222, Val loss 2.324\n",
            "Ep 28 (Step 014750): Train loss 1.292, Val loss 2.327\n",
            "Ep 28 (Step 014800): Train loss 1.280, Val loss 2.317\n",
            "Ep 28 (Step 014850): Train loss 1.248, Val loss 2.312\n",
            "Ep 28 (Step 014900): Train loss 1.232, Val loss 2.300\n",
            "Ep 28 (Step 014950): Train loss 1.241, Val loss 2.304\n",
            "Ep 28 (Step 015000): Train loss 1.218, Val loss 2.301\n",
            "Ep 28 (Step 015050): Train loss 1.223, Val loss 2.310\n",
            "Ep 28 (Step 015100): Train loss 1.235, Val loss 2.311\n",
            "Ep 28 (Step 015150): Train loss 1.212, Val loss 2.288\n",
            "Ep 28 (Step 015200): Train loss 1.160, Val loss 2.286\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to train a model, and for downstream tasks, as evidenced by the smaller models trained for the smaller models.  ### Response: The study compares different models of similar sizes of different sizes (1.1B and 1B, 1.5\n",
            "Ep 29 (Step 015250): Train loss 1.203, Val loss 2.292\n",
            "Ep 29 (Step 015300): Train loss 1.157, Val loss 2.280\n",
            "Ep 29 (Step 015350): Train loss 1.181, Val loss 2.302\n",
            "Ep 29 (Step 015400): Train loss 1.183, Val loss 2.280\n",
            "Ep 29 (Step 015450): Train loss 1.139, Val loss 2.277\n",
            "Ep 29 (Step 015500): Train loss 1.163, Val loss 2.273\n",
            "Ep 29 (Step 015550): Train loss 1.163, Val loss 2.276\n",
            "Ep 29 (Step 015600): Train loss 1.176, Val loss 2.282\n",
            "Ep 29 (Step 015650): Train loss 1.157, Val loss 2.271\n",
            "Ep 29 (Step 015700): Train loss 1.173, Val loss 2.278\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to be close to the optimal model.  ### Response: The study compares the performance of smaller models with larger models trained on various downstream tasks, showing that larger models trained on downstream tasks, with larger models like GPT-3 and PaLM\n",
            "Ep 30 (Step 015750): Train loss 1.141, Val loss 2.264\n",
            "Ep 30 (Step 015800): Train loss 1.153, Val loss 2.272\n",
            "Ep 30 (Step 015850): Train loss 1.154, Val loss 2.263\n",
            "Ep 30 (Step 015900): Train loss 1.134, Val loss 2.249\n",
            "Ep 30 (Step 015950): Train loss 1.127, Val loss 2.262\n",
            "Ep 30 (Step 016000): Train loss 1.137, Val loss 2.265\n",
            "Ep 30 (Step 016050): Train loss 1.083, Val loss 2.250\n",
            "Ep 30 (Step 016100): Train loss 1.148, Val loss 2.252\n",
            "Ep 30 (Step 016150): Train loss 1.100, Val loss 2.247\n",
            "Ep 30 (Step 016200): Train loss 1.109, Val loss 2.242\n",
            "Ep 30 (Step 016250): Train loss 1.089, Val loss 2.233\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to be close to the best (see Appendix 3.7).  ### Response: The study compares the performance of different language models on various downstream tasks, showing that larger models (1.5B, 1.5B, and 1.\n",
            "Ep 31 (Step 016300): Train loss 1.090, Val loss 2.236\n",
            "Ep 31 (Step 016350): Train loss 1.079, Val loss 2.243\n",
            "Ep 31 (Step 016400): Train loss 1.072, Val loss 2.240\n",
            "Ep 31 (Step 016450): Train loss 1.060, Val loss 2.240\n",
            "Ep 31 (Step 016500): Train loss 1.059, Val loss 2.243\n",
            "Ep 31 (Step 016550): Train loss 1.056, Val loss 2.246\n",
            "Ep 31 (Step 016600): Train loss 1.077, Val loss 2.232\n",
            "Ep 31 (Step 016650): Train loss 1.066, Val loss 2.229\n",
            "Ep 31 (Step 016700): Train loss 1.048, Val loss 2.212\n",
            "Ep 31 (Step 016750): Train loss 1.065, Val loss 2.210\n",
            "Ep 31 (Step 016800): Train loss 1.054, Val loss 2.212\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to be close to the best model.  ### Response: The study compares the performance of different language models, showing that larger models trained on various downstream tasks, showing that larger models trained on a 1.5 billion parameter model, and 1.\n",
            "Ep 32 (Step 016850): Train loss 0.999, Val loss 2.210\n",
            "Ep 32 (Step 016900): Train loss 1.044, Val loss 2.224\n",
            "Ep 32 (Step 016950): Train loss 1.007, Val loss 2.216\n",
            "Ep 32 (Step 017000): Train loss 1.017, Val loss 2.207\n",
            "Ep 32 (Step 017050): Train loss 1.028, Val loss 2.208\n",
            "Ep 32 (Step 017100): Train loss 0.984, Val loss 2.196\n",
            "Ep 32 (Step 017150): Train loss 0.994, Val loss 2.214\n",
            "Ep 32 (Step 017200): Train loss 0.993, Val loss 2.211\n",
            "Ep 32 (Step 017250): Train loss 0.993, Val loss 2.208\n",
            "Ep 32 (Step 017300): Train loss 1.017, Val loss 2.199\n",
            "Ep 32 (Step 017350): Train loss 1.003, Val loss 2.200\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to train a model, and that is not the smaller model, but also for downstream tasks.  ### Response: The study explores the performance of different language models on various downstream tasks, showing that larger models trained on various downstream tasks, with smaller\n",
            "Ep 33 (Step 017400): Train loss 0.967, Val loss 2.171\n",
            "Ep 33 (Step 017450): Train loss 0.980, Val loss 2.205\n",
            "Ep 33 (Step 017500): Train loss 0.972, Val loss 2.183\n",
            "Ep 33 (Step 017550): Train loss 0.966, Val loss 2.189\n",
            "Ep 33 (Step 017600): Train loss 0.955, Val loss 2.188\n",
            "Ep 33 (Step 017650): Train loss 0.986, Val loss 2.198\n",
            "Ep 33 (Step 017700): Train loss 1.007, Val loss 2.188\n",
            "Ep 33 (Step 017750): Train loss 0.966, Val loss 2.186\n",
            "Ep 33 (Step 017800): Train loss 0.978, Val loss 2.174\n",
            "Ep 33 (Step 017850): Train loss 0.931, Val loss 2.163\n",
            "Ep 33 (Step 017900): Train loss 0.963, Val loss 2.169\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to be close to the best model, and for downstream tasks.  ### Response: The study compares the performance of different language models on various downstream tasks, showing that larger models like GPT-3 and FLOPs, despite being smaller size\n",
            "Ep 34 (Step 017950): Train loss 0.915, Val loss 2.174\n",
            "Ep 34 (Step 018000): Train loss 0.935, Val loss 2.155\n",
            "Ep 34 (Step 018050): Train loss 0.965, Val loss 2.165\n",
            "Ep 34 (Step 018100): Train loss 0.922, Val loss 2.164\n",
            "Ep 34 (Step 018150): Train loss 0.912, Val loss 2.166\n",
            "Ep 34 (Step 018200): Train loss 0.910, Val loss 2.171\n",
            "Ep 34 (Step 018250): Train loss 0.887, Val loss 2.152\n",
            "Ep 34 (Step 018300): Train loss 0.903, Val loss 2.164\n",
            "Ep 34 (Step 018350): Train loss 0.901, Val loss 2.171\n",
            "Ep 34 (Step 018400): Train loss 0.879, Val loss 2.147\n",
            "Ep 34 (Step 018450): Train loss 0.894, Val loss 2.147\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to be close to the best model, and for downstream tasks.  ### Response: The study compares various models of varying sizes (1B and 1.5B, 1.5B, 1.5B, and 1.0B\n",
            "Ep 35 (Step 018500): Train loss 0.864, Val loss 2.141\n",
            "Ep 35 (Step 018550): Train loss 0.885, Val loss 2.149\n",
            "Ep 35 (Step 018600): Train loss 0.898, Val loss 2.150\n",
            "Ep 35 (Step 018650): Train loss 0.871, Val loss 2.151\n",
            "Ep 35 (Step 018700): Train loss 0.866, Val loss 2.147\n",
            "Ep 35 (Step 018750): Train loss 0.856, Val loss 2.140\n",
            "Ep 35 (Step 018800): Train loss 0.865, Val loss 2.132\n",
            "Ep 35 (Step 018850): Train loss 0.876, Val loss 2.137\n",
            "Ep 35 (Step 018900): Train loss 0.868, Val loss 2.136\n",
            "Ep 35 (Step 018950): Train loss 0.848, Val loss 2.108\n",
            "Ep 35 (Step 019000): Train loss 0.847, Val loss 2.122\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to be close to the optimal model.  ### Response: The study compares different model sizes (1B, showing that larger models with varying parameters, showing that larger models trained on-scale models like FLOPs, with more parameters, showing\n",
            "Ep 36 (Step 019050): Train loss 0.838, Val loss 2.116\n",
            "Ep 36 (Step 019100): Train loss 0.848, Val loss 2.134\n",
            "Ep 36 (Step 019150): Train loss 0.814, Val loss 2.133\n",
            "Ep 36 (Step 019200): Train loss 0.840, Val loss 2.116\n",
            "Ep 36 (Step 019250): Train loss 0.827, Val loss 2.133\n",
            "Ep 36 (Step 019300): Train loss 0.808, Val loss 2.128\n",
            "Ep 36 (Step 019350): Train loss 0.794, Val loss 2.108\n",
            "Ep 36 (Step 019400): Train loss 0.783, Val loss 2.126\n",
            "Ep 36 (Step 019450): Train loss 0.833, Val loss 2.137\n",
            "Ep 36 (Step 019500): Train loss 0.810, Val loss 2.110\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to be close to the best (see Appendix A.1 for the best vs.7 for downstream tasks).  ### Response: The study examines how scaling laws for different language models (8B and 1.5B) underperforms FL\n",
            "Ep 37 (Step 019550): Train loss 0.800, Val loss 2.098\n",
            "Ep 37 (Step 019600): Train loss 0.794, Val loss 2.103\n",
            "Ep 37 (Step 019650): Train loss 0.791, Val loss 2.099\n",
            "Ep 37 (Step 019700): Train loss 0.802, Val loss 2.102\n",
            "Ep 37 (Step 019750): Train loss 0.787, Val loss 2.096\n",
            "Ep 37 (Step 019800): Train loss 0.785, Val loss 2.101\n",
            "Ep 37 (Step 019850): Train loss 0.770, Val loss 2.090\n",
            "Ep 37 (Step 019900): Train loss 0.769, Val loss 2.099\n",
            "Ep 37 (Step 019950): Train loss 0.770, Val loss 2.091\n",
            "Ep 37 (Step 020000): Train loss 0.733, Val loss 2.077\n",
            "Ep 37 (Step 020050): Train loss 0.783, Val loss 2.087\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to  ### Response: The study compares the performance of different language models on various downstream tasks, showing that larger models trained on downstream tasks, with varying FLOPs, showing consistent performance metrics. The study also observed that larger models trained on downstream\n",
            "Ep 38 (Step 020100): Train loss 0.750, Val loss 2.094\n",
            "Ep 38 (Step 020150): Train loss 0.727, Val loss 2.091\n",
            "Ep 38 (Step 020200): Train loss 0.781, Val loss 2.111\n",
            "Ep 38 (Step 020250): Train loss 0.738, Val loss 2.099\n",
            "Ep 38 (Step 020300): Train loss 0.748, Val loss 2.106\n",
            "Ep 38 (Step 020350): Train loss 0.740, Val loss 2.103\n",
            "Ep 38 (Step 020400): Train loss 0.717, Val loss 2.090\n",
            "Ep 38 (Step 020450): Train loss 0.732, Val loss 2.081\n",
            "Ep 38 (Step 020500): Train loss 0.722, Val loss 2.082\n",
            "Ep 38 (Step 020550): Train loss 0.746, Val loss 2.079\n",
            "Ep 38 (Step 020600): Train loss 0.715, Val loss 2.085\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to  ### Response: The study explores the performance of different model sizes (1B, 1.5B, and 1.5B, showing that larger models (2B) generally outperforms larger sizes (2B, and 1.\n",
            "Ep 39 (Step 020650): Train loss 0.708, Val loss 2.083\n",
            "Ep 39 (Step 020700): Train loss 0.708, Val loss 2.063\n",
            "Ep 39 (Step 020750): Train loss 0.711, Val loss 2.078\n",
            "Ep 39 (Step 020800): Train loss 0.712, Val loss 2.077\n",
            "Ep 39 (Step 020850): Train loss 0.695, Val loss 2.088\n",
            "Ep 39 (Step 020900): Train loss 0.683, Val loss 2.075\n",
            "Ep 39 (Step 020950): Train loss 0.702, Val loss 2.067\n",
            "Ep 39 (Step 021000): Train loss 0.707, Val loss 2.090\n",
            "Ep 39 (Step 021050): Train loss 0.680, Val loss 2.064\n",
            "Ep 39 (Step 021100): Train loss 0.689, Val loss 2.071\n",
            "Ep 39 (Step 021150): Train loss 0.681, Val loss 2.066\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to  ### Response: The study examines the performance of different model sizes, showing that larger models with 1.5 billion parameters, showing that larger models trained on downstream tasks, showing that larger models can be more efficient. However, Table 3 shows\n",
            "Ep 40 (Step 021200): Train loss 0.691, Val loss 2.060\n",
            "Ep 40 (Step 021250): Train loss 0.664, Val loss 2.063\n",
            "Ep 40 (Step 021300): Train loss 0.670, Val loss 2.074\n",
            "Ep 40 (Step 021350): Train loss 0.652, Val loss 2.054\n",
            "Ep 40 (Step 021400): Train loss 0.660, Val loss 2.058\n",
            "Ep 40 (Step 021450): Train loss 0.676, Val loss 2.055\n",
            "Ep 40 (Step 021500): Train loss 0.674, Val loss 2.064\n",
            "Ep 40 (Step 021550): Train loss 0.641, Val loss 2.053\n",
            "Ep 40 (Step 021600): Train loss 0.667, Val loss 2.044\n",
            "Ep 40 (Step 021650): Train loss 0.639, Val loss 2.051\n",
            "Ep 40 (Step 021700): Train loss 0.657, Val loss 2.051\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to be close to the best model, and for downstream tasks.  ### Response: The study compares the performance of various language models on various downstream tasks, showing that scaling laws for various downstream tasks, with model size, and training loss is closest\n",
            "Ep 41 (Step 021750): Train loss 0.637, Val loss 2.042\n",
            "Ep 41 (Step 021800): Train loss 0.629, Val loss 2.057\n",
            "Ep 41 (Step 021850): Train loss 0.650, Val loss 2.068\n",
            "Ep 41 (Step 021900): Train loss 0.627, Val loss 2.058\n",
            "Ep 41 (Step 021950): Train loss 0.626, Val loss 2.061\n",
            "Ep 41 (Step 022000): Train loss 0.622, Val loss 2.047\n",
            "Ep 41 (Step 022050): Train loss 0.624, Val loss 2.047\n",
            "Ep 41 (Step 022100): Train loss 0.628, Val loss 2.057\n",
            "Ep 41 (Step 022150): Train loss 0.623, Val loss 2.051\n",
            "Ep 41 (Step 022200): Train loss 0.604, Val loss 2.040\n",
            "Ep 41 (Step 022250): Train loss 0.606, Val loss 2.037\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to  ### Response: The study examines the performance of different language models on various downstream tasks, showing that larger models trained on downstream tasks like FLOPs (2B and FLOPs) but generally outperforms FLOPs on FLOP\n",
            "Ep 42 (Step 022300): Train loss 0.615, Val loss 2.038\n",
            "Ep 42 (Step 022350): Train loss 0.587, Val loss 2.042\n",
            "Ep 42 (Step 022400): Train loss 0.602, Val loss 2.044\n",
            "Ep 42 (Step 022450): Train loss 0.597, Val loss 2.043\n",
            "Ep 42 (Step 022500): Train loss 0.600, Val loss 2.046\n",
            "Ep 42 (Step 022550): Train loss 0.605, Val loss 2.051\n",
            "Ep 42 (Step 022600): Train loss 0.604, Val loss 2.045\n",
            "Ep 42 (Step 022650): Train loss 0.592, Val loss 2.044\n",
            "Ep 42 (Step 022700): Train loss 0.596, Val loss 2.044\n",
            "Ep 42 (Step 022750): Train loss 0.577, Val loss 2.035\n",
            "Ep 42 (Step 022800): Train loss 0.571, Val loss 2.040\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to  ### Response: The study compares the performance of different language models on various downstream tasks, showing that larger models trained on downstream tasks, showing that larger models trained on more downstream tasks, while larger models smaller size remains more accessible. However,\n",
            "Ep 43 (Step 022850): Train loss 0.554, Val loss 2.036\n",
            "Ep 43 (Step 022900): Train loss 0.561, Val loss 2.040\n",
            "Ep 43 (Step 022950): Train loss 0.562, Val loss 2.037\n",
            "Ep 43 (Step 023000): Train loss 0.569, Val loss 2.031\n",
            "Ep 43 (Step 023050): Train loss 0.591, Val loss 2.038\n",
            "Ep 43 (Step 023100): Train loss 0.539, Val loss 2.028\n",
            "Ep 43 (Step 023150): Train loss 0.574, Val loss 2.020\n",
            "Ep 43 (Step 023200): Train loss 0.568, Val loss 2.021\n",
            "Ep 43 (Step 023250): Train loss 0.568, Val loss 2.014\n",
            "Ep 43 (Step 023300): Train loss 0.545, Val loss 2.021\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to  ### Response: The study explores the optimal model's performance on downstream tasks, showing that larger models (1.5B, and 1.5B) show that larger models (1B and 1.0B) perform better than\n",
            "Ep 44 (Step 023350): Train loss 0.546, Val loss 2.020\n",
            "Ep 44 (Step 023400): Train loss 0.548, Val loss 2.013\n",
            "Ep 44 (Step 023450): Train loss 0.543, Val loss 2.025\n",
            "Ep 44 (Step 023500): Train loss 0.518, Val loss 2.019\n",
            "Ep 44 (Step 023550): Train loss 0.519, Val loss 2.021\n",
            "Ep 44 (Step 023600): Train loss 0.536, Val loss 2.025\n",
            "Ep 44 (Step 023650): Train loss 0.521, Val loss 2.024\n",
            "Ep 44 (Step 023700): Train loss 0.547, Val loss 2.036\n",
            "Ep 44 (Step 023750): Train loss 0.526, Val loss 2.024\n",
            "Ep 44 (Step 023800): Train loss 0.507, Val loss 2.010\n",
            "Ep 44 (Step 023850): Train loss 0.513, Val loss 2.009\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to  ### Response: The study examines the performance of various language models using various metrics, showing that optimal model size, and training loss achieves comparable performance with larger models like FLOPs, and 1.5B, with more parameters, showing\n",
            "Ep 45 (Step 023900): Train loss 0.514, Val loss 1.995\n",
            "Ep 45 (Step 023950): Train loss 0.515, Val loss 2.024\n",
            "Ep 45 (Step 024000): Train loss 0.522, Val loss 2.009\n",
            "Ep 45 (Step 024050): Train loss 0.514, Val loss 2.011\n",
            "Ep 45 (Step 024100): Train loss 0.501, Val loss 2.007\n",
            "Ep 45 (Step 024150): Train loss 0.496, Val loss 2.005\n",
            "Ep 45 (Step 024200): Train loss 0.488, Val loss 2.005\n",
            "Ep 45 (Step 024250): Train loss 0.501, Val loss 2.011\n",
            "Ep 45 (Step 024300): Train loss 0.497, Val loss 2.025\n",
            "Ep 45 (Step 024350): Train loss 0.495, Val loss 2.009\n",
            "Ep 45 (Step 024400): Train loss 0.494, Val loss 2.012\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to  ### Response: The study examines the performance of different language models on downstream tasks, showing that larger models perform on downstream tasks, with model size, and also decreases with model size, showing consistent improvements in downstream tasks. The study also note\n",
            "Ep 46 (Step 024450): Train loss 0.484, Val loss 2.015\n",
            "Ep 46 (Step 024500): Train loss 0.458, Val loss 2.018\n",
            "Ep 46 (Step 024550): Train loss 0.483, Val loss 2.005\n",
            "Ep 46 (Step 024600): Train loss 0.478, Val loss 2.018\n",
            "Ep 46 (Step 024650): Train loss 0.488, Val loss 1.999\n",
            "Ep 46 (Step 024700): Train loss 0.466, Val loss 2.000\n",
            "Ep 46 (Step 024750): Train loss 0.481, Val loss 1.998\n",
            "Ep 46 (Step 024800): Train loss 0.479, Val loss 2.004\n",
            "Ep 46 (Step 024850): Train loss 0.459, Val loss 1.999\n",
            "Ep 46 (Step 024900): Train loss 0.464, Val loss 2.005\n",
            "Ep 46 (Step 024950): Train loss 0.473, Val loss 2.008\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to be close to the optimal model size for downstream tasks.  ### Response: The study reports the optimal model size for downstream tasks, showing that optimal training loss loss is more efficient, but slightly underperforms optimal model size, and downstream metrics\n",
            "Ep 47 (Step 025000): Train loss 0.457, Val loss 1.995\n",
            "Ep 47 (Step 025050): Train loss 0.437, Val loss 2.007\n",
            "Ep 47 (Step 025100): Train loss 0.446, Val loss 2.002\n",
            "Ep 47 (Step 025150): Train loss 0.459, Val loss 2.023\n",
            "Ep 47 (Step 025200): Train loss 0.451, Val loss 2.006\n",
            "Ep 47 (Step 025250): Train loss 0.435, Val loss 1.994\n",
            "Ep 47 (Step 025300): Train loss 0.455, Val loss 1.998\n",
            "Ep 47 (Step 025350): Train loss 0.446, Val loss 1.994\n",
            "Ep 47 (Step 025400): Train loss 0.433, Val loss 1.995\n",
            "Ep 47 (Step 025450): Train loss 0.443, Val loss 2.002\n",
            "Ep 47 (Step 025500): Train loss 0.421, Val loss 1.985\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to  ### Response: The study examines the performance of different language models on downstream tasks, showing that larger models, with 1.5 billion parameters, and 1.0 Pro model, and 1.5B parameters, despite having fewer parameters,\n",
            "Ep 48 (Step 025550): Train loss 0.421, Val loss 2.000\n",
            "Ep 48 (Step 025600): Train loss 0.432, Val loss 2.003\n",
            "Ep 48 (Step 025650): Train loss 0.431, Val loss 1.994\n",
            "Ep 48 (Step 025700): Train loss 0.412, Val loss 2.000\n",
            "Ep 48 (Step 025750): Train loss 0.427, Val loss 1.999\n",
            "Ep 48 (Step 025800): Train loss 0.417, Val loss 1.994\n",
            "Ep 48 (Step 025850): Train loss 0.402, Val loss 1.992\n",
            "Ep 48 (Step 025900): Train loss 0.414, Val loss 1.996\n",
            "Ep 48 (Step 025950): Train loss 0.415, Val loss 1.996\n",
            "Ep 48 (Step 026000): Train loss 0.415, Val loss 2.011\n",
            "Ep 48 (Step 026050): Train loss 0.414, Val loss 2.003\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to  ### Response: The study compares the performance of various models on downstream tasks, especially in zero-shot, and zero-shot settings, showing that larger models perform better on downstream tasks, despite being smaller (with better than 1.5\n",
            "Ep 49 (Step 026100): Train loss 0.404, Val loss 1.993\n",
            "Ep 49 (Step 026150): Train loss 0.389, Val loss 1.994\n",
            "Ep 49 (Step 026200): Train loss 0.401, Val loss 1.994\n",
            "Ep 49 (Step 026250): Train loss 0.399, Val loss 1.999\n",
            "Ep 49 (Step 026300): Train loss 0.407, Val loss 1.996\n",
            "Ep 49 (Step 026350): Train loss 0.406, Val loss 2.001\n",
            "Ep 49 (Step 026400): Train loss 0.420, Val loss 2.002\n",
            "Ep 49 (Step 026450): Train loss 0.399, Val loss 2.001\n",
            "Ep 49 (Step 026500): Train loss 0.403, Val loss 1.987\n",
            "Ep 49 (Step 026550): Train loss 0.391, Val loss 1.993\n",
            "Ep 49 (Step 026600): Train loss 0.398, Val loss 1.996\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to  ### Response: The study explores the optimal model parameters for downstream tasks, with a 1.5 billion parameters, showing that larger models (6B parameters) perform better on downstream tasks, especially on downstream tasks, especially in downstream tasks like\n",
            "Ep 50 (Step 026650): Train loss 0.383, Val loss 1.997\n",
            "Ep 50 (Step 026700): Train loss 0.375, Val loss 1.998\n",
            "Ep 50 (Step 026750): Train loss 0.387, Val loss 1.996\n",
            "Ep 50 (Step 026800): Train loss 0.370, Val loss 1.993\n",
            "Ep 50 (Step 026850): Train loss 0.391, Val loss 1.992\n",
            "Ep 50 (Step 026900): Train loss 0.381, Val loss 1.987\n",
            "Ep 50 (Step 026950): Train loss 0.376, Val loss 1.980\n",
            "Ep 50 (Step 027000): Train loss 0.370, Val loss 1.990\n",
            "Ep 50 (Step 027050): Train loss 0.367, Val loss 1.992\n",
            "Ep 50 (Step 027100): Train loss 0.377, Val loss 1.982\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to  ### Response: The study explores the performance of different models using various downstream metrics, showing that a 1-token prediction loss is not due to scaling loss, suggesting that a 1-token pass loss loss is not present in downstream metrics,\n",
            "Ep 51 (Step 027150): Train loss 0.370, Val loss 1.994\n",
            "Ep 51 (Step 027200): Train loss 0.359, Val loss 1.986\n",
            "Ep 51 (Step 027250): Train loss 0.366, Val loss 1.989\n",
            "Ep 51 (Step 027300): Train loss 0.350, Val loss 1.991\n",
            "Ep 51 (Step 027350): Train loss 0.349, Val loss 1.995\n",
            "Ep 51 (Step 027400): Train loss 0.349, Val loss 1.998\n",
            "Ep 51 (Step 027450): Train loss 0.358, Val loss 1.988\n",
            "Ep 51 (Step 027500): Train loss 0.339, Val loss 1.996\n",
            "Ep 51 (Step 027550): Train loss 0.344, Val loss 1.994\n",
            "Ep 51 (Step 027600): Train loss 0.345, Val loss 1.984\n",
            "Ep 51 (Step 027650): Train loss 0.355, Val loss 2.002\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to  ### Response: The study explores the performance of different language models on downstream tasks, showing that a 1-token prediction loss improves model, with increases in performance loss and downstream tasks, showing that larger models perform well on downstream tasks, despite\n",
            "Ep 52 (Step 027700): Train loss 0.358, Val loss 1.992\n",
            "Ep 52 (Step 027750): Train loss 0.333, Val loss 1.987\n",
            "Ep 52 (Step 027800): Train loss 0.337, Val loss 1.987\n",
            "Ep 52 (Step 027850): Train loss 0.331, Val loss 1.984\n",
            "Ep 52 (Step 027900): Train loss 0.335, Val loss 1.989\n",
            "Ep 52 (Step 027950): Train loss 0.334, Val loss 1.995\n",
            "Ep 52 (Step 028000): Train loss 0.319, Val loss 1.993\n",
            "Ep 52 (Step 028050): Train loss 0.314, Val loss 1.999\n",
            "Ep 52 (Step 028100): Train loss 0.333, Val loss 1.991\n",
            "Ep 52 (Step 028150): Train loss 0.329, Val loss 1.985\n",
            "Ep 52 (Step 028200): Train loss 0.317, Val loss 1.987\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to  ### Response: The study explores the optimal model's performance across downstream tasks, showing that larger models (1.5B, and 1.5B, despite having fewer FLOPs (FLOPs), suggesting that smaller models perform\n",
            "Ep 53 (Step 028250): Train loss 0.308, Val loss 1.977\n",
            "Ep 53 (Step 028300): Train loss 0.313, Val loss 1.989\n",
            "Ep 53 (Step 028350): Train loss 0.323, Val loss 1.994\n",
            "Ep 53 (Step 028400): Train loss 0.320, Val loss 1.987\n",
            "Ep 53 (Step 028450): Train loss 0.327, Val loss 2.004\n",
            "Ep 53 (Step 028500): Train loss 0.308, Val loss 1.989\n",
            "Ep 53 (Step 028550): Train loss 0.310, Val loss 1.984\n",
            "Ep 53 (Step 028600): Train loss 0.325, Val loss 1.987\n",
            "Ep 53 (Step 028650): Train loss 0.309, Val loss 1.996\n",
            "Ep 53 (Step 028700): Train loss 0.309, Val loss 1.980\n",
            "Ep 53 (Step 028750): Train loss 0.304, Val loss 1.980\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to  ### Response: The study explores the optimal model's training loss for downstream tasks, examining its performance on downstream tasks, showing that larger models trained on downstream tasks, particularly focusing on downstream tasks, while also exhibit similar loss function selection rather than\n",
            "Ep 54 (Step 028800): Train loss 0.293, Val loss 1.997\n",
            "Ep 54 (Step 028850): Train loss 0.293, Val loss 1.994\n",
            "Ep 54 (Step 028900): Train loss 0.292, Val loss 1.991\n",
            "Ep 54 (Step 028950): Train loss 0.317, Val loss 1.996\n",
            "Ep 54 (Step 029000): Train loss 0.292, Val loss 1.990\n",
            "Ep 54 (Step 029050): Train loss 0.312, Val loss 1.999\n",
            "Ep 54 (Step 029100): Train loss 0.297, Val loss 1.992\n",
            "Ep 54 (Step 029150): Train loss 0.293, Val loss 1.985\n",
            "Ep 54 (Step 029200): Train loss 0.286, Val loss 1.995\n",
            "Ep 54 (Step 029250): Train loss 0.274, Val loss 1.988\n",
            "Ep 54 (Step 029300): Train loss 0.282, Val loss 1.982\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to  ### Response: The study explores the optimal model size for downstream tasks, showing that the optimal model size, showing that larger models perform on downstream tasks, with similar size, and FLOPs on downstream tasks, though scaling laws are detailed\n",
            "Ep 55 (Step 029350): Train loss 0.291, Val loss 1.988\n",
            "Ep 55 (Step 029400): Train loss 0.293, Val loss 1.991\n",
            "Ep 55 (Step 029450): Train loss 0.278, Val loss 1.982\n",
            "Ep 55 (Step 029500): Train loss 0.286, Val loss 1.989\n",
            "Ep 55 (Step 029550): Train loss 0.284, Val loss 1.997\n",
            "Ep 55 (Step 029600): Train loss 0.276, Val loss 1.979\n",
            "Ep 55 (Step 029650): Train loss 0.283, Val loss 1.993\n",
            "Ep 55 (Step 029700): Train loss 0.292, Val loss 1.998\n",
            "Ep 55 (Step 029750): Train loss 0.273, Val loss 1.990\n",
            "Ep 55 (Step 029800): Train loss 0.270, Val loss 1.982\n",
            "Ep 55 (Step 029850): Train loss 0.279, Val loss 1.991\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to  ### Response: The study explores the performance of different language models on downstream tasks, showing that increasing model size, model size, and FLOPs on downstream tasks, showing that larger models' performance gains with comparable sizes (8B,\n",
            "🟨 saved → gpt2_finetuned.pth\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gpt2 = True\n",
        "model_gpt2 = model_gpt2.to(device)\n",
        "model_gpt2.train()\n",
        "\n",
        "optimizer_gpt2 = torch.optim.AdamW(model_gpt2.parameters(), lr=5e-5, weight_decay=0.1)\n",
        "\n",
        "torch.manual_seed(123)\n",
        "train_losses_g2, val_losses_g2, tokens_g2 = train_model_simple(\n",
        "    model_gpt2, train_loader_gpt2, val_loader_gpt2, optimizer_gpt2, device,\n",
        "    num_epochs=55, eval_freq=50, eval_iter=20,\n",
        "    start_context=format_input(val_set[0]), tokenizer=tokenizer_gpt2)\n",
        "\n",
        "torch.save(model_gpt2.state_dict(), \"models/gpt2_finetuned.pth\")\n",
        "print(\"🟨 saved → models/gpt2_finetuned.pth\")\n",
        "\n",
        "epochs_tensor = torch.linspace(0, 55, len(train_losses_g2))\n",
        "plot_losses(epochs_tensor, tokens_g2, train_losses_g2, val_losses_g2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5b19268",
      "metadata": {},
      "source": [
        "### Step 4: Load the fine-tuned Regex and GPT-2 models from saved checkpoints\n",
        "\n",
        "This step restores the previously trained models (`regex_finetuned.pth` and `gpt2_finetuned.pth`) so they can be used later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "d9b1e073",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🟦 Regex finetuned model loaded.\n",
            "🟨 GPT-2 finetuned model loaded.\n"
          ]
        }
      ],
      "source": [
        "GPT_CONFIG = {\n",
        "    \"vocab_size\": 1000,  # placeholder, will be updated\n",
        "    \"context_length\": 1024,\n",
        "    \"emb_dim\": 768,\n",
        "    \"n_heads\": 12,\n",
        "    \"n_layers\": 12,\n",
        "    \"drop_rate\": 0.3,\n",
        "    \"qkv_bias\": False}\n",
        "\n",
        "# === Load fine-tuned Regex model ===\n",
        "gpt2 = False\n",
        "tokenizer_v2 = SimpleTokenizerV2(vocab)\n",
        "vocab_size = len(tokenizer_v2.tokens2ids)\n",
        "GPT_CONFIG[\"vocab_size\"] = vocab_size\n",
        "\n",
        "model_regex = GPTModel(GPT_CONFIG).to(device)\n",
        "model_regex.load_state_dict(torch.load(\"regex_finetuned.pth\", map_location=device))\n",
        "model_regex.eval()\n",
        "print(\"🟦 Regex finetuned model loaded.\")\n",
        "\n",
        "# === Load fine-tuned GPT-2 model ===\n",
        "gpt2 = True\n",
        "tokenizer_gpt2 = tiktoken.get_encoding(\"gpt2\")\n",
        "vocab_size = tokenizer_gpt2.n_vocab\n",
        "GPT_CONFIG[\"vocab_size\"] = vocab_size\n",
        "\n",
        "model_gpt2 = GPTModel(GPT_CONFIG).to(device)\n",
        "model_gpt2.load_state_dict(torch.load(\"gpt2_finetuned.pth\", map_location=device))\n",
        "model_gpt2.eval()\n",
        "print(\"🟨 GPT-2 finetuned model loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b88c00ef",
      "metadata": {},
      "source": [
        "### Step 5: Generating Model Responses on the Validation Set\n",
        "\n",
        "- The function `generate_text()` uses the helper `generate_text_simple()` to autoregressively predict the next tokens up to `max_new_tokens = 256`.\n",
        "- Each prompt follows the **Alpaca-style format**, combining:\n",
        "  - `### Instruction:` (task description)\n",
        "  - `### Input:` (optional contextual passage)\n",
        "  - `### Response:` (expected model output)\n",
        "- Each generated response is stored under the key `\"model_response\"` for later evaluation.\n",
        "- The resulting subset is saved as **`gpt2_val_responses.json`**.\n",
        "\n",
        "This step effectively simulates how the fine-tuned model would respond to unseen instructions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "c65e15a4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_text(model, tokenizer, prompt, max_new_tokens=256):\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "    encoded = text_to_token_ids(prompt, tokenizer).to(device)\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate_text_simple(model=model, idx=encoded, max_new_tokens=max_new_tokens, context_size=context_size)\n",
        "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    return decoded_text[len(prompt):].replace(\"### Response:\", \"\").strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e358ef4",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating models responses: 100%|██████████| 500/500 [25:00<00:00,  3.00s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🟦 Saved Regex model responses → regex_val_responses.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating models responses: 100%|██████████| 500/500 [29:36<00:00,  3.55s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🟨 Saved GPT-2 model responses → gpt2_val_responses.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# debug_subset = val_set[:15] # use only first 15 entries for quick testing.\n",
        "\n",
        "# === Generate Regex model responses ===\n",
        "gpt2 = False\n",
        "for i, entry in tqdm(enumerate(val_set), total=len(val_set), desc=\"Generating models responses\"):\n",
        "    alpaca_prompt = (\n",
        "        f\"Below is an instruction that describes a task. \"\n",
        "        f\"Write a response that appropriately completes the request.\"\n",
        "        f\"\\n\\n### Instruction:\\n{entry['Instruction']}\")\n",
        "    if entry['Input']:\n",
        "        alpaca_prompt += f\"\\n\\n### Input:\\n{entry['Input']}\"\n",
        "    alpaca_prompt += \"\\n\\n### Response:\\n\"\n",
        "\n",
        "    entry[\"model_response\"] = generate_text(model_regex, tokenizer_v2, alpaca_prompt)\n",
        "\n",
        "with open(\"data\\regex_val_responses.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(val_set, f, indent=2, ensure_ascii=False)\n",
        "print(\"🟦 Saved Regex model responses → data\\regex_val_responses.json\")\n",
        "\n",
        "\n",
        "# === Generate GPT-2 model responses ===\n",
        "gpt2 = True\n",
        "for i, entry in tqdm(enumerate(val_set), total=len(val_set), desc=\"Generating models responses\"):\n",
        "    alpaca_prompt = (\n",
        "        f\"Below is an instruction that describes a task. \"\n",
        "        f\"Write a response that appropriately completes the request.\"\n",
        "        f\"\\n\\n### Instruction:\\n{entry['Instruction']}\")\n",
        "    if entry['Input']:\n",
        "        alpaca_prompt += f\"\\n\\n### Input:\\n{entry['Input']}\"\n",
        "    alpaca_prompt += \"\\n\\n### Response:\\n\"\n",
        "\n",
        "    entry[\"model_response\"] = generate_text(model_gpt2, tokenizer_gpt2, alpaca_prompt)\n",
        "\n",
        "with open(\"data\\gpt2_val_responses.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(val_set, f, indent=2, ensure_ascii=False)\n",
        "print(\"🟨 Saved GPT-2 model responses → data\\gpt2_val_responses.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05992a25",
      "metadata": {},
      "source": [
        "### Step 6: Loading Saved Model Response Validation Files\n",
        "\n",
        "Here we load the JSON files containing the generated validation responses from both fine-tuned models:\n",
        "\n",
        "- **Regex Model:** `Regex_val_responses.json`\n",
        "- **GPT-2 Model:** `gpt2_val_responses.json`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "1a280ba1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🟦 Loaded 500 Regex val responses.\n",
            "🟨 Loaded 500 GPT-2 val responses.\n"
          ]
        }
      ],
      "source": [
        "# Load Regex model responses\n",
        "with open(\"regex_val_responses.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    regex_val_responses = json.load(f)\n",
        "print(f\"🟦 Loaded {len(regex_val_responses)} Regex val responses.\")\n",
        "\n",
        "# Load GPT-2 model responses\n",
        "with open(\"gpt2_val_responses.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    gpt2_val_responses = json.load(f)\n",
        "print(f\"🟨 Loaded {len(gpt2_val_responses)} GPT-2 val responses.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70747227",
      "metadata": {},
      "source": [
        "## Part 4 — Evaluation on the Validation Set"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f23ccfa",
      "metadata": {},
      "source": [
        "We evaluate the fine-tuned models using:\n",
        "1.  **Automatic scoring (Quantitatively):** Using **BLEU-4**, **ROUGE-1/2/L-F1**, **METEOR**, **Token-F1**, and **BERTScore** metrics.\n",
        "2.  **LLM-based evaluation (Qualitatively):** Using an LLM model (**`Phi-3-mini-4k-instruct`**) as a judge to grade responses from 0–100.\n",
        "\n",
        "This helps quantify how well the model follows instructions compared to ground-truth responses."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7ece86a",
      "metadata": {},
      "source": [
        "### Step 1: Automatic scoring (Quantitatively).\n",
        "- Using **BLEU-4**, **ROUGE-1/2/L-F1**, **METEOR**, **Token-F1**, and **BERTScore** metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d09202f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List, Dict, Tuple, Optional\n",
        "import re, random\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import sacrebleu\n",
        "from rouge_score import rouge_scorer\n",
        "import nltk\n",
        "from nltk.translate.meteor_score import meteor_score as nltk_meteor_score\n",
        "from bert_score import score as bertscore_score\n",
        "\n",
        "# -------- Utilities --------\n",
        "def _flat(s: Optional[str]) -> str:\n",
        "    return (s or \"\").strip()\n",
        "\n",
        "def _normalize(s: str) -> str:\n",
        "    return re.sub(r\"\\s+\", \" \", s.lower().strip())\n",
        "\n",
        "def _fmt(x, nd=3):\n",
        "    try: return f\"{float(x):.{nd}f}\"\n",
        "    except Exception: return \"n/a\"\n",
        "\n",
        "# -------- Metrics ----------\n",
        "def compute_bleu(hyps: List[str], refs: List[str]) -> Dict:\n",
        "    bleu = sacrebleu.corpus_bleu(hyps, [refs])\n",
        "    return {\n",
        "        \"BLEU-4\": float(bleu.score),\n",
        "        \"P1\": float(bleu.precisions[0]),\n",
        "        \"P2\": float(bleu.precisions[1]),\n",
        "        \"P3\": float(bleu.precisions[2]),\n",
        "        \"P4\": float(bleu.precisions[3]),\n",
        "        \"BP\": float(bleu.bp),\n",
        "        \"sys_len\": int(bleu.sys_len),\n",
        "        \"ref_len\": int(bleu.ref_len),\n",
        "    }\n",
        "\n",
        "def compute_rouge(hyps: List[str], refs: List[str]) -> Dict:\n",
        "    scorer = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeL\"], use_stemmer=True)\n",
        "    r1 = r2 = rl = 0.0\n",
        "    for h, r in zip(hyps, refs):\n",
        "        s = scorer.score(r, h)\n",
        "        r1 += s[\"rouge1\"].fmeasure; r2 += s[\"rouge2\"].fmeasure; rl += s[\"rougeL\"].fmeasure\n",
        "    n = max(1, len(hyps))\n",
        "    return {\"ROUGE-1-F1\": r1/n, \"ROUGE-2-F1\": r2/n, \"ROUGE-L-F1\": rl/n}\n",
        "\n",
        "def compute_meteor(hyps: List[str], refs: List[str]) -> Dict:\n",
        "    for pkg in [\"corpora/wordnet\", \"corpora/omw-1.4\"]:\n",
        "        try: nltk.data.find(pkg)\n",
        "        except LookupError: nltk.download(pkg.split(\"/\")[1], quiet=True)\n",
        "    scores = [nltk_meteor_score([_flat(r).split()], _flat(h).split())\n",
        "              for h, r in zip(hyps, refs) if _flat(h) and _flat(r)]\n",
        "    return {\"METEOR\": float(sum(scores)/len(scores)) if scores else 0.0, \"used\": len(scores)}\n",
        "\n",
        "def compute_token_f1(hyps: List[str], refs: List[str]) -> Dict:\n",
        "    tp=fp=fn=0\n",
        "    for h, r in zip(hyps, refs):\n",
        "        ch, cr = Counter(_flat(h).split()), Counter(_flat(r).split())\n",
        "        common = ch & cr\n",
        "        tp += sum(common.values()); fp += sum((ch - cr).values()); fn += sum((cr - ch).values())\n",
        "    prec = tp/(tp+fp) if tp+fp else 0.0\n",
        "    rec  = tp/(tp+fn) if tp+fn else 0.0\n",
        "    f1   = 2*prec*rec/(prec+rec) if prec+rec else 0.0\n",
        "    return {\"precision\": prec, \"recall\": rec, \"F1\": f1}\n",
        "\n",
        "def compute_bertscore(hyps: List[str], refs: List[str]) -> Dict:\n",
        "    P,R,F1 = bertscore_score(hyps, refs, lang=\"en\",\n",
        "                             model_type=\"roberta-large\",\n",
        "                             rescale_with_baseline=True)\n",
        "    return {\"P\": float(P.mean()), \"R\": float(R.mean()), \"F1\": float(F1.mean()),\n",
        "            \"model_type\": \"roberta-large\", \"baseline_rescale\": True}\n",
        "\n",
        "def compute_all_metrics(hyps: List[str], refs: List[str]) -> Dict:\n",
        "    return {\n",
        "        \"BLEU\": compute_bleu(hyps, refs),\n",
        "        \"ROUGE\": compute_rouge(hyps, refs),\n",
        "        \"METEOR\": compute_meteor(hyps, refs),\n",
        "        \"TokenF1\": compute_token_f1(hyps, refs),\n",
        "        \"BERTScore\": compute_bertscore(hyps, refs),\n",
        "        \"count\": len(hyps)\n",
        "    }\n",
        "\n",
        "# -------- Helpers --------\n",
        "def _pick(d: dict, keys: List[str]) -> Optional[str]:\n",
        "    for k in keys:\n",
        "        if k in d and d[k] is not None:\n",
        "            return str(d[k])\n",
        "    return None\n",
        "\n",
        "def _extract_pairs(dataset: List[dict],\n",
        "                   pred_keys=(\"model_response\",\"prediction\",\"output\",\"hypothesis\",\"hyp\"),\n",
        "                   ref_keys=(\"Response\",\"response\",\"reference\",\"ref\",\"target\",\"gold\"),\n",
        "                   normalize_text: bool=False) -> Tuple[List[str], List[str]]:\n",
        "    hyps, refs = [], []\n",
        "    for ex in dataset:\n",
        "        h = _pick(ex, pred_keys) or \"\"\n",
        "        r = _pick(ex, ref_keys) or \"\"\n",
        "        if normalize_text: h, r = _normalize(h), _normalize(r)\n",
        "        hyps.append(h); refs.append(r)\n",
        "    return hyps, refs\n",
        "\n",
        "# -------- Output --------\n",
        "def _print_metrics_table(model_name: str, M: Dict):\n",
        "    print(f\"\\n=== {model_name} Quantitative Metrics ===\")\n",
        "    b = M[\"BLEU\"]\n",
        "    print(f\"BLEU-4:    {_fmt(b['BLEU-4'],2)}  (BP={_fmt(b['BP'],2)}, P1={_fmt(b['P1'],2)}, P2={_fmt(b['P2'],2)}, P3={_fmt(b['P3'],2)}, P4={_fmt(b['P4'],2)})\")\n",
        "    r = M[\"ROUGE\"]\n",
        "    print(f\"ROUGE-1/2/L-F1: {_fmt(r['ROUGE-1-F1'])} / {_fmt(r['ROUGE-2-F1'])} / {_fmt(r['ROUGE-L-F1'])}\")\n",
        "    m = M[\"METEOR\"]\n",
        "    print(f\"METEOR:    {_fmt(m['METEOR'])} (used {m['used']})\")\n",
        "    tf1 = M[\"TokenF1\"]; print(f\"Token-F1:  {_fmt(tf1['F1'])} (P={_fmt(tf1['precision'])}, R={_fmt(tf1['recall'])})\")\n",
        "    bs = M[\"BERTScore\"]\n",
        "    print(f\"BERTScore: {_fmt(bs['F1'])} (P={_fmt(bs['P'])}, R={_fmt(bs['R'])}, model={bs['model_type']}, baseline={bs['baseline_rescale']})\")\n",
        "    print(f\"Count:     {M['count']}\")\n",
        "\n",
        "# -------- Main --------\n",
        "def evaluate_model_outputs(model_name: str, dataset: List[dict], *, normalize_text=False, seed=1234) -> Dict:\n",
        "    random.seed(seed); np.random.seed(seed)\n",
        "    try:\n",
        "        import torch; torch.manual_seed(seed)\n",
        "    except Exception:\n",
        "        pass\n",
        "    hyps, refs = _extract_pairs(dataset, normalize_text=normalize_text)\n",
        "    M = compute_all_metrics(hyps, refs)\n",
        "    _print_metrics_table(model_name, M)\n",
        "    return M"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "067a73d5",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Regex Quantitative Metrics ===\n",
            "BLEU-4:    4.85  (BP=1.00, P1=24.31, P2=7.57, P3=2.73, P4=1.10)\n",
            "ROUGE-1/2/L-F1: 0.379 / 0.111 / 0.197\n",
            "METEOR:    0.232 (used 500)\n",
            "Token-F1:  0.296 (P=0.248, R=0.366)\n",
            "BERTScore: -0.108 (P=-0.239, R=0.031, model=roberta-large, baseline=True)\n",
            "Count:     500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== GPT-2 Quantitative Metrics ===\n",
            "BLEU-4:    9.33  (BP=1.00, P1=35.69, P2=12.85, P3=5.71, P4=2.89)\n",
            "ROUGE-1/2/L-F1: 0.398 / 0.120 / 0.211\n",
            "METEOR:    0.229 (used 500)\n",
            "Token-F1:  0.321 (P=0.304, R=0.340)\n",
            "BERTScore: 0.060 (P=0.009, R=0.112, model=roberta-large, baseline=True)\n",
            "Count:     500\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'BLEU': {'BLEU-4': 9.328819865294001,\n",
              "  'P1': 35.688016345385094,\n",
              "  'P2': 12.847327563130186,\n",
              "  'P3': 5.706731200143408,\n",
              "  'P4': 2.8945709912667685,\n",
              "  'BP': 1.0,\n",
              "  'sys_len': 112570,\n",
              "  'ref_len': 92124},\n",
              " 'ROUGE': {'ROUGE-1-F1': 0.39797663532878325,\n",
              "  'ROUGE-2-F1': 0.12017853719603012,\n",
              "  'ROUGE-L-F1': 0.2113959410263563},\n",
              " 'METEOR': {'METEOR': 0.2291513181671363, 'used': 500},\n",
              " 'TokenF1': {'precision': 0.30446001609772266,\n",
              "  'recall': 0.3401885968972769,\n",
              "  'F1': 0.3213342078141104},\n",
              " 'BERTScore': {'P': 0.009487519040703773,\n",
              "  'R': 0.1116442158818245,\n",
              "  'F1': 0.05963706597685814,\n",
              "  'model_type': 'roberta-large',\n",
              "  'baseline_rescale': True},\n",
              " 'count': 500}"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate_model_outputs(\"Regex\", regex_val_responses)\n",
        "evaluate_model_outputs(\"GPT-2\", gpt2_val_responses)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45c9f306",
      "metadata": {},
      "source": [
        "### Step 2: LLM-based evaluation (Qualitatively):\n",
        "- Using an LLM model (**`Phi-3-mini-4k-instruct`**) as a judge to grade responses from 0–100."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "0fc7dbb2",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]\n",
            "Device set to use cuda:0\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from typing import List, Optional\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "JUDGE_MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "\n",
        "# ---- Load judge (simple device/dtype selection) ----\n",
        "torch_dtype = (torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
        "               else (torch.float16 if torch.cuda.is_available() else None))\n",
        "tokenizer_judge = AutoTokenizer.from_pretrained(JUDGE_MODEL_NAME)\n",
        "model_judge = AutoModelForCausalLM.from_pretrained(JUDGE_MODEL_NAME, torch_dtype=torch_dtype, device_map=\"auto\")\n",
        "\n",
        "judge_pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_judge,\n",
        "    tokenizer=tokenizer_judge,\n",
        "    batch_size=4,\n",
        "    do_sample=False,\n",
        "    temperature=1e-5,\n",
        "    max_new_tokens=64,\n",
        "    pad_token_id=tokenizer_judge.eos_token_id,\n",
        ")\n",
        "\n",
        "# ---- Prompt + score parsing ----\n",
        "SYSTEM_RUBRIC = (\n",
        "    \"You are an expert grader. You will rate a candidate answer against a reference.\\n\"\n",
        "    \"Scoring: 0 to 100 (100 = perfect). Consider correctness, faithfulness to the reference, \"\n",
        "    \"coverage/completeness, and clarity. IMPORTANT: Respond with ONLY the final integer score, \"\n",
        "    \"no words, no punctuation, no explanations.\"\n",
        ")\n",
        "\n",
        "def make_prompt(instruction: str, inp: Optional[str], reference: str, hypothesis: str) -> str:\n",
        "    return (\n",
        "        f\"{SYSTEM_RUBRIC}\\n\\n\"\n",
        "        f\"Instruction:\\n{(instruction or '').strip()}\\n\\n\"\n",
        "        f\"Input:\\n{(inp or '').strip()}\\n\\n\"\n",
        "        f\"Reference answer:\\n{(reference or '').strip()}\\n\\n\"\n",
        "        f\"Model's answer:\\n{(hypothesis or '').strip()}\\n\\n\"\n",
        "        \"Final score (0-100):\"\n",
        "    )\n",
        "\n",
        "_num_pat = re.compile(r\"\\b(\\d{1,3})\\b\")\n",
        "def parse_score(text: str) -> Optional[int]:\n",
        "    m = _num_pat.findall(text or \"\")\n",
        "    if not m: return None\n",
        "    try:\n",
        "        return max(0, min(100, int(m[-1])))\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# ---- Batch query ----\n",
        "def query_judge_batch(prompts: List[str]) -> List[Optional[int]]:\n",
        "    outs = judge_pipe(prompts)\n",
        "    scores = []\n",
        "    for o in outs:\n",
        "        if isinstance(o, list): o = o[0]\n",
        "        scores.append(parse_score(o.get(\"generated_text\", \"\")))\n",
        "    return scores\n",
        "\n",
        "# ---- Evaluate one model ----\n",
        "def evaluate_with_judge(model_name: str, dataset: List[dict], *, batch_size: int = 4, limit: Optional[int] = None, seed: int = 1234) -> dict:\n",
        "    random.seed(seed)\n",
        "    n = len(dataset) if limit is None else min(len(dataset), int(limit))\n",
        "    prompts = [\n",
        "        make_prompt(ex.get(\"Instruction\",\"\"), ex.get(\"Input\",\"\"), ex.get(\"Response\",\"\"), ex.get(\"model_response\",\"\"))\n",
        "        for ex in dataset[:n]\n",
        "    ]\n",
        "\n",
        "    scores: List[int] = []\n",
        "    for i in tqdm(range(0, len(prompts), batch_size), desc=f\"Judge Scoring ({model_name})\"):\n",
        "        batch_scores = query_judge_batch(prompts[i:i+batch_size])\n",
        "        scores.extend(int(s) for s in batch_scores if isinstance(s, int))\n",
        "\n",
        "    if scores:\n",
        "        avg, mn, mx = sum(scores)/len(scores), min(scores), max(scores)\n",
        "        print(f\"\\n[Judge] {model_name}: avg={avg:.1f}% | min={mn} | max={mx} | n={len(scores)}\\n\")\n",
        "        return {\"model\": model_name, \"avg\": avg, \"min\": mn, \"max\": mx, \"n\": len(scores), \"scores\": scores}\n",
        "    else:\n",
        "        print(f\"\\n[Judge] {model_name}: no valid scores parsed.\\n\")\n",
        "        return {\"model\": model_name, \"avg\": None, \"min\": None, \"max\": None, \"n\": 0, \"scores\": []}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "a5a1a114",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Judge Scoring (Regex):   8%|▊         | 10/125 [00:26<04:53,  2.55s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "Judge Scoring (Regex): 100%|██████████| 125/125 [05:13<00:00,  2.51s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Judge] Regex: avg=58.5% | min=0 | max=100 | n=500\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Judge Scoring (GPT-2): 100%|██████████| 125/125 [05:08<00:00,  2.47s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Judge] GPT-2: avg=62.0% | min=0 | max=100 | n=500\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'model': 'GPT-2',\n",
              " 'avg': 62.026,\n",
              " 'min': 0,\n",
              " 'max': 100,\n",
              " 'n': 500,\n",
              " 'scores': [100,\n",
              "  1,\n",
              "  85,\n",
              "  85,\n",
              "  19,\n",
              "  100,\n",
              "  85,\n",
              "  85,\n",
              "  95,\n",
              "  85,\n",
              "  4,\n",
              "  85,\n",
              "  92,\n",
              "  80,\n",
              "  85,\n",
              "  95,\n",
              "  85,\n",
              "  1,\n",
              "  85,\n",
              "  85,\n",
              "  2,\n",
              "  2,\n",
              "  37,\n",
              "  1,\n",
              "  75,\n",
              "  1,\n",
              "  85,\n",
              "  100,\n",
              "  75,\n",
              "  90,\n",
              "  95,\n",
              "  85,\n",
              "  20,\n",
              "  85,\n",
              "  12,\n",
              "  85,\n",
              "  19,\n",
              "  100,\n",
              "  85,\n",
              "  10,\n",
              "  3,\n",
              "  5,\n",
              "  85,\n",
              "  95,\n",
              "  4,\n",
              "  30,\n",
              "  85,\n",
              "  95,\n",
              "  100,\n",
              "  95,\n",
              "  95,\n",
              "  95,\n",
              "  37,\n",
              "  95,\n",
              "  85,\n",
              "  1,\n",
              "  85,\n",
              "  85,\n",
              "  85,\n",
              "  85,\n",
              "  100,\n",
              "  85,\n",
              "  95,\n",
              "  95,\n",
              "  0,\n",
              "  75,\n",
              "  85,\n",
              "  85,\n",
              "  10,\n",
              "  85,\n",
              "  7,\n",
              "  92,\n",
              "  85,\n",
              "  2,\n",
              "  85,\n",
              "  10,\n",
              "  95,\n",
              "  85,\n",
              "  85,\n",
              "  85,\n",
              "  100,\n",
              "  85,\n",
              "  95,\n",
              "  75,\n",
              "  95,\n",
              "  1,\n",
              "  2,\n",
              "  100,\n",
              "  1,\n",
              "  85,\n",
              "  95,\n",
              "  85,\n",
              "  5,\n",
              "  0,\n",
              "  4,\n",
              "  95,\n",
              "  100,\n",
              "  100,\n",
              "  85,\n",
              "  75,\n",
              "  85,\n",
              "  2,\n",
              "  85,\n",
              "  75,\n",
              "  1,\n",
              "  100,\n",
              "  95,\n",
              "  2,\n",
              "  85,\n",
              "  95,\n",
              "  1,\n",
              "  95,\n",
              "  100,\n",
              "  90,\n",
              "  0,\n",
              "  10,\n",
              "  100,\n",
              "  7,\n",
              "  92,\n",
              "  92,\n",
              "  95,\n",
              "  95,\n",
              "  15,\n",
              "  85,\n",
              "  5,\n",
              "  2,\n",
              "  85,\n",
              "  2,\n",
              "  5,\n",
              "  2,\n",
              "  92,\n",
              "  85,\n",
              "  95,\n",
              "  19,\n",
              "  75,\n",
              "  85,\n",
              "  95,\n",
              "  2,\n",
              "  75,\n",
              "  10,\n",
              "  1,\n",
              "  65,\n",
              "  85,\n",
              "  85,\n",
              "  20,\n",
              "  40,\n",
              "  100,\n",
              "  2,\n",
              "  85,\n",
              "  19,\n",
              "  5,\n",
              "  2,\n",
              "  85,\n",
              "  11,\n",
              "  90,\n",
              "  85,\n",
              "  11,\n",
              "  85,\n",
              "  2,\n",
              "  1,\n",
              "  85,\n",
              "  7,\n",
              "  35,\n",
              "  1,\n",
              "  100,\n",
              "  2,\n",
              "  100,\n",
              "  92,\n",
              "  2,\n",
              "  90,\n",
              "  75,\n",
              "  68,\n",
              "  3,\n",
              "  5,\n",
              "  95,\n",
              "  95,\n",
              "  100,\n",
              "  75,\n",
              "  85,\n",
              "  2,\n",
              "  95,\n",
              "  95,\n",
              "  85,\n",
              "  5,\n",
              "  100,\n",
              "  95,\n",
              "  85,\n",
              "  85,\n",
              "  2,\n",
              "  95,\n",
              "  85,\n",
              "  95,\n",
              "  85,\n",
              "  10,\n",
              "  85,\n",
              "  92,\n",
              "  95,\n",
              "  92,\n",
              "  0,\n",
              "  92,\n",
              "  90,\n",
              "  1,\n",
              "  80,\n",
              "  85,\n",
              "  2,\n",
              "  0,\n",
              "  8,\n",
              "  85,\n",
              "  85,\n",
              "  100,\n",
              "  2,\n",
              "  85,\n",
              "  85,\n",
              "  100,\n",
              "  75,\n",
              "  100,\n",
              "  85,\n",
              "  15,\n",
              "  3,\n",
              "  95,\n",
              "  20,\n",
              "  5,\n",
              "  85,\n",
              "  90,\n",
              "  85,\n",
              "  85,\n",
              "  92,\n",
              "  85,\n",
              "  70,\n",
              "  85,\n",
              "  85,\n",
              "  85,\n",
              "  100,\n",
              "  85,\n",
              "  2,\n",
              "  100,\n",
              "  95,\n",
              "  5,\n",
              "  10,\n",
              "  1,\n",
              "  85,\n",
              "  85,\n",
              "  5,\n",
              "  10,\n",
              "  95,\n",
              "  85,\n",
              "  95,\n",
              "  95,\n",
              "  2,\n",
              "  1,\n",
              "  1,\n",
              "  2,\n",
              "  75,\n",
              "  42,\n",
              "  3,\n",
              "  95,\n",
              "  85,\n",
              "  100,\n",
              "  95,\n",
              "  0,\n",
              "  100,\n",
              "  85,\n",
              "  85,\n",
              "  100,\n",
              "  100,\n",
              "  85,\n",
              "  55,\n",
              "  90,\n",
              "  100,\n",
              "  75,\n",
              "  95,\n",
              "  85,\n",
              "  75,\n",
              "  85,\n",
              "  85,\n",
              "  5,\n",
              "  10,\n",
              "  87,\n",
              "  85,\n",
              "  85,\n",
              "  85,\n",
              "  85,\n",
              "  78,\n",
              "  100,\n",
              "  10,\n",
              "  1,\n",
              "  2,\n",
              "  85,\n",
              "  92,\n",
              "  100,\n",
              "  1,\n",
              "  95,\n",
              "  90,\n",
              "  85,\n",
              "  2,\n",
              "  100,\n",
              "  85,\n",
              "  1,\n",
              "  2,\n",
              "  1,\n",
              "  95,\n",
              "  75,\n",
              "  100,\n",
              "  85,\n",
              "  5,\n",
              "  0,\n",
              "  6,\n",
              "  2,\n",
              "  13,\n",
              "  95,\n",
              "  5,\n",
              "  3,\n",
              "  92,\n",
              "  5,\n",
              "  85,\n",
              "  95,\n",
              "  3,\n",
              "  90,\n",
              "  1,\n",
              "  85,\n",
              "  92,\n",
              "  92,\n",
              "  85,\n",
              "  85,\n",
              "  20,\n",
              "  85,\n",
              "  2,\n",
              "  50,\n",
              "  95,\n",
              "  100,\n",
              "  92,\n",
              "  75,\n",
              "  92,\n",
              "  4,\n",
              "  92,\n",
              "  100,\n",
              "  85,\n",
              "  95,\n",
              "  100,\n",
              "  85,\n",
              "  85,\n",
              "  85,\n",
              "  5,\n",
              "  85,\n",
              "  85,\n",
              "  95,\n",
              "  95,\n",
              "  20,\n",
              "  100,\n",
              "  2,\n",
              "  75,\n",
              "  90,\n",
              "  5,\n",
              "  85,\n",
              "  95,\n",
              "  75,\n",
              "  85,\n",
              "  1,\n",
              "  92,\n",
              "  3,\n",
              "  100,\n",
              "  85,\n",
              "  85,\n",
              "  85,\n",
              "  95,\n",
              "  100,\n",
              "  2,\n",
              "  85,\n",
              "  34,\n",
              "  92,\n",
              "  4,\n",
              "  82,\n",
              "  85,\n",
              "  7,\n",
              "  5,\n",
              "  85,\n",
              "  5,\n",
              "  75,\n",
              "  3,\n",
              "  85,\n",
              "  50,\n",
              "  75,\n",
              "  95,\n",
              "  75,\n",
              "  1,\n",
              "  100,\n",
              "  10,\n",
              "  95,\n",
              "  20,\n",
              "  85,\n",
              "  95,\n",
              "  10,\n",
              "  75,\n",
              "  5,\n",
              "  2,\n",
              "  1,\n",
              "  85,\n",
              "  92,\n",
              "  20,\n",
              "  85,\n",
              "  3,\n",
              "  10,\n",
              "  95,\n",
              "  85,\n",
              "  95,\n",
              "  85,\n",
              "  2,\n",
              "  13,\n",
              "  85,\n",
              "  85,\n",
              "  95,\n",
              "  85,\n",
              "  92,\n",
              "  85,\n",
              "  100,\n",
              "  85,\n",
              "  85,\n",
              "  75,\n",
              "  75,\n",
              "  85,\n",
              "  85,\n",
              "  95,\n",
              "  85,\n",
              "  10,\n",
              "  85,\n",
              "  1,\n",
              "  75,\n",
              "  2,\n",
              "  0,\n",
              "  100,\n",
              "  5,\n",
              "  3,\n",
              "  90,\n",
              "  3,\n",
              "  0,\n",
              "  100,\n",
              "  95,\n",
              "  0,\n",
              "  1,\n",
              "  95,\n",
              "  92,\n",
              "  85,\n",
              "  75,\n",
              "  92,\n",
              "  100,\n",
              "  2,\n",
              "  85,\n",
              "  100,\n",
              "  85,\n",
              "  95,\n",
              "  95,\n",
              "  100,\n",
              "  100,\n",
              "  85,\n",
              "  30,\n",
              "  85,\n",
              "  92,\n",
              "  18,\n",
              "  100,\n",
              "  5,\n",
              "  92,\n",
              "  85,\n",
              "  20,\n",
              "  90,\n",
              "  88,\n",
              "  95,\n",
              "  95,\n",
              "  100,\n",
              "  92,\n",
              "  85,\n",
              "  3,\n",
              "  10,\n",
              "  100,\n",
              "  10,\n",
              "  6,\n",
              "  100,\n",
              "  5,\n",
              "  85,\n",
              "  95,\n",
              "  85,\n",
              "  95,\n",
              "  95,\n",
              "  100,\n",
              "  90,\n",
              "  85,\n",
              "  5,\n",
              "  95,\n",
              "  2,\n",
              "  85,\n",
              "  85,\n",
              "  85,\n",
              "  2,\n",
              "  95,\n",
              "  85,\n",
              "  0,\n",
              "  75,\n",
              "  87,\n",
              "  95,\n",
              "  85,\n",
              "  92]}"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate_with_judge(\"Regex\", regex_val_responses, batch_size=4)\n",
        "evaluate_with_judge(\"GPT-2\", gpt2_val_responses,  batch_size=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc978a66",
      "metadata": {},
      "source": [
        "## Part 5 - Using GPT-2 Pretrained Weights to Compare with Our Regex and GPT-2 Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d420018",
      "metadata": {},
      "source": [
        "\n",
        "In this part, we use the **GPT-2 (small, 124M parameters)** pretrained weights to establish a stronger baseline and compare against our Regex and GPT-2 models.\n",
        "\n",
        "**Steps:**\n",
        "1. **Download** the pretrained GPT-2o model weights (`gpt2-small (124M)`) and load it.\n",
        "2. **Fine-tune** the model on our custom **instruction dataset**.\n",
        "3. **Generate** validation responses using the fine-tuned GPT-2o model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "9f8b45c0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
            "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
            "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
            "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
            "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
            "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
            "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "# import tensorflow\n",
        "url = (\"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/01_main-chapter-code/gpt_download.py\") \n",
        "filename = url.split('/')[-1]\n",
        "urllib.request.urlretrieve(url, filename)\n",
        "\n",
        "from gpt_download import download_and_load_gpt2\n",
        "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b868f2f",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_configs = {\n",
        "\"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "\"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "\"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "\"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "\n",
        "model_name = \"gpt2-small (124M)\"\n",
        "gpt2 = True\n",
        "NEW_CONFIG = GPT_CONFIG.copy()\n",
        "NEW_CONFIG.update(model_configs[model_name])\n",
        "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
        "gpt2o = GPTModel(NEW_CONFIG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54cdbf26",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 768)\n",
              "  (pos_emb): Embedding(1024, 768)\n",
              "  (drop_emb): Dropout(p=0.3, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (drop_shortcut): Dropout(p=0.3, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (drop_shortcut): Dropout(p=0.3, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (drop_shortcut): Dropout(p=0.3, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (drop_shortcut): Dropout(p=0.3, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (drop_shortcut): Dropout(p=0.3, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (drop_shortcut): Dropout(p=0.3, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (drop_shortcut): Dropout(p=0.3, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (drop_shortcut): Dropout(p=0.3, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (drop_shortcut): Dropout(p=0.3, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (drop_shortcut): Dropout(p=0.3, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (drop_shortcut): Dropout(p=0.3, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (drop_shortcut): Dropout(p=0.3, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def assign(left, right):\n",
        "    if left.shape != right.shape:\n",
        "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
        "    return torch.nn.Parameter(torch.tensor(right))\n",
        "\n",
        "def load_weights_into_gpt(gpt2o, params):\n",
        "    \"\"\"Load GPT-2o weights from a params dict into a PyTorch GPT model.\"\"\"\n",
        "\n",
        "    # --- Embeddings ---\n",
        "    gpt2o.pos_emb.weight = assign(gpt2o.pos_emb.weight, params[\"wpe\"])\n",
        "    gpt2o.tok_emb.weight = assign(gpt2o.tok_emb.weight, params[\"wte\"])\n",
        "\n",
        "    # --- Transformer Blocks ---\n",
        "    num_blocks = len(params[\"blocks\"])\n",
        "    for b in range(num_blocks):\n",
        "        block = params[\"blocks\"][b]\n",
        "\n",
        "        # === Attention Projections ===\n",
        "        # Split qkv weights\n",
        "        q_w, k_w, v_w = np.split(block[\"attn\"][\"c_attn\"][\"w\"], 3, axis=-1)\n",
        "        q_b, k_b, v_b = np.split(block[\"attn\"][\"c_attn\"][\"b\"], 3, axis=-1)\n",
        "\n",
        "        gpt2o.trf_blocks[b].att.W_query.weight = assign(\n",
        "            gpt2o.trf_blocks[b].att.W_query.weight, q_w.T)\n",
        "        gpt2o.trf_blocks[b].att.W_key.weight = assign(\n",
        "            gpt2o.trf_blocks[b].att.W_key.weight, k_w.T)\n",
        "        gpt2o.trf_blocks[b].att.W_value.weight = assign(\n",
        "            gpt2o.trf_blocks[b].att.W_value.weight, v_w.T)\n",
        "\n",
        "        gpt2o.trf_blocks[b].att.W_query.bias = assign(\n",
        "            gpt2o.trf_blocks[b].att.W_query.bias, q_b)\n",
        "        gpt2o.trf_blocks[b].att.W_key.bias = assign(\n",
        "            gpt2o.trf_blocks[b].att.W_key.bias, k_b)\n",
        "        gpt2o.trf_blocks[b].att.W_value.bias = assign(\n",
        "            gpt2o.trf_blocks[b].att.W_value.bias, v_b)\n",
        "\n",
        "        # Output projection\n",
        "        gpt2o.trf_blocks[b].att.out_proj.weight = assign(\n",
        "            gpt2o.trf_blocks[b].att.out_proj.weight, block[\"attn\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt2o.trf_blocks[b].att.out_proj.bias = assign(\n",
        "            gpt2o.trf_blocks[b].att.out_proj.bias, block[\"attn\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        # === MLP / Feed Forward ===\n",
        "        gpt2o.trf_blocks[b].ff.layers[0].weight = assign(\n",
        "            gpt2o.trf_blocks[b].ff.layers[0].weight, block[\"mlp\"][\"c_fc\"][\"w\"].T)\n",
        "        gpt2o.trf_blocks[b].ff.layers[0].bias = assign(\n",
        "            gpt2o.trf_blocks[b].ff.layers[0].bias, block[\"mlp\"][\"c_fc\"][\"b\"])\n",
        "\n",
        "        gpt2o.trf_blocks[b].ff.layers[2].weight = assign(\n",
        "            gpt2o.trf_blocks[b].ff.layers[2].weight, block[\"mlp\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt2o.trf_blocks[b].ff.layers[2].bias = assign(\n",
        "            gpt2o.trf_blocks[b].ff.layers[2].bias, block[\"mlp\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        # === LayerNorm Fix (PyTorch uses weight/bias, not scale/shift) ===\n",
        "        gpt2o.trf_blocks[b].norm1.weight = assign(\n",
        "            gpt2o.trf_blocks[b].norm1.weight, block[\"ln_1\"][\"g\"])\n",
        "        gpt2o.trf_blocks[b].norm1.bias = assign(\n",
        "            gpt2o.trf_blocks[b].norm1.bias, block[\"ln_1\"][\"b\"])\n",
        "\n",
        "        gpt2o.trf_blocks[b].norm2.weight = assign(\n",
        "            gpt2o.trf_blocks[b].norm2.weight, block[\"ln_2\"][\"g\"])\n",
        "        gpt2o.trf_blocks[b].norm2.bias = assign(\n",
        "            gpt2o.trf_blocks[b].norm2.bias, block[\"ln_2\"][\"b\"])\n",
        "\n",
        "    # --- Final LayerNorm ---\n",
        "    gpt2o.final_norm.weight = assign(gpt2o.final_norm.weight, params[\"g\"])\n",
        "    gpt2o.final_norm.bias = assign(gpt2o.final_norm.bias, params[\"b\"])\n",
        "\n",
        "    # --- Output head tied to embedding ---\n",
        "    gpt2o.out_head.weight = assign(gpt2o.out_head.weight, params[\"wte\"])\n",
        "    \n",
        "load_weights_into_gpt(gpt2o, params)\n",
        "gpt2o.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03b2fc7f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🟨 GPT-2 (Pretrained Weights) tokenizer model output:\n",
            " large language models are not very. of is more like a real; if a- what's the size to the different- is, as are are a really people on being around the people who are. being are the people. That (and are) a) the\n",
            "\n",
            "🟨 GPT-2 loaders already ready.\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "token_ids = generate(\n",
        "    model=gpt2o,\n",
        "    idx=text_to_token_ids(\"large language models are\", tokenizer_gpt2).to(device),\n",
        "    max_new_tokens=50,\n",
        "    context_size=NEW_CONFIG[\"context_length\"],\n",
        "    top_k=25,\n",
        "    temperature=1.4\n",
        ")\n",
        "print(\"\\n🟨 GPT-2o (Pretrained Weights) tokenizer model output:\\n\", token_ids_to_text(token_ids, tokenizer_gpt2))\n",
        "print(\"\\n🟨 GPT-2o loaders already ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57b70d15",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep 1 (Step 000000): Train loss 3.230, Val loss 3.251\n",
            "Ep 1 (Step 000050): Train loss 2.621, Val loss 2.672\n",
            "Ep 1 (Step 000100): Train loss 2.470, Val loss 2.527\n",
            "Ep 1 (Step 000150): Train loss 2.461, Val loss 2.461\n",
            "Ep 1 (Step 000200): Train loss 2.417, Val loss 2.412\n",
            "Ep 1 (Step 000250): Train loss 2.347, Val loss 2.379\n",
            "Ep 1 (Step 000300): Train loss 2.314, Val loss 2.348\n",
            "Ep 1 (Step 000350): Train loss 2.283, Val loss 2.328\n",
            "Ep 1 (Step 000400): Train loss 2.291, Val loss 2.301\n",
            "Ep 1 (Step 000450): Train loss 2.280, Val loss 2.279\n",
            "Ep 1 (Step 000500): Train loss 2.195, Val loss 2.263\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: excerpt describes a method for generating safety preprompts using adversarial prompts to generate safer responses. Specifically, the method involves prefixing a safety preprompt to adversarial prompts to generate safer responses. Specifically, the method\n",
            "Ep 2 (Step 000550): Train loss 2.199, Val loss 2.247\n",
            "Ep 2 (Step 000600): Train loss 2.194, Val loss 2.233\n",
            "Ep 2 (Step 000650): Train loss 2.168, Val loss 2.218\n",
            "Ep 2 (Step 000700): Train loss 2.172, Val loss 2.204\n",
            "Ep 2 (Step 000750): Train loss 2.172, Val loss 2.190\n",
            "Ep 2 (Step 000800): Train loss 2.108, Val loss 2.174\n",
            "Ep 2 (Step 000850): Train loss 2.106, Val loss 2.168\n",
            "Ep 2 (Step 000900): Train loss 2.065, Val loss 2.156\n",
            "Ep 2 (Step 000950): Train loss 2.109, Val loss 2.147\n",
            "Ep 2 (Step 001000): Train loss 2.084, Val loss 2.137\n",
            "Ep 2 (Step 001050): Train loss 2.068, Val loss 2.133\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: context distillation is a technique used to fine-tune the model on adversarial prompts to generate safer responses, particularly when dealing with adversarial prompts that are not explicitly labeled as unsafe. Specifically, context distillation involves\n",
            "Ep 3 (Step 001100): Train loss 2.036, Val loss 2.119\n",
            "Ep 3 (Step 001150): Train loss 2.031, Val loss 2.111\n",
            "Ep 3 (Step 001200): Train loss 2.018, Val loss 2.100\n",
            "Ep 3 (Step 001250): Train loss 1.967, Val loss 2.094\n",
            "Ep 3 (Step 001300): Train loss 2.048, Val loss 2.084\n",
            "Ep 3 (Step 001350): Train loss 1.997, Val loss 2.080\n",
            "Ep 3 (Step 001400): Train loss 1.973, Val loss 2.066\n",
            "Ep 3 (Step 001450): Train loss 2.002, Val loss 2.062\n",
            "Ep 3 (Step 001500): Train loss 1.975, Val loss 2.049\n",
            "Ep 3 (Step 001550): Train loss 1.958, Val loss 2.046\n",
            "Ep 3 (Step 001600): Train loss 1.956, Val loss 2.040\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: context distillation is applied to adversarial prompts generated by prefixing safety preprompts to adversarial prompts, and fine-tune the model on its own safe output given the adversarial prompt without the preprompt.\n",
            "Ep 4 (Step 001650): Train loss 1.924, Val loss 2.035\n",
            "Ep 4 (Step 001700): Train loss 1.920, Val loss 2.029\n",
            "Ep 4 (Step 001750): Train loss 1.938, Val loss 2.024\n",
            "Ep 4 (Step 001800): Train loss 1.914, Val loss 2.011\n",
            "Ep 4 (Step 001850): Train loss 1.916, Val loss 2.008\n",
            "Ep 4 (Step 001900): Train loss 1.907, Val loss 2.006\n",
            "Ep 4 (Step 001950): Train loss 1.906, Val loss 2.002\n",
            "Ep 4 (Step 002000): Train loss 1.895, Val loss 1.990\n",
            "Ep 4 (Step 002050): Train loss 1.895, Val loss 1.984\n",
            "Ep 4 (Step 002100): Train loss 1.812, Val loss 1.974\n",
            "Ep 4 (Step 002150): Train loss 1.851, Val loss 1.970\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: Context distillation is applied to adversarial prompts generated by prefixing safety preprompts to adversarial prompts, and fine-tuning the model on its own safe output given the adversarial prompt without the preprompt.\n",
            "Ep 5 (Step 002200): Train loss 1.854, Val loss 1.969\n",
            "Ep 5 (Step 002250): Train loss 1.830, Val loss 1.964\n",
            "Ep 5 (Step 002300): Train loss 1.857, Val loss 1.962\n",
            "Ep 5 (Step 002350): Train loss 1.802, Val loss 1.955\n",
            "Ep 5 (Step 002400): Train loss 1.845, Val loss 1.952\n",
            "Ep 5 (Step 002450): Train loss 1.823, Val loss 1.948\n",
            "Ep 5 (Step 002500): Train loss 1.763, Val loss 1.943\n",
            "Ep 5 (Step 002550): Train loss 1.863, Val loss 1.932\n",
            "Ep 5 (Step 002600): Train loss 1.808, Val loss 1.923\n",
            "Ep 5 (Step 002650): Train loss 1.796, Val loss 1.920\n",
            "Ep 5 (Step 002700): Train loss 1.762, Val loss 1.916\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: Context distillation involves prefixing safety preprompts to adversarial prompts to generate safer responses, and fine-tuning the model on its own safe output given the adversarial prompt without the preprompt. This approach ensures\n",
            "Ep 6 (Step 002750): Train loss 1.759, Val loss 1.912\n",
            "Ep 6 (Step 002800): Train loss 1.757, Val loss 1.910\n",
            "Ep 6 (Step 002850): Train loss 1.755, Val loss 1.900\n",
            "Ep 6 (Step 002900): Train loss 1.732, Val loss 1.897\n",
            "Ep 6 (Step 002950): Train loss 1.748, Val loss 1.892\n",
            "Ep 6 (Step 003000): Train loss 1.739, Val loss 1.887\n",
            "Ep 6 (Step 003050): Train loss 1.722, Val loss 1.882\n",
            "Ep 6 (Step 003100): Train loss 1.742, Val loss 1.881\n",
            "Ep 6 (Step 003150): Train loss 1.746, Val loss 1.872\n",
            "Ep 6 (Step 003200): Train loss 1.742, Val loss 1.872\n",
            "Ep 6 (Step 003250): Train loss 1.728, Val loss 1.865\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: Context distillation involves prefixing safety preprompts to adversarial prompts to generate safer responses, and fine-tune the model on its own safe output given the adversarial prompt without the preprompt. This approach ensures\n",
            "Ep 7 (Step 003300): Train loss 1.678, Val loss 1.867\n",
            "Ep 7 (Step 003350): Train loss 1.709, Val loss 1.861\n",
            "Ep 7 (Step 003400): Train loss 1.690, Val loss 1.852\n",
            "Ep 7 (Step 003450): Train loss 1.698, Val loss 1.851\n",
            "Ep 7 (Step 003500): Train loss 1.661, Val loss 1.845\n",
            "Ep 7 (Step 003550): Train loss 1.655, Val loss 1.843\n",
            "Ep 7 (Step 003600): Train loss 1.672, Val loss 1.844\n",
            "Ep 7 (Step 003650): Train loss 1.642, Val loss 1.836\n",
            "Ep 7 (Step 003700): Train loss 1.660, Val loss 1.834\n",
            "Ep 7 (Step 003750): Train loss 1.655, Val loss 1.826\n",
            "Ep 7 (Step 003800): Train loss 1.674, Val loss 1.823\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: Context distillation involves prefixing a safety preprompt to adversarial prompts to generate safer responses, and fine-tuning the model on its own safe output given the adversarial prompt without the preprompt. This process ensures\n",
            "Ep 8 (Step 003850): Train loss 1.650, Val loss 1.818\n",
            "Ep 8 (Step 003900): Train loss 1.626, Val loss 1.813\n",
            "Ep 8 (Step 003950): Train loss 1.630, Val loss 1.814\n",
            "Ep 8 (Step 004000): Train loss 1.621, Val loss 1.808\n",
            "Ep 8 (Step 004050): Train loss 1.639, Val loss 1.807\n",
            "Ep 8 (Step 004100): Train loss 1.579, Val loss 1.799\n",
            "Ep 8 (Step 004150): Train loss 1.610, Val loss 1.796\n",
            "Ep 8 (Step 004200): Train loss 1.586, Val loss 1.789\n",
            "Ep 8 (Step 004250): Train loss 1.590, Val loss 1.785\n",
            "Ep 8 (Step 004300): Train loss 1.590, Val loss 1.786\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: Context distillation is applied to adversarial prompts to generate safer responses, and fine-tunes the model on its own safe output given the adversarial prompt without preprompting. This approach ensures that the model adheres\n",
            "Ep 9 (Step 004350): Train loss 1.560, Val loss 1.782\n",
            "Ep 9 (Step 004400): Train loss 1.589, Val loss 1.778\n",
            "Ep 9 (Step 004450): Train loss 1.617, Val loss 1.777\n",
            "Ep 9 (Step 004500): Train loss 1.573, Val loss 1.773\n",
            "Ep 9 (Step 004550): Train loss 1.561, Val loss 1.770\n",
            "Ep 9 (Step 004600): Train loss 1.558, Val loss 1.763\n",
            "Ep 9 (Step 004650): Train loss 1.544, Val loss 1.757\n",
            "Ep 9 (Step 004700): Train loss 1.567, Val loss 1.754\n",
            "Ep 9 (Step 004750): Train loss 1.522, Val loss 1.751\n",
            "Ep 9 (Step 004800): Train loss 1.518, Val loss 1.753\n",
            "Ep 9 (Step 004850): Train loss 1.510, Val loss 1.746\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: Context distillation involves prefixing safety preprompts to adversarial prompts to generate safer responses, and fine-tune the model on its own safe output given the adversarial prompt without the preprompt. This process ensures\n",
            "Ep 10 (Step 004900): Train loss 1.477, Val loss 1.750\n",
            "Ep 10 (Step 004950): Train loss 1.562, Val loss 1.740\n",
            "Ep 10 (Step 005000): Train loss 1.510, Val loss 1.733\n",
            "Ep 10 (Step 005050): Train loss 1.492, Val loss 1.734\n",
            "Ep 10 (Step 005100): Train loss 1.504, Val loss 1.730\n",
            "Ep 10 (Step 005150): Train loss 1.478, Val loss 1.727\n",
            "Ep 10 (Step 005200): Train loss 1.495, Val loss 1.721\n",
            "Ep 10 (Step 005250): Train loss 1.476, Val loss 1.720\n",
            "Ep 10 (Step 005300): Train loss 1.505, Val loss 1.715\n",
            "Ep 10 (Step 005350): Train loss 1.510, Val loss 1.716\n",
            "Ep 10 (Step 005400): Train loss 1.459, Val loss 1.708\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: Context distillation involves prefixing safety preprompts to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. This process\n",
            "Ep 11 (Step 005450): Train loss 1.473, Val loss 1.710\n",
            "Ep 11 (Step 005500): Train loss 1.468, Val loss 1.706\n",
            "Ep 11 (Step 005550): Train loss 1.424, Val loss 1.703\n",
            "Ep 11 (Step 005600): Train loss 1.454, Val loss 1.699\n",
            "Ep 11 (Step 005650): Train loss 1.447, Val loss 1.694\n",
            "Ep 11 (Step 005700): Train loss 1.448, Val loss 1.689\n",
            "Ep 11 (Step 005750): Train loss 1.385, Val loss 1.687\n",
            "Ep 11 (Step 005800): Train loss 1.434, Val loss 1.686\n",
            "Ep 11 (Step 005850): Train loss 1.431, Val loss 1.680\n",
            "Ep 11 (Step 005900): Train loss 1.429, Val loss 1.681\n",
            "Ep 11 (Step 005950): Train loss 1.431, Val loss 1.671\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: Context distillation involves prefixing safety preprompts to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without preprompting. This process\n",
            "Ep 12 (Step 006000): Train loss 1.410, Val loss 1.676\n",
            "Ep 12 (Step 006050): Train loss 1.417, Val loss 1.670\n",
            "Ep 12 (Step 006100): Train loss 1.396, Val loss 1.672\n",
            "Ep 12 (Step 006150): Train loss 1.369, Val loss 1.665\n",
            "Ep 12 (Step 006200): Train loss 1.396, Val loss 1.664\n",
            "Ep 12 (Step 006250): Train loss 1.405, Val loss 1.659\n",
            "Ep 12 (Step 006300): Train loss 1.414, Val loss 1.655\n",
            "Ep 12 (Step 006350): Train loss 1.414, Val loss 1.652\n",
            "Ep 12 (Step 006400): Train loss 1.379, Val loss 1.643\n",
            "Ep 12 (Step 006450): Train loss 1.368, Val loss 1.644\n",
            "Ep 12 (Step 006500): Train loss 1.384, Val loss 1.641\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39. We find that safety preprompts are more effective than supervised fine-tuning, and we find that safety preprompts are more effective than supervised fine-tuning.  ### Response: Context distillation is applied to adversarial prompts\n",
            "Ep 13 (Step 006550): Train loss 1.370, Val loss 1.638\n",
            "Ep 13 (Step 006600): Train loss 1.349, Val loss 1.638\n",
            "Ep 13 (Step 006650): Train loss 1.341, Val loss 1.632\n",
            "Ep 13 (Step 006700): Train loss 1.324, Val loss 1.631\n",
            "Ep 13 (Step 006750): Train loss 1.374, Val loss 1.628\n",
            "Ep 13 (Step 006800): Train loss 1.321, Val loss 1.623\n",
            "Ep 13 (Step 006850): Train loss 1.338, Val loss 1.624\n",
            "Ep 13 (Step 006900): Train loss 1.313, Val loss 1.617\n",
            "Ep 13 (Step 006950): Train loss 1.344, Val loss 1.613\n",
            "Ep 13 (Step 007000): Train loss 1.317, Val loss 1.612\n",
            "Ep 13 (Step 007050): Train loss 1.341, Val loss 1.609\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: Context distillation involves prefixing safety preprompts to adversarial prompts to generate safer responses, and then fine-tuning the model on its own safe output given the adversarial prompt without the preprompt. This process\n",
            "Ep 14 (Step 007100): Train loss 1.355, Val loss 1.609\n",
            "Ep 14 (Step 007150): Train loss 1.289, Val loss 1.607\n",
            "Ep 14 (Step 007200): Train loss 1.316, Val loss 1.599\n",
            "Ep 14 (Step 007250): Train loss 1.310, Val loss 1.599\n",
            "Ep 14 (Step 007300): Train loss 1.297, Val loss 1.599\n",
            "Ep 14 (Step 007350): Train loss 1.253, Val loss 1.595\n",
            "Ep 14 (Step 007400): Train loss 1.308, Val loss 1.593\n",
            "Ep 14 (Step 007450): Train loss 1.246, Val loss 1.590\n",
            "Ep 14 (Step 007500): Train loss 1.269, Val loss 1.586\n",
            "Ep 14 (Step 007550): Train loss 1.301, Val loss 1.583\n",
            "Ep 14 (Step 007600): Train loss 1.254, Val loss 1.577\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: Context distillation involves prefixing safety preprompts with adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. This process\n",
            "Ep 15 (Step 007650): Train loss 1.273, Val loss 1.575\n",
            "Ep 15 (Step 007700): Train loss 1.224, Val loss 1.572\n",
            "Ep 15 (Step 007750): Train loss 1.250, Val loss 1.567\n",
            "Ep 15 (Step 007800): Train loss 1.269, Val loss 1.569\n",
            "Ep 15 (Step 007850): Train loss 1.267, Val loss 1.570\n",
            "Ep 15 (Step 007900): Train loss 1.222, Val loss 1.563\n",
            "Ep 15 (Step 007950): Train loss 1.223, Val loss 1.561\n",
            "Ep 15 (Step 008000): Train loss 1.230, Val loss 1.558\n",
            "Ep 15 (Step 008050): Train loss 1.221, Val loss 1.554\n",
            "Ep 15 (Step 008100): Train loss 1.242, Val loss 1.552\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: Context distillation involves prefixing safety preprompts to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. This process\n",
            "Ep 16 (Step 008150): Train loss 1.236, Val loss 1.549\n",
            "Ep 16 (Step 008200): Train loss 1.242, Val loss 1.548\n",
            "Ep 16 (Step 008250): Train loss 1.183, Val loss 1.545\n",
            "Ep 16 (Step 008300): Train loss 1.222, Val loss 1.547\n",
            "Ep 16 (Step 008350): Train loss 1.169, Val loss 1.540\n",
            "Ep 16 (Step 008400): Train loss 1.232, Val loss 1.540\n",
            "Ep 16 (Step 008450): Train loss 1.186, Val loss 1.532\n",
            "Ep 16 (Step 008500): Train loss 1.196, Val loss 1.529\n",
            "Ep 16 (Step 008550): Train loss 1.204, Val loss 1.527\n",
            "Ep 16 (Step 008600): Train loss 1.183, Val loss 1.522\n",
            "Ep 16 (Step 008650): Train loss 1.180, Val loss 1.521\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: Context distillation involves applying context distillation to adversarial prompts to generate safer responses, using templates to guide the model's responses. These templates include adjectives typically associated with safe behavior such as \"responsible,\" \"respectful\n",
            "Ep 17 (Step 008700): Train loss 1.187, Val loss 1.524\n",
            "Ep 17 (Step 008750): Train loss 1.181, Val loss 1.520\n",
            "Ep 17 (Step 008800): Train loss 1.169, Val loss 1.513\n",
            "Ep 17 (Step 008850): Train loss 1.187, Val loss 1.519\n",
            "Ep 17 (Step 008900): Train loss 1.162, Val loss 1.512\n",
            "Ep 17 (Step 008950): Train loss 1.183, Val loss 1.507\n",
            "Ep 17 (Step 009000): Train loss 1.183, Val loss 1.507\n",
            "Ep 17 (Step 009050): Train loss 1.158, Val loss 1.505\n",
            "Ep 17 (Step 009100): Train loss 1.169, Val loss 1.499\n",
            "Ep 17 (Step 009150): Train loss 1.137, Val loss 1.495\n",
            "Ep 17 (Step 009200): Train loss 1.130, Val loss 1.494\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: Context distillation involves prefixing safety preprompts with adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. This process\n",
            "Ep 18 (Step 009250): Train loss 1.132, Val loss 1.495\n",
            "Ep 18 (Step 009300): Train loss 1.145, Val loss 1.491\n",
            "Ep 18 (Step 009350): Train loss 1.159, Val loss 1.491\n",
            "Ep 18 (Step 009400): Train loss 1.125, Val loss 1.492\n",
            "Ep 18 (Step 009450): Train loss 1.114, Val loss 1.485\n",
            "Ep 18 (Step 009500): Train loss 1.115, Val loss 1.485\n",
            "Ep 18 (Step 009550): Train loss 1.103, Val loss 1.479\n",
            "Ep 18 (Step 009600): Train loss 1.108, Val loss 1.480\n",
            "Ep 18 (Step 009650): Train loss 1.112, Val loss 1.475\n",
            "Ep 18 (Step 009700): Train loss 1.093, Val loss 1.472\n",
            "Ep 18 (Step 009750): Train loss 1.100, Val loss 1.470\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: Context distillation involves prefixing safety preprompts to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. This process\n",
            "Ep 19 (Step 009800): Train loss 1.092, Val loss 1.462\n",
            "Ep 19 (Step 009850): Train loss 1.102, Val loss 1.464\n",
            "Ep 19 (Step 009900): Train loss 1.070, Val loss 1.466\n",
            "Ep 19 (Step 009950): Train loss 1.093, Val loss 1.456\n",
            "Ep 19 (Step 010000): Train loss 1.094, Val loss 1.457\n",
            "Ep 19 (Step 010050): Train loss 1.096, Val loss 1.454\n",
            "Ep 19 (Step 010100): Train loss 1.061, Val loss 1.451\n",
            "Ep 19 (Step 010150): Train loss 1.048, Val loss 1.449\n",
            "Ep 19 (Step 010200): Train loss 1.075, Val loss 1.443\n",
            "Ep 19 (Step 010250): Train loss 1.079, Val loss 1.443\n",
            "Ep 19 (Step 010300): Train loss 1.069, Val loss 1.438\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: Context distillation is employed to generate safer responses by prefixing safety preprompts with adversarial prompts, and then fine-tunes the model on its own safe output given the adversarial prompt without the preprompt.\n",
            "Ep 20 (Step 010350): Train loss 1.059, Val loss 1.442\n",
            "Ep 20 (Step 010400): Train loss 1.059, Val loss 1.435\n",
            "Ep 20 (Step 010450): Train loss 1.051, Val loss 1.435\n",
            "Ep 20 (Step 010500): Train loss 1.065, Val loss 1.435\n",
            "Ep 20 (Step 010550): Train loss 1.023, Val loss 1.433\n",
            "Ep 20 (Step 010600): Train loss 1.043, Val loss 1.427\n",
            "Ep 20 (Step 010650): Train loss 1.025, Val loss 1.427\n",
            "Ep 20 (Step 010700): Train loss 1.026, Val loss 1.426\n",
            "Ep 20 (Step 010750): Train loss 1.045, Val loss 1.420\n",
            "Ep 20 (Step 010800): Train loss 1.029, Val loss 1.419\n",
            "Ep 20 (Step 010850): Train loss 0.996, Val loss 1.413\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: Context distillation involves applying context distillation to adversarial prompts to generate safer responses, and then fine-tunes the model on its own safe output given the adversarial prompt without the preprompt. This process involves generating\n",
            "Ep 21 (Step 010900): Train loss 1.035, Val loss 1.413\n",
            "Ep 21 (Step 010950): Train loss 0.996, Val loss 1.409\n",
            "Ep 21 (Step 011000): Train loss 1.069, Val loss 1.412\n",
            "Ep 21 (Step 011050): Train loss 1.026, Val loss 1.409\n",
            "Ep 21 (Step 011100): Train loss 0.989, Val loss 1.406\n",
            "Ep 21 (Step 011150): Train loss 0.995, Val loss 1.401\n",
            "Ep 21 (Step 011200): Train loss 0.960, Val loss 1.398\n",
            "Ep 21 (Step 011250): Train loss 1.009, Val loss 1.396\n",
            "Ep 21 (Step 011300): Train loss 1.011, Val loss 1.395\n",
            "Ep 21 (Step 011350): Train loss 0.982, Val loss 1.394\n",
            "Ep 21 (Step 011400): Train loss 0.979, Val loss 1.390\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: Context distillation involves applying context distillation to generate safer responses by prefixing safety preprompts with adversarial prompts, and then fine-tuning the model on its own safe output given the adversarial prompt without the\n",
            "Ep 22 (Step 011450): Train loss 0.983, Val loss 1.393\n",
            "Ep 22 (Step 011500): Train loss 0.982, Val loss 1.390\n",
            "Ep 22 (Step 011550): Train loss 0.965, Val loss 1.384\n",
            "Ep 22 (Step 011600): Train loss 0.980, Val loss 1.381\n",
            "Ep 22 (Step 011650): Train loss 0.967, Val loss 1.375\n",
            "Ep 22 (Step 011700): Train loss 0.962, Val loss 1.379\n",
            "Ep 22 (Step 011750): Train loss 0.954, Val loss 1.372\n",
            "Ep 22 (Step 011800): Train loss 0.972, Val loss 1.369\n",
            "Ep 22 (Step 011850): Train loss 0.965, Val loss 1.363\n",
            "Ep 22 (Step 011900): Train loss 0.973, Val loss 1.367\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: Context distillation involves applying context distillation to adversarial prompts to generate safer responses, followed by fine-tuning the model on its own safe output given the adversarial prompt without the preprompt. This process involves generating\n",
            "Ep 23 (Step 011950): Train loss 0.907, Val loss 1.366\n",
            "Ep 23 (Step 012000): Train loss 0.952, Val loss 1.363\n",
            "Ep 23 (Step 012050): Train loss 0.923, Val loss 1.362\n",
            "Ep 23 (Step 012100): Train loss 0.972, Val loss 1.361\n",
            "Ep 23 (Step 012150): Train loss 0.937, Val loss 1.361\n",
            "Ep 23 (Step 012200): Train loss 0.939, Val loss 1.359\n",
            "Ep 23 (Step 012250): Train loss 0.925, Val loss 1.352\n",
            "Ep 23 (Step 012300): Train loss 0.944, Val loss 1.351\n",
            "Ep 23 (Step 012350): Train loss 0.913, Val loss 1.348\n",
            "Ep 23 (Step 012400): Train loss 0.936, Val loss 1.343\n",
            "Ep 23 (Step 012450): Train loss 0.898, Val loss 1.341\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: Context distillation involves applying context distillation to adversarial prompts to generate safer responses, followed by fine-tuning the model on its own safe output given the adversarial prompt. This process involves generating safety preprompts\n",
            "Ep 24 (Step 012500): Train loss 0.914, Val loss 1.342\n",
            "Ep 24 (Step 012550): Train loss 0.894, Val loss 1.345\n",
            "Ep 24 (Step 012600): Train loss 0.893, Val loss 1.342\n",
            "Ep 24 (Step 012650): Train loss 0.898, Val loss 1.340\n",
            "Ep 24 (Step 012700): Train loss 0.940, Val loss 1.335\n",
            "Ep 24 (Step 012750): Train loss 0.861, Val loss 1.332\n",
            "Ep 24 (Step 012800): Train loss 0.886, Val loss 1.329\n",
            "Ep 24 (Step 012850): Train loss 0.884, Val loss 1.327\n",
            "Ep 24 (Step 012900): Train loss 0.868, Val loss 1.329\n",
            "Ep 24 (Step 012950): Train loss 0.879, Val loss 1.325\n",
            "Ep 24 (Step 013000): Train loss 0.876, Val loss 1.320\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39. We find that safety preprompts are more effective than adversarial prompts in generating safer responses, even when the preprompt is paired with adversarial prompts.  ### Response: Context distillation involves applying context distillation to generate safer responses by\n",
            "Ep 25 (Step 013050): Train loss 0.882, Val loss 1.318\n",
            "Ep 25 (Step 013100): Train loss 0.888, Val loss 1.320\n",
            "Ep 25 (Step 013150): Train loss 0.864, Val loss 1.320\n",
            "Ep 25 (Step 013200): Train loss 0.869, Val loss 1.312\n",
            "Ep 25 (Step 013250): Train loss 0.855, Val loss 1.307\n",
            "Ep 25 (Step 013300): Train loss 0.867, Val loss 1.304\n",
            "Ep 25 (Step 013350): Train loss 0.866, Val loss 1.305\n",
            "Ep 25 (Step 013400): Train loss 0.866, Val loss 1.305\n",
            "Ep 25 (Step 013450): Train loss 0.876, Val loss 1.299\n",
            "Ep 25 (Step 013500): Train loss 0.856, Val loss 1.298\n",
            "Ep 25 (Step 013550): Train loss 0.863, Val loss 1.298\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: Context distillation involves prefixing safety preprompts with adversarial prompts to generate safer responses, followed by fine-tuning the model on its own safe output using templates. These preprompts are generated automatically with templates\n",
            "Ep 26 (Step 013600): Train loss 0.849, Val loss 1.297\n",
            "Ep 26 (Step 013650): Train loss 0.862, Val loss 1.299\n",
            "Ep 26 (Step 013700): Train loss 0.845, Val loss 1.295\n",
            "Ep 26 (Step 013750): Train loss 0.856, Val loss 1.295\n",
            "Ep 26 (Step 013800): Train loss 0.843, Val loss 1.291\n",
            "Ep 26 (Step 013850): Train loss 0.826, Val loss 1.291\n",
            "Ep 26 (Step 013900): Train loss 0.844, Val loss 1.286\n",
            "Ep 26 (Step 013950): Train loss 0.793, Val loss 1.284\n",
            "Ep 26 (Step 014000): Train loss 0.832, Val loss 1.281\n",
            "Ep 26 (Step 014050): Train loss 0.847, Val loss 1.280\n",
            "Ep 26 (Step 014100): Train loss 0.827, Val loss 1.279\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: Context distillation involves prefixing safety preprompts with adversarial prompts to generate safer responses, and then fine-tuning the model on its own safe output given the adversarial prompt. This process involves generating safety prep\n",
            "Ep 27 (Step 014150): Train loss 0.838, Val loss 1.275\n",
            "Ep 27 (Step 014200): Train loss 0.835, Val loss 1.273\n",
            "Ep 27 (Step 014250): Train loss 0.813, Val loss 1.270\n",
            "Ep 27 (Step 014300): Train loss 0.802, Val loss 1.268\n",
            "Ep 27 (Step 014350): Train loss 0.818, Val loss 1.267\n",
            "Ep 27 (Step 014400): Train loss 0.829, Val loss 1.261\n",
            "Ep 27 (Step 014450): Train loss 0.821, Val loss 1.266\n",
            "Ep 27 (Step 014500): Train loss 0.811, Val loss 1.258\n",
            "Ep 27 (Step 014550): Train loss 0.800, Val loss 1.256\n",
            "Ep 27 (Step 014600): Train loss 0.798, Val loss 1.250\n",
            "Ep 27 (Step 014650): Train loss 0.805, Val loss 1.254\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: Context distillation involves applying context distillation to generate safer responses by prefixing safety preprompts with adversarial prompts, then fine-tuning the model on its own safe output given the adversarial prompt without the prep\n",
            "Ep 28 (Step 014700): Train loss 0.782, Val loss 1.255\n",
            "Ep 28 (Step 014750): Train loss 0.776, Val loss 1.255\n",
            "Ep 28 (Step 014800): Train loss 0.792, Val loss 1.251\n",
            "Ep 28 (Step 014850): Train loss 0.786, Val loss 1.253\n",
            "Ep 28 (Step 014900): Train loss 0.817, Val loss 1.247\n",
            "Ep 28 (Step 014950): Train loss 0.769, Val loss 1.248\n",
            "Ep 28 (Step 015000): Train loss 0.779, Val loss 1.243\n",
            "Ep 28 (Step 015050): Train loss 0.768, Val loss 1.239\n",
            "Ep 28 (Step 015100): Train loss 0.740, Val loss 1.238\n",
            "Ep 28 (Step 015150): Train loss 0.769, Val loss 1.235\n",
            "Ep 28 (Step 015200): Train loss 0.765, Val loss 1.231\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: Context distillation is employed to generate safer responses by prefixing safety preprompts with adversarial prompts, and fine-tuning the model on its own safe output given the adversarial prompt without the preprompt. This\n",
            "Ep 29 (Step 015250): Train loss 0.759, Val loss 1.234\n",
            "Ep 29 (Step 015300): Train loss 0.763, Val loss 1.238\n",
            "Ep 29 (Step 015350): Train loss 0.764, Val loss 1.233\n",
            "Ep 29 (Step 015400): Train loss 0.745, Val loss 1.232\n",
            "Ep 29 (Step 015450): Train loss 0.765, Val loss 1.229\n",
            "Ep 29 (Step 015500): Train loss 0.751, Val loss 1.227\n",
            "Ep 29 (Step 015550): Train loss 0.767, Val loss 1.226\n",
            "Ep 29 (Step 015600): Train loss 0.744, Val loss 1.218\n",
            "Ep 29 (Step 015650): Train loss 0.766, Val loss 1.217\n",
            "Ep 29 (Step 015700): Train loss 0.754, Val loss 1.217\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: Context distillation involves applying context distillation to adversarial prompts to generate safer responses, followed by fine-tuning the model on its own safe output. This process involves generating safety preprompts using templates, adjectives\n",
            "Ep 30 (Step 015750): Train loss 0.754, Val loss 1.218\n",
            "Ep 30 (Step 015800): Train loss 0.739, Val loss 1.216\n",
            "Ep 30 (Step 015850): Train loss 0.738, Val loss 1.213\n",
            "Ep 30 (Step 015900): Train loss 0.737, Val loss 1.212\n",
            "Ep 30 (Step 015950): Train loss 0.742, Val loss 1.211\n",
            "Ep 30 (Step 016000): Train loss 0.738, Val loss 1.206\n",
            "Ep 30 (Step 016050): Train loss 0.718, Val loss 1.204\n",
            "Ep 30 (Step 016100): Train loss 0.729, Val loss 1.200\n",
            "Ep 30 (Step 016150): Train loss 0.724, Val loss 1.200\n",
            "Ep 30 (Step 016200): Train loss 0.725, Val loss 1.198\n",
            "Ep 30 (Step 016250): Train loss 0.732, Val loss 1.198\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: Context distillation is employed to generate safer responses by prefixing safety preprompts with adversarial prompts, and then fine-tuning the model on its own safe output given the adversarial prompt without the preprompt.\n",
            "Ep 31 (Step 016300): Train loss 0.719, Val loss 1.196\n",
            "Ep 31 (Step 016350): Train loss 0.702, Val loss 1.193\n",
            "Ep 31 (Step 016400): Train loss 0.720, Val loss 1.193\n",
            "Ep 31 (Step 016450): Train loss 0.696, Val loss 1.192\n",
            "Ep 31 (Step 016500): Train loss 0.687, Val loss 1.194\n",
            "Ep 31 (Step 016550): Train loss 0.702, Val loss 1.192\n",
            "Ep 31 (Step 016600): Train loss 0.712, Val loss 1.189\n",
            "Ep 31 (Step 016650): Train loss 0.704, Val loss 1.183\n",
            "Ep 31 (Step 016700): Train loss 0.713, Val loss 1.186\n",
            "Ep 31 (Step 016750): Train loss 0.683, Val loss 1.182\n",
            "Ep 31 (Step 016800): Train loss 0.699, Val loss 1.180\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: Context distillation involves applying context distillation to adversarial prompts to generate safer responses, followed by fine-tuning the model on its own safe output. This process involves generating safety preprompts using templates, adjectives\n",
            "Ep 32 (Step 016850): Train loss 0.704, Val loss 1.181\n",
            "Ep 32 (Step 016900): Train loss 0.694, Val loss 1.179\n",
            "Ep 32 (Step 016950): Train loss 0.679, Val loss 1.175\n",
            "Ep 32 (Step 017000): Train loss 0.693, Val loss 1.170\n",
            "Ep 32 (Step 017050): Train loss 0.682, Val loss 1.172\n",
            "Ep 32 (Step 017100): Train loss 0.677, Val loss 1.169\n",
            "Ep 32 (Step 017150): Train loss 0.675, Val loss 1.164\n",
            "Ep 32 (Step 017200): Train loss 0.675, Val loss 1.165\n",
            "Ep 32 (Step 017250): Train loss 0.687, Val loss 1.162\n",
            "Ep 32 (Step 017300): Train loss 0.688, Val loss 1.166\n",
            "Ep 32 (Step 017350): Train loss 0.670, Val loss 1.161\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39. We also generate safety preprompts in Appendix Table 40. We find that the model associates more negative stereotypes with positive traits when the preprompt is paired with a positive one, suggesting that the model is more likely to associate certain types of undesirable attributes with\n",
            "Ep 33 (Step 017400): Train loss 0.671, Val loss 1.161\n",
            "Ep 33 (Step 017450): Train loss 0.661, Val loss 1.163\n",
            "Ep 33 (Step 017500): Train loss 0.664, Val loss 1.159\n",
            "Ep 33 (Step 017550): Train loss 0.648, Val loss 1.154\n",
            "Ep 33 (Step 017600): Train loss 0.672, Val loss 1.156\n",
            "Ep 33 (Step 017650): Train loss 0.648, Val loss 1.154\n",
            "Ep 33 (Step 017700): Train loss 0.656, Val loss 1.149\n",
            "Ep 33 (Step 017750): Train loss 0.640, Val loss 1.148\n",
            "Ep 33 (Step 017800): Train loss 0.663, Val loss 1.148\n",
            "Ep 33 (Step 017850): Train loss 0.645, Val loss 1.145\n",
            "Ep 33 (Step 017900): Train loss 0.674, Val loss 1.143\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: Context distillation involves applying context distillation to generate safer responses by prefixing a safety preprompt with a safety preprompt, then fine-tuning the model on its own safe output without the preprompt. This process\n",
            "Ep 34 (Step 017950): Train loss 0.654, Val loss 1.146\n",
            "Ep 34 (Step 018000): Train loss 0.636, Val loss 1.147\n",
            "Ep 34 (Step 018050): Train loss 0.646, Val loss 1.144\n",
            "Ep 34 (Step 018100): Train loss 0.651, Val loss 1.142\n",
            "Ep 34 (Step 018150): Train loss 0.651, Val loss 1.144\n",
            "Ep 34 (Step 018200): Train loss 0.638, Val loss 1.137\n",
            "Ep 34 (Step 018250): Train loss 0.621, Val loss 1.137\n",
            "Ep 34 (Step 018300): Train loss 0.628, Val loss 1.135\n",
            "Ep 34 (Step 018350): Train loss 0.633, Val loss 1.131\n",
            "Ep 34 (Step 018400): Train loss 0.639, Val loss 1.128\n",
            "Ep 34 (Step 018450): Train loss 0.623, Val loss 1.130\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: Context distillation involves applying context distillation to generate safer responses by prefixing a safety preprompt with a safety preprompt, followed by fine-tuning the model on its own safe output. This process involves generating safety\n",
            "Ep 35 (Step 018500): Train loss 0.616, Val loss 1.131\n",
            "Ep 35 (Step 018550): Train loss 0.621, Val loss 1.129\n",
            "Ep 35 (Step 018600): Train loss 0.620, Val loss 1.132\n",
            "Ep 35 (Step 018650): Train loss 0.612, Val loss 1.127\n",
            "Ep 35 (Step 018700): Train loss 0.630, Val loss 1.125\n",
            "Ep 35 (Step 018750): Train loss 0.626, Val loss 1.118\n",
            "Ep 35 (Step 018800): Train loss 0.635, Val loss 1.120\n",
            "Ep 35 (Step 018850): Train loss 0.596, Val loss 1.121\n",
            "Ep 35 (Step 018900): Train loss 0.598, Val loss 1.116\n",
            "Ep 35 (Step 018950): Train loss 0.627, Val loss 1.116\n",
            "Ep 35 (Step 019000): Train loss 0.588, Val loss 1.113\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: Context distillation involves preprompting a safety preprompt to adversarial prompts to generate safer responses, followed by fine-tuning the model on its own safe output. This process involves generating safety preprompts using templates\n",
            "Ep 36 (Step 019050): Train loss 0.601, Val loss 1.115\n",
            "Ep 36 (Step 019100): Train loss 0.610, Val loss 1.113\n",
            "Ep 36 (Step 019150): Train loss 0.592, Val loss 1.110\n",
            "Ep 36 (Step 019200): Train loss 0.614, Val loss 1.110\n",
            "Ep 36 (Step 019250): Train loss 0.607, Val loss 1.107\n",
            "Ep 36 (Step 019300): Train loss 0.588, Val loss 1.105\n",
            "Ep 36 (Step 019350): Train loss 0.583, Val loss 1.104\n",
            "Ep 36 (Step 019400): Train loss 0.611, Val loss 1.101\n",
            "Ep 36 (Step 019450): Train loss 0.607, Val loss 1.097\n",
            "Ep 36 (Step 019500): Train loss 0.602, Val loss 1.099\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: Context distillation involves preprompting safety preprompts to adversarial prompts to generate safer responses, followed by fine-tuning the model on its own safe output. This process involves generating safety preprompts automatically with\n",
            "Ep 37 (Step 019550): Train loss 0.583, Val loss 1.097\n",
            "Ep 37 (Step 019600): Train loss 0.600, Val loss 1.100\n",
            "Ep 37 (Step 019650): Train loss 0.573, Val loss 1.098\n",
            "Ep 37 (Step 019700): Train loss 0.585, Val loss 1.098\n",
            "Ep 37 (Step 019750): Train loss 0.575, Val loss 1.096\n",
            "Ep 37 (Step 019800): Train loss 0.590, Val loss 1.098\n",
            "Ep 37 (Step 019850): Train loss 0.596, Val loss 1.092\n",
            "Ep 37 (Step 019900): Train loss 0.574, Val loss 1.093\n",
            "Ep 37 (Step 019950): Train loss 0.569, Val loss 1.088\n",
            "Ep 37 (Step 020000): Train loss 0.569, Val loss 1.087\n",
            "Ep 37 (Step 020050): Train loss 0.575, Val loss 1.084\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39. We use the same safety preprompts for all Gemma 3 models.  ### Response: Context distillation involves preprompting safety preprompts to adversarial prompts to generate safer responses, followed by fine-tuning the model on\n",
            "Ep 38 (Step 020100): Train loss 0.554, Val loss 1.083\n",
            "Ep 38 (Step 020150): Train loss 0.571, Val loss 1.089\n",
            "Ep 38 (Step 020200): Train loss 0.566, Val loss 1.086\n",
            "Ep 38 (Step 020250): Train loss 0.565, Val loss 1.081\n",
            "Ep 38 (Step 020300): Train loss 0.559, Val loss 1.082\n",
            "Ep 38 (Step 020350): Train loss 0.555, Val loss 1.076\n",
            "Ep 38 (Step 020400): Train loss 0.558, Val loss 1.076\n",
            "Ep 38 (Step 020450): Train loss 0.565, Val loss 1.074\n",
            "Ep 38 (Step 020500): Train loss 0.558, Val loss 1.074\n",
            "Ep 38 (Step 020550): Train loss 0.539, Val loss 1.073\n",
            "Ep 38 (Step 020600): Train loss 0.552, Val loss 1.073\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: Context distillation involves applying context distillation to generate safer responses by prefixing a safety preprompt with a safety preprompt, followed by fine-tuning the model on its own safe output. This process involves generating safety\n",
            "Ep 39 (Step 020650): Train loss 0.550, Val loss 1.074\n",
            "Ep 39 (Step 020700): Train loss 0.561, Val loss 1.072\n",
            "Ep 39 (Step 020750): Train loss 0.551, Val loss 1.071\n",
            "Ep 39 (Step 020800): Train loss 0.532, Val loss 1.070\n",
            "Ep 39 (Step 020850): Train loss 0.534, Val loss 1.068\n",
            "Ep 39 (Step 020900): Train loss 0.552, Val loss 1.066\n",
            "Ep 39 (Step 020950): Train loss 0.544, Val loss 1.065\n",
            "Ep 39 (Step 021000): Train loss 0.518, Val loss 1.063\n",
            "Ep 39 (Step 021050): Train loss 0.559, Val loss 1.061\n",
            "Ep 39 (Step 021100): Train loss 0.545, Val loss 1.060\n",
            "Ep 39 (Step 021150): Train loss 0.537, Val loss 1.060\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: Context distillation involves applying context distillation to adversarial prompts to generate safer responses, followed by fine-tuning the model on its own safe output. This process involves generating safety preprompts using templates, adjectives\n",
            "Ep 40 (Step 021200): Train loss 0.541, Val loss 1.064\n",
            "Ep 40 (Step 021250): Train loss 0.535, Val loss 1.060\n",
            "Ep 40 (Step 021300): Train loss 0.525, Val loss 1.058\n",
            "Ep 40 (Step 021350): Train loss 0.520, Val loss 1.060\n",
            "Ep 40 (Step 021400): Train loss 0.538, Val loss 1.054\n",
            "Ep 40 (Step 021450): Train loss 0.535, Val loss 1.056\n",
            "Ep 40 (Step 021500): Train loss 0.515, Val loss 1.056\n",
            "Ep 40 (Step 021550): Train loss 0.529, Val loss 1.052\n",
            "Ep 40 (Step 021600): Train loss 0.509, Val loss 1.051\n",
            "Ep 40 (Step 021650): Train loss 0.531, Val loss 1.051\n",
            "Ep 40 (Step 021700): Train loss 0.537, Val loss 1.048\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: Context distillation involves applying context distillation to generate safer responses by prefixing a safety preprompt with a safety preprompt, followed by fine-tuning the model on its own safe output. This process ensures that the\n",
            "Ep 41 (Step 021750): Train loss 0.535, Val loss 1.052\n",
            "Ep 41 (Step 021800): Train loss 0.504, Val loss 1.052\n",
            "Ep 41 (Step 021850): Train loss 0.536, Val loss 1.050\n",
            "Ep 41 (Step 021900): Train loss 0.508, Val loss 1.051\n",
            "Ep 41 (Step 021950): Train loss 0.516, Val loss 1.045\n",
            "Ep 41 (Step 022000): Train loss 0.511, Val loss 1.045\n",
            "Ep 41 (Step 022050): Train loss 0.517, Val loss 1.045\n",
            "Ep 41 (Step 022100): Train loss 0.504, Val loss 1.041\n",
            "Ep 41 (Step 022150): Train loss 0.501, Val loss 1.039\n",
            "Ep 41 (Step 022200): Train loss 0.516, Val loss 1.038\n",
            "Ep 41 (Step 022250): Train loss 0.504, Val loss 1.038\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: Context distillation involves applying context distillation to generate safer responses by prefixing a safety preprompt with a safety preprompt, followed by fine-tuning the model on its own safe output. This process involves generating safety\n",
            "Ep 42 (Step 022300): Train loss 0.509, Val loss 1.038\n",
            "Ep 42 (Step 022350): Train loss 0.496, Val loss 1.039\n",
            "Ep 42 (Step 022400): Train loss 0.511, Val loss 1.039\n",
            "Ep 42 (Step 022450): Train loss 0.501, Val loss 1.035\n",
            "Ep 42 (Step 022500): Train loss 0.497, Val loss 1.037\n",
            "Ep 42 (Step 022550): Train loss 0.505, Val loss 1.033\n",
            "Ep 42 (Step 022600): Train loss 0.490, Val loss 1.032\n",
            "Ep 42 (Step 022650): Train loss 0.501, Val loss 1.031\n",
            "Ep 42 (Step 022700): Train loss 0.497, Val loss 1.029\n",
            "Ep 42 (Step 022750): Train loss 0.489, Val loss 1.029\n",
            "Ep 42 (Step 022800): Train loss 0.507, Val loss 1.029\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: Context distillation involves applying context distillation to generate safer responses by prefixing a safety preprompt with a safety preprompt, followed by fine-tuning the model on its own safe output. This process involves generating safety\n",
            "Ep 43 (Step 022850): Train loss 0.481, Val loss 1.029\n",
            "Ep 43 (Step 022900): Train loss 0.503, Val loss 1.031\n",
            "Ep 43 (Step 022950): Train loss 0.501, Val loss 1.032\n",
            "Ep 43 (Step 023000): Train loss 0.477, Val loss 1.029\n",
            "Ep 43 (Step 023050): Train loss 0.466, Val loss 1.029\n",
            "Ep 43 (Step 023100): Train loss 0.478, Val loss 1.026\n",
            "Ep 43 (Step 023150): Train loss 0.499, Val loss 1.024\n",
            "Ep 43 (Step 023200): Train loss 0.490, Val loss 1.018\n",
            "Ep 43 (Step 023250): Train loss 0.472, Val loss 1.017\n",
            "Ep 43 (Step 023300): Train loss 0.471, Val loss 1.018\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: Context distillation involves applying context distillation to generate safer responses by prefixing safety preprompts with adversarial prompts, and then fine-tuning the model on its own safe output given the adversarial prompt without the\n",
            "Ep 44 (Step 023350): Train loss 0.491, Val loss 1.019\n",
            "Ep 44 (Step 023400): Train loss 0.466, Val loss 1.020\n",
            "Ep 44 (Step 023450): Train loss 0.482, Val loss 1.021\n",
            "Ep 44 (Step 023500): Train loss 0.465, Val loss 1.019\n",
            "Ep 44 (Step 023550): Train loss 0.453, Val loss 1.022\n",
            "Ep 44 (Step 023600): Train loss 0.465, Val loss 1.019\n",
            "Ep 44 (Step 023650): Train loss 0.458, Val loss 1.020\n",
            "Ep 44 (Step 023700): Train loss 0.473, Val loss 1.016\n",
            "Ep 44 (Step 023750): Train loss 0.444, Val loss 1.011\n",
            "Ep 44 (Step 023800): Train loss 0.471, Val loss 1.014\n",
            "Ep 44 (Step 023850): Train loss 0.464, Val loss 1.008\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: Context distillation involves applying context distillation to generate safer responses by prefixing a safety preprompt with a safety preprompt, followed by fine-tuning the model on its own safe output. This process involves generating safety\n",
            "Ep 45 (Step 023900): Train loss 0.478, Val loss 1.014\n",
            "Ep 45 (Step 023950): Train loss 0.452, Val loss 1.012\n",
            "Ep 45 (Step 024000): Train loss 0.459, Val loss 1.015\n",
            "Ep 45 (Step 024050): Train loss 0.465, Val loss 1.010\n",
            "Ep 45 (Step 024100): Train loss 0.448, Val loss 1.011\n",
            "Ep 45 (Step 024150): Train loss 0.458, Val loss 1.008\n",
            "Ep 45 (Step 024200): Train loss 0.446, Val loss 1.007\n",
            "Ep 45 (Step 024250): Train loss 0.457, Val loss 1.009\n",
            "Ep 45 (Step 024300): Train loss 0.449, Val loss 1.003\n",
            "Ep 45 (Step 024350): Train loss 0.448, Val loss 1.005\n",
            "Ep 45 (Step 024400): Train loss 0.448, Val loss 1.006\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: Context distillation involves applying context distillation to generate safer responses by prefixing a safety preprompt with a safety preprompt, followed by fine-tuning the model on its own safe output. This process involves generating safety\n",
            "Ep 46 (Step 024450): Train loss 0.458, Val loss 1.006\n",
            "Ep 46 (Step 024500): Train loss 0.449, Val loss 1.012\n",
            "Ep 46 (Step 024550): Train loss 0.432, Val loss 1.007\n",
            "Ep 46 (Step 024600): Train loss 0.433, Val loss 1.001\n",
            "Ep 46 (Step 024650): Train loss 0.446, Val loss 1.002\n",
            "Ep 46 (Step 024700): Train loss 0.426, Val loss 0.999\n",
            "Ep 46 (Step 024750): Train loss 0.427, Val loss 0.998\n",
            "Ep 46 (Step 024800): Train loss 0.440, Val loss 0.996\n",
            "Ep 46 (Step 024850): Train loss 0.444, Val loss 0.998\n",
            "Ep 46 (Step 024900): Train loss 0.454, Val loss 0.995\n",
            "Ep 46 (Step 024950): Train loss 0.447, Val loss 0.997\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: Context distillation involves applying context distillation to generate safer responses by prefixing a safety preprompt with a safety preprompt, followed by fine-tuning the model on its own safe output. This process involves generating safety\n",
            "Ep 47 (Step 025000): Train loss 0.446, Val loss 0.997\n",
            "Ep 47 (Step 025050): Train loss 0.439, Val loss 1.000\n",
            "Ep 47 (Step 025100): Train loss 0.427, Val loss 0.995\n",
            "Ep 47 (Step 025150): Train loss 0.439, Val loss 0.998\n",
            "Ep 47 (Step 025200): Train loss 0.436, Val loss 0.995\n",
            "Ep 47 (Step 025250): Train loss 0.418, Val loss 0.992\n",
            "Ep 47 (Step 025300): Train loss 0.432, Val loss 0.994\n",
            "Ep 47 (Step 025350): Train loss 0.440, Val loss 0.990\n",
            "Ep 47 (Step 025400): Train loss 0.427, Val loss 0.989\n",
            "Ep 47 (Step 025450): Train loss 0.426, Val loss 0.991\n",
            "Ep 47 (Step 025500): Train loss 0.416, Val loss 0.989\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: Context distillation involves applying context distillation to generate safer responses by prefixing a safety preprompt with a safety preprompt, then fine-tuning the model on its own safe output without the preprompt. This process\n",
            "Ep 48 (Step 025550): Train loss 0.415, Val loss 0.991\n",
            "Ep 48 (Step 025600): Train loss 0.418, Val loss 0.992\n",
            "Ep 48 (Step 025650): Train loss 0.420, Val loss 0.993\n",
            "Ep 48 (Step 025700): Train loss 0.436, Val loss 0.991\n",
            "Ep 48 (Step 025750): Train loss 0.421, Val loss 0.989\n",
            "Ep 48 (Step 025800): Train loss 0.427, Val loss 0.987\n",
            "Ep 48 (Step 025850): Train loss 0.415, Val loss 0.988\n",
            "Ep 48 (Step 025900): Train loss 0.428, Val loss 0.987\n",
            "Ep 48 (Step 025950): Train loss 0.413, Val loss 0.981\n",
            "Ep 48 (Step 026000): Train loss 0.416, Val loss 0.986\n",
            "Ep 48 (Step 026050): Train loss 0.404, Val loss 0.979\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: Context distillation involves applying context distillation to generate safer responses by prefixing a safety preprompt with a safety preprompt, followed by fine-tuning the model on its own safe output. This process involves generating safety\n",
            "Ep 49 (Step 026100): Train loss 0.416, Val loss 0.988\n",
            "Ep 49 (Step 026150): Train loss 0.417, Val loss 0.983\n",
            "Ep 49 (Step 026200): Train loss 0.419, Val loss 0.983\n",
            "Ep 49 (Step 026250): Train loss 0.415, Val loss 0.982\n",
            "Ep 49 (Step 026300): Train loss 0.394, Val loss 0.980\n",
            "Ep 49 (Step 026350): Train loss 0.415, Val loss 0.981\n",
            "Ep 49 (Step 026400): Train loss 0.415, Val loss 0.977\n",
            "Ep 49 (Step 026450): Train loss 0.390, Val loss 0.983\n",
            "Ep 49 (Step 026500): Train loss 0.396, Val loss 0.976\n",
            "Ep 49 (Step 026550): Train loss 0.401, Val loss 0.974\n",
            "Ep 49 (Step 026600): Train loss 0.398, Val loss 0.973\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: Context distillation involves applying context distillation to generate safer responses by prefixing a safety preprompt with a safety preprompt, then fine-tuning the model on its own safe output. This process involves generating safety prep\n",
            "Ep 50 (Step 026650): Train loss 0.390, Val loss 0.981\n",
            "Ep 50 (Step 026700): Train loss 0.387, Val loss 0.981\n",
            "Ep 50 (Step 026750): Train loss 0.402, Val loss 0.979\n",
            "Ep 50 (Step 026800): Train loss 0.406, Val loss 0.981\n",
            "Ep 50 (Step 026850): Train loss 0.390, Val loss 0.980\n",
            "Ep 50 (Step 026900): Train loss 0.397, Val loss 0.974\n",
            "Ep 50 (Step 026950): Train loss 0.394, Val loss 0.976\n",
            "Ep 50 (Step 027000): Train loss 0.401, Val loss 0.973\n",
            "Ep 50 (Step 027050): Train loss 0.395, Val loss 0.971\n",
            "Ep 50 (Step 027100): Train loss 0.393, Val loss 0.971\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: Context distillation involves applying context distillation to generate safer responses by prefixing a safety preprompt to adversarial prompts, then fine-tuning the model on its own safe output. This process involves generating safety preprompt\n",
            "Ep 51 (Step 027150): Train loss 0.395, Val loss 0.970\n",
            "Ep 51 (Step 027200): Train loss 0.389, Val loss 0.969\n",
            "Ep 51 (Step 027250): Train loss 0.378, Val loss 0.977\n",
            "Ep 51 (Step 027300): Train loss 0.389, Val loss 0.975\n",
            "Ep 51 (Step 027350): Train loss 0.394, Val loss 0.972\n",
            "Ep 51 (Step 027400): Train loss 0.387, Val loss 0.973\n",
            "Ep 51 (Step 027450): Train loss 0.393, Val loss 0.971\n",
            "Ep 51 (Step 027500): Train loss 0.378, Val loss 0.969\n",
            "Ep 51 (Step 027550): Train loss 0.385, Val loss 0.970\n",
            "Ep 51 (Step 027600): Train loss 0.361, Val loss 0.970\n",
            "Ep 51 (Step 027650): Train loss 0.384, Val loss 0.968\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: Context distillation involves prepending a safety preprompt to adversarial prompts to generate safer responses, followed by fine-tuning the model on its own safe output. This process involves generating safety preprompts automatically using templates\n",
            "Ep 52 (Step 027700): Train loss 0.390, Val loss 0.969\n",
            "Ep 52 (Step 027750): Train loss 0.366, Val loss 0.966\n",
            "Ep 52 (Step 027800): Train loss 0.383, Val loss 0.973\n",
            "Ep 52 (Step 027850): Train loss 0.371, Val loss 0.971\n",
            "Ep 52 (Step 027900): Train loss 0.389, Val loss 0.971\n",
            "Ep 52 (Step 027950): Train loss 0.372, Val loss 0.971\n",
            "Ep 52 (Step 028000): Train loss 0.388, Val loss 0.968\n",
            "Ep 52 (Step 028050): Train loss 0.374, Val loss 0.966\n",
            "Ep 52 (Step 028100): Train loss 0.379, Val loss 0.965\n",
            "Ep 52 (Step 028150): Train loss 0.382, Val loss 0.967\n",
            "Ep 52 (Step 028200): Train loss 0.363, Val loss 0.962\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: Context distillation involves applying context distillation to adversarial prompts to generate safer responses, ensuring that the model generates responses that align with positive societal values. This process involves generating safety preprompts automatically with templates, including adject\n",
            "Ep 53 (Step 028250): Train loss 0.373, Val loss 0.959\n",
            "Ep 53 (Step 028300): Train loss 0.389, Val loss 0.966\n",
            "Ep 53 (Step 028350): Train loss 0.360, Val loss 0.967\n",
            "Ep 53 (Step 028400): Train loss 0.353, Val loss 0.965\n",
            "Ep 53 (Step 028450): Train loss 0.389, Val loss 0.964\n",
            "Ep 53 (Step 028500): Train loss 0.368, Val loss 0.964\n",
            "Ep 53 (Step 028550): Train loss 0.360, Val loss 0.963\n",
            "Ep 53 (Step 028600): Train loss 0.367, Val loss 0.958\n",
            "Ep 53 (Step 028650): Train loss 0.364, Val loss 0.965\n",
            "Ep 53 (Step 028700): Train loss 0.353, Val loss 0.959\n",
            "Ep 53 (Step 028750): Train loss 0.362, Val loss 0.962\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: Context distillation involves applying context distillation to generate safer responses by prefixing a safety preprompt with a safety preprompt, followed by fine-tuning the model on its own safe output. This process generates safety prep\n",
            "Ep 54 (Step 028800): Train loss 0.338, Val loss 0.963\n",
            "Ep 54 (Step 028850): Train loss 0.379, Val loss 0.966\n",
            "Ep 54 (Step 028900): Train loss 0.374, Val loss 0.964\n",
            "Ep 54 (Step 028950): Train loss 0.346, Val loss 0.965\n",
            "Ep 54 (Step 029000): Train loss 0.356, Val loss 0.963\n",
            "Ep 54 (Step 029050): Train loss 0.363, Val loss 0.958\n",
            "Ep 54 (Step 029100): Train loss 0.349, Val loss 0.959\n",
            "Ep 54 (Step 029150): Train loss 0.355, Val loss 0.959\n",
            "Ep 54 (Step 029200): Train loss 0.345, Val loss 0.956\n",
            "Ep 54 (Step 029250): Train loss 0.345, Val loss 0.958\n",
            "Ep 54 (Step 029300): Train loss 0.354, Val loss 0.958\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: Context distillation involves applying context distillation to generate safer responses by prefixing a safety preprompt with a safety preprompt, then fine-tuning the model on its own safe output. This process involves generating safety prep\n",
            "Ep 55 (Step 029350): Train loss 0.363, Val loss 0.957\n",
            "Ep 55 (Step 029400): Train loss 0.360, Val loss 0.965\n",
            "Ep 55 (Step 029450): Train loss 0.363, Val loss 0.960\n",
            "Ep 55 (Step 029500): Train loss 0.362, Val loss 0.959\n",
            "Ep 55 (Step 029550): Train loss 0.357, Val loss 0.959\n",
            "Ep 55 (Step 029600): Train loss 0.355, Val loss 0.959\n",
            "Ep 55 (Step 029650): Train loss 0.355, Val loss 0.955\n",
            "Ep 55 (Step 029700): Train loss 0.347, Val loss 0.959\n",
            "Ep 55 (Step 029750): Train loss 0.359, Val loss 0.955\n",
            "Ep 55 (Step 029800): Train loss 0.344, Val loss 0.952\n",
            "Ep 55 (Step 029850): Train loss 0.337, Val loss 0.952\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: Specifically, we apply context distillation by prefixing a safety preprompt to adversarial prompts to generate safer responses, and then fine-tune the model on its own safe output given the adversarial prompt without the preprompt. We generate safety preprompts automatically with templates. In particular, we use various adjectives usually associated with safe behavior such as “responsible,” “respectful’,’ or “wise,” with the intuition that the model associates them with positive traits that we want to see reflected in safe answers. We show examples of safety preprompts in Appendix Table 39.  ### Response: Context distillation involves applying context distillation to generate safer responses by prefixing a safety preprompt, followed by fine-tuning the model on its own safe output. This process involves generating safety preprompts automatically using\n",
            "🟨 saved → gpt_finetuned.pth\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gpt2o = gpt2o.to(device)\n",
        "gpt2o.train()\n",
        "\n",
        "optimizer_gpt2o = torch.optim.AdamW(gpt2o.parameters(), lr=5e-5, weight_decay=0.1)\n",
        "\n",
        "torch.manual_seed(123)\n",
        "train_losses_g, val_losses_g, tokens_g = train_model_simple(\n",
        "    gpt2o, train_loader_gpt2, val_loader_gpt2, optimizer_gpt2o, device,\n",
        "    num_epochs=55, eval_freq=50, eval_iter=20,\n",
        "    start_context=format_input(val_set[0]), tokenizer=tokenizer_gpt2)\n",
        "\n",
        "torch.save(gpt2o.state_dict(), \"models/gpt2o_finetuned.pth\")\n",
        "print(\"🟨 saved → models/gpt2o_finetuned.pth\")\n",
        "\n",
        "epochs_tensor = torch.linspace(0, 55, len(train_losses_g))\n",
        "plot_losses(epochs_tensor, tokens_g, train_losses_g, val_losses_g)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76584e05",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🟨 GPT finetuned model loaded.\n"
          ]
        }
      ],
      "source": [
        "# === Load fine-tuned GPT-2o model ===\n",
        "gpt2 = True\n",
        "tokenizer_gpt2 = tiktoken.get_encoding(\"gpt2\")\n",
        "vocab_size = tokenizer_gpt2.n_vocab\n",
        "NEW_CONFIG[\"vocab_size\"] = vocab_size\n",
        "\n",
        "gpt2o = GPTModel(NEW_CONFIG).to(device)\n",
        "gpt2o.load_state_dict(torch.load(\"gpt2o_finetuned.pth\", map_location=device))\n",
        "gpt2o.eval()\n",
        "print(\"🟨 GPT-2o finetuned model loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ead01a4",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating models responses: 100%|██████████| 500/500 [19:32<00:00,  2.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🟨 Saved GPT model responses → gpt_val_responses.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# === Generate GPT model responses ===\n",
        "gpt2 = True\n",
        "for i, entry in tqdm(enumerate(val_set), total=len(val_set), desc=\"Generating models responses\"):\n",
        "    alpaca_prompt = (\n",
        "        f\"Below is an instruction that describes a task. \"\n",
        "        f\"Write a response that appropriately completes the request.\"\n",
        "        f\"\\n\\n### Instruction:\\n{entry['Instruction']}\")\n",
        "    if entry['Input']:\n",
        "        alpaca_prompt += f\"\\n\\n### Input:\\n{entry['Input']}\"\n",
        "    alpaca_prompt += \"\\n\\n### Response:\\n\"\n",
        "\n",
        "    entry[\"model_response\"] = generate_text(gpt2o, tokenizer_gpt2, alpaca_prompt)\n",
        "\n",
        "with open(\"data\\gpt2o_val_responses.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(val_set, f, indent=2, ensure_ascii=False)\n",
        "print(\"🟨 Saved GPT-2o model responses → data\\gpt2o_val_responses.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab8e22f8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🟨 Loaded 500 GPT val responses.\n"
          ]
        }
      ],
      "source": [
        "# Load GPT model responses\n",
        "with open(\"data\\gpt2o_val_responses.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    gpt2o_val_responses = json.load(f)\n",
        "print(f\"🟨 Loaded {len(gpt2o_val_responses)} GPT2o val responses.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9327cb5",
      "metadata": {},
      "source": [
        "# Thirdly: Compare Performance \n",
        "Compare performance both **Quantitatively** and **Qualitatively** for the three models:\n",
        "   - **Regex**,\n",
        "   - **GPT-2**,\n",
        "   - **GPT-2o (Pretrained Weights)**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44ded91f",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Regex Quantitative Metrics ===\n",
            "BLEU-4:    4.85  (BP=1.00, P1=24.31, P2=7.57, P3=2.73, P4=1.10)\n",
            "ROUGE-1/2/L-F1: 0.379 / 0.111 / 0.197\n",
            "METEOR:    0.232 (used 500)\n",
            "Token-F1:  0.296 (P=0.248, R=0.366)\n",
            "BERTScore: -0.108 (P=-0.239, R=0.031, model=roberta-large, baseline=True)\n",
            "Count:     500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== GPT-2 Quantitative Metrics ===\n",
            "BLEU-4:    9.33  (BP=1.00, P1=35.69, P2=12.85, P3=5.71, P4=2.89)\n",
            "ROUGE-1/2/L-F1: 0.398 / 0.120 / 0.211\n",
            "METEOR:    0.229 (used 500)\n",
            "Token-F1:  0.321 (P=0.304, R=0.340)\n",
            "BERTScore: 0.060 (P=0.009, R=0.112, model=roberta-large, baseline=True)\n",
            "Count:     500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== GPT Quantitative Metrics ===\n",
            "BLEU-4:    14.67  (BP=1.00, P1=42.41, P2=18.23, P3=9.96, P4=6.02)\n",
            "ROUGE-1/2/L-F1: 0.485 / 0.194 / 0.271\n",
            "METEOR:    0.312 (used 500)\n",
            "Token-F1:  0.388 (P=0.361, R=0.421)\n",
            "BERTScore: 0.229 (P=0.193, R=0.266, model=roberta-large, baseline=True)\n",
            "Count:     500\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'BLEU': {'BLEU-4': 14.6746406441629,\n",
              "  'P1': 42.40594972166082,\n",
              "  'P2': 18.23046513284422,\n",
              "  'P3': 9.95689192573248,\n",
              "  'P4': 6.024507690096306,\n",
              "  'BP': 1.0,\n",
              "  'sys_len': 112812,\n",
              "  'ref_len': 94081},\n",
              " 'ROUGE': {'ROUGE-1-F1': 0.48514108417973556,\n",
              "  'ROUGE-2-F1': 0.19427320201089737,\n",
              "  'ROUGE-L-F1': 0.2714083533097513},\n",
              " 'METEOR': {'METEOR': 0.3119298204283951, 'used': 500},\n",
              " 'TokenF1': {'precision': 0.360515352569048,\n",
              "  'recall': 0.4207364914401842,\n",
              "  'F1': 0.38830491272012874},\n",
              " 'BERTScore': {'P': 0.1932792067527771,\n",
              "  'R': 0.26608356833457947,\n",
              "  'F1': 0.22898806631565094,\n",
              "  'model_type': 'roberta-large',\n",
              "  'baseline_rescale': True},\n",
              " 'count': 500}"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate_model_outputs(\"Regex\", regex_val_responses)\n",
        "evaluate_model_outputs(\"GPT-2\", gpt2_val_responses)\n",
        "evaluate_model_outputs(\"GPT-2o\", gpt2o_val_responses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "564fade6",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Judge Scoring (Regex): 100%|██████████| 125/125 [05:11<00:00,  2.49s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Judge] Regex: avg=58.5% | min=0 | max=100 | n=500\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Judge Scoring (GPT-2): 100%|██████████| 125/125 [05:09<00:00,  2.47s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Judge] GPT-2: avg=62.0% | min=0 | max=100 | n=500\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Judge Scoring (GPT): 100%|██████████| 125/125 [05:10<00:00,  2.49s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Judge] GPT: avg=64.8% | min=0 | max=100 | n=500\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'model': 'GPT',\n",
              " 'avg': 64.844,\n",
              " 'min': 0,\n",
              " 'max': 100,\n",
              " 'n': 500,\n",
              " 'scores': [100,\n",
              "  95,\n",
              "  69,\n",
              "  92,\n",
              "  1,\n",
              "  95,\n",
              "  92,\n",
              "  95,\n",
              "  19,\n",
              "  100,\n",
              "  1,\n",
              "  1,\n",
              "  95,\n",
              "  2,\n",
              "  95,\n",
              "  100,\n",
              "  0,\n",
              "  100,\n",
              "  100,\n",
              "  3,\n",
              "  85,\n",
              "  100,\n",
              "  95,\n",
              "  1,\n",
              "  3,\n",
              "  95,\n",
              "  2,\n",
              "  100,\n",
              "  85,\n",
              "  85,\n",
              "  95,\n",
              "  1,\n",
              "  100,\n",
              "  100,\n",
              "  100,\n",
              "  95,\n",
              "  4,\n",
              "  4,\n",
              "  100,\n",
              "  72,\n",
              "  3,\n",
              "  95,\n",
              "  85,\n",
              "  90,\n",
              "  71,\n",
              "  95,\n",
              "  2,\n",
              "  1,\n",
              "  95,\n",
              "  95,\n",
              "  5,\n",
              "  3,\n",
              "  3,\n",
              "  87,\n",
              "  95,\n",
              "  95,\n",
              "  100,\n",
              "  95,\n",
              "  90,\n",
              "  4,\n",
              "  1,\n",
              "  85,\n",
              "  100,\n",
              "  95,\n",
              "  85,\n",
              "  95,\n",
              "  95,\n",
              "  1,\n",
              "  19,\n",
              "  85,\n",
              "  5,\n",
              "  5,\n",
              "  2,\n",
              "  19,\n",
              "  95,\n",
              "  95,\n",
              "  85,\n",
              "  100,\n",
              "  95,\n",
              "  100,\n",
              "  75,\n",
              "  24,\n",
              "  2,\n",
              "  3,\n",
              "  95,\n",
              "  85,\n",
              "  95,\n",
              "  95,\n",
              "  82,\n",
              "  85,\n",
              "  10,\n",
              "  85,\n",
              "  2,\n",
              "  3,\n",
              "  95,\n",
              "  27,\n",
              "  95,\n",
              "  85,\n",
              "  92,\n",
              "  95,\n",
              "  1,\n",
              "  95,\n",
              "  95,\n",
              "  2,\n",
              "  26,\n",
              "  3,\n",
              "  5,\n",
              "  0,\n",
              "  90,\n",
              "  3,\n",
              "  85,\n",
              "  95,\n",
              "  90,\n",
              "  100,\n",
              "  95,\n",
              "  95,\n",
              "  5,\n",
              "  100,\n",
              "  85,\n",
              "  5,\n",
              "  100,\n",
              "  1,\n",
              "  1,\n",
              "  3,\n",
              "  85,\n",
              "  75,\n",
              "  1,\n",
              "  11,\n",
              "  8,\n",
              "  95,\n",
              "  100,\n",
              "  85,\n",
              "  5,\n",
              "  95,\n",
              "  95,\n",
              "  2,\n",
              "  95,\n",
              "  2,\n",
              "  95,\n",
              "  1,\n",
              "  95,\n",
              "  90,\n",
              "  3,\n",
              "  100,\n",
              "  95,\n",
              "  90,\n",
              "  1,\n",
              "  100,\n",
              "  92,\n",
              "  95,\n",
              "  95,\n",
              "  85,\n",
              "  95,\n",
              "  75,\n",
              "  1,\n",
              "  29,\n",
              "  65,\n",
              "  0,\n",
              "  95,\n",
              "  85,\n",
              "  92,\n",
              "  95,\n",
              "  95,\n",
              "  1,\n",
              "  95,\n",
              "  30,\n",
              "  95,\n",
              "  1,\n",
              "  95,\n",
              "  90,\n",
              "  95,\n",
              "  90,\n",
              "  5,\n",
              "  85,\n",
              "  95,\n",
              "  92,\n",
              "  5,\n",
              "  3,\n",
              "  1,\n",
              "  95,\n",
              "  3,\n",
              "  90,\n",
              "  95,\n",
              "  95,\n",
              "  95,\n",
              "  1,\n",
              "  90,\n",
              "  95,\n",
              "  85,\n",
              "  100,\n",
              "  5,\n",
              "  1,\n",
              "  95,\n",
              "  8,\n",
              "  16,\n",
              "  100,\n",
              "  9,\n",
              "  2,\n",
              "  19,\n",
              "  95,\n",
              "  100,\n",
              "  85,\n",
              "  2,\n",
              "  95,\n",
              "  4,\n",
              "  95,\n",
              "  85,\n",
              "  95,\n",
              "  95,\n",
              "  95,\n",
              "  19,\n",
              "  9,\n",
              "  95,\n",
              "  100,\n",
              "  95,\n",
              "  100,\n",
              "  1,\n",
              "  100,\n",
              "  85,\n",
              "  5,\n",
              "  95,\n",
              "  12,\n",
              "  95,\n",
              "  100,\n",
              "  92,\n",
              "  95,\n",
              "  3,\n",
              "  95,\n",
              "  2,\n",
              "  95,\n",
              "  95,\n",
              "  92,\n",
              "  95,\n",
              "  75,\n",
              "  9,\n",
              "  95,\n",
              "  85,\n",
              "  95,\n",
              "  85,\n",
              "  100,\n",
              "  100,\n",
              "  85,\n",
              "  12,\n",
              "  2,\n",
              "  85,\n",
              "  92,\n",
              "  2,\n",
              "  5,\n",
              "  20,\n",
              "  95,\n",
              "  90,\n",
              "  95,\n",
              "  6,\n",
              "  100,\n",
              "  95,\n",
              "  19,\n",
              "  85,\n",
              "  95,\n",
              "  95,\n",
              "  100,\n",
              "  100,\n",
              "  100,\n",
              "  100,\n",
              "  95,\n",
              "  2,\n",
              "  0,\n",
              "  95,\n",
              "  2,\n",
              "  95,\n",
              "  95,\n",
              "  100,\n",
              "  21,\n",
              "  60,\n",
              "  95,\n",
              "  96,\n",
              "  5,\n",
              "  85,\n",
              "  20,\n",
              "  100,\n",
              "  1,\n",
              "  95,\n",
              "  95,\n",
              "  85,\n",
              "  5,\n",
              "  85,\n",
              "  95,\n",
              "  92,\n",
              "  95,\n",
              "  85,\n",
              "  92,\n",
              "  5,\n",
              "  5,\n",
              "  95,\n",
              "  95,\n",
              "  95,\n",
              "  95,\n",
              "  85,\n",
              "  100,\n",
              "  95,\n",
              "  95,\n",
              "  95,\n",
              "  95,\n",
              "  95,\n",
              "  100,\n",
              "  95,\n",
              "  3,\n",
              "  95,\n",
              "  100,\n",
              "  92,\n",
              "  1,\n",
              "  95,\n",
              "  5,\n",
              "  95,\n",
              "  100,\n",
              "  3,\n",
              "  85,\n",
              "  2,\n",
              "  95,\n",
              "  5,\n",
              "  92,\n",
              "  100,\n",
              "  100,\n",
              "  95,\n",
              "  90,\n",
              "  95,\n",
              "  95,\n",
              "  85,\n",
              "  88,\n",
              "  92,\n",
              "  2,\n",
              "  19,\n",
              "  100,\n",
              "  95,\n",
              "  12,\n",
              "  100,\n",
              "  19,\n",
              "  95,\n",
              "  2,\n",
              "  95,\n",
              "  92,\n",
              "  85,\n",
              "  80,\n",
              "  20,\n",
              "  1,\n",
              "  2,\n",
              "  3,\n",
              "  95,\n",
              "  5,\n",
              "  2,\n",
              "  1,\n",
              "  100,\n",
              "  5,\n",
              "  95,\n",
              "  80,\n",
              "  15,\n",
              "  2,\n",
              "  100,\n",
              "  95,\n",
              "  100,\n",
              "  90,\n",
              "  1,\n",
              "  95,\n",
              "  95,\n",
              "  4,\n",
              "  75,\n",
              "  5,\n",
              "  19,\n",
              "  0,\n",
              "  95,\n",
              "  85,\n",
              "  95,\n",
              "  69,\n",
              "  100,\n",
              "  1,\n",
              "  1,\n",
              "  92,\n",
              "  95,\n",
              "  95,\n",
              "  100,\n",
              "  100,\n",
              "  100,\n",
              "  92,\n",
              "  95,\n",
              "  95,\n",
              "  95,\n",
              "  95,\n",
              "  85,\n",
              "  2,\n",
              "  9,\n",
              "  92,\n",
              "  15,\n",
              "  100,\n",
              "  5,\n",
              "  100,\n",
              "  75,\n",
              "  95,\n",
              "  1,\n",
              "  100,\n",
              "  95,\n",
              "  85,\n",
              "  5,\n",
              "  95,\n",
              "  95,\n",
              "  3,\n",
              "  1,\n",
              "  5,\n",
              "  95,\n",
              "  95,\n",
              "  1,\n",
              "  1,\n",
              "  95,\n",
              "  95,\n",
              "  100,\n",
              "  100,\n",
              "  85,\n",
              "  95,\n",
              "  92,\n",
              "  95,\n",
              "  95,\n",
              "  2,\n",
              "  1,\n",
              "  95,\n",
              "  92,\n",
              "  3,\n",
              "  85,\n",
              "  95,\n",
              "  85,\n",
              "  100,\n",
              "  95,\n",
              "  95,\n",
              "  100,\n",
              "  3,\n",
              "  92,\n",
              "  82,\n",
              "  1,\n",
              "  95,\n",
              "  95,\n",
              "  19,\n",
              "  100,\n",
              "  95,\n",
              "  100,\n",
              "  3,\n",
              "  0,\n",
              "  95,\n",
              "  2,\n",
              "  100,\n",
              "  92,\n",
              "  20,\n",
              "  95,\n",
              "  95,\n",
              "  100,\n",
              "  95,\n",
              "  92,\n",
              "  85,\n",
              "  90,\n",
              "  82,\n",
              "  1,\n",
              "  100,\n",
              "  95,\n",
              "  95,\n",
              "  1,\n",
              "  95,\n",
              "  95,\n",
              "  100,\n",
              "  90,\n",
              "  85,\n",
              "  90,\n",
              "  75,\n",
              "  85,\n",
              "  85,\n",
              "  90,\n",
              "  92,\n",
              "  2,\n",
              "  100,\n",
              "  95,\n",
              "  1,\n",
              "  4,\n",
              "  85,\n",
              "  100,\n",
              "  1,\n",
              "  85,\n",
              "  90,\n",
              "  100,\n",
              "  1,\n",
              "  92,\n",
              "  92,\n",
              "  2,\n",
              "  95,\n",
              "  85,\n",
              "  100,\n",
              "  100,\n",
              "  100,\n",
              "  95,\n",
              "  95,\n",
              "  6,\n",
              "  95,\n",
              "  9,\n",
              "  95,\n",
              "  95,\n",
              "  4]}"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate_with_judge(\"Regex\", regex_val_responses, batch_size=4)\n",
        "evaluate_with_judge(\"GPT-2\", gpt2_val_responses,  batch_size=4)\n",
        "evaluate_with_judge(\"GPT-2o\", gpt2o_val_responses,  batch_size=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c99283e7",
      "metadata": {},
      "source": [
        "### Interpretation\n",
        "\n",
        "- The **GPT-2 Pretrained model** performs best across all metrics, as expected.\n",
        "- The **GPT-2 tokenizer model trained from scratch** performs reasonably well and significantly better than the regex version.\n",
        "- The **Regex tokenizer model** performs the weakest, highlighting limitations of simple tokenizers in LLM training.\n",
        "\n",
        "This comparison demonstrates the impact of:\n",
        "- Tokenizer choice  \n",
        "- Pretraining vs. starting from scratch  \n",
        "- Dataset quality and instruction tuning  \n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "LLM",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
