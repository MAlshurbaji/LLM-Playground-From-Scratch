{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fd91e009",
      "metadata": {},
      "source": [
        "# Firstly: Preprocessing Workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "986c574a",
      "metadata": {},
      "source": [
        "## Step 1: Downloading PDFs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81c32c08",
      "metadata": {},
      "source": [
        "#### In this step, we will download all the provided technical-report PDFs, extracts text from each, and combines everything into a single txt file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "583b6a18",
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "import pathlib, re, logging\n",
        "import requests\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "from pdfminer.high_level import extract_text\n",
        "import PyPDF2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e5ffd4c",
      "metadata": {},
      "source": [
        "The provided PDFs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d81142c3",
      "metadata": {},
      "outputs": [],
      "source": [
        "PDF_LIST = [\n",
        "    (\"GPT-3 (Language Models are Few-Shot Learners, 2020)\", \"https://arxiv.org/pdf/2005.14165.pdf\"),\n",
        "    (\"GPT-4 Technical Report (2023)\", \"https://arxiv.org/pdf/2303.08774.pdf\"),\n",
        "    (\"PaLM (2022)\", \"https://arxiv.org/pdf/2204.02311.pdf\"),\n",
        "    (\"PaLM 2 Technical Report (2023)\", \"https://arxiv.org/pdf/2305.10403.pdf\"),\n",
        "    (\"Gemini 1.0 (2023)\", \"https://arxiv.org/pdf/2312.11805.pdf\"),\n",
        "    (\"Gemini 1.5 (2024)\", \"https://arxiv.org/pdf/2403.05530.pdf\"),\n",
        "    (\"Gemma (2024)\", \"https://arxiv.org/pdf/2403.08295.pdf\"),\n",
        "    (\"Gemma 2 (2024)\", \"https://arxiv.org/pdf/2408.00118.pdf\"),\n",
        "    (\"Gemma 3 Technical Report (2025)\", \"https://arxiv.org/pdf/2503.19786.pdf\"),\n",
        "    (\"CodeGemma (2024)\", \"https://arxiv.org/pdf/2406.11409.pdf\"),\n",
        "    (\"RecurrentGemma (2024)\", \"https://arxiv.org/pdf/2404.07839.pdf\"),\n",
        "    (\"LLaMA (2023)\", \"https://arxiv.org/pdf/2302.13971.pdf\"),\n",
        "    (\"Llama 2 (2023)\", \"https://arxiv.org/pdf/2307.09288.pdf\"),\n",
        "    (\"Llama 3 (2024)\", \"https://arxiv.org/pdf/2407.21783.pdf\"),\n",
        "    (\"Mistral 7B (2023)\", \"https://arxiv.org/pdf/2310.06825.pdf\"),\n",
        "    (\"Mixtral of Experts 8x7B (2024)\", \"https://arxiv.org/pdf/2401.04088.pdf\"),\n",
        "    (\"Nemotron-4 340B Technical Report (2024)\", \"https://arxiv.org/pdf/2406.11704.pdf\"),\n",
        "    (\"NVLM 1.0 (2024)\", \"https://arxiv.org/pdf/2409.11402.pdf\"),\n",
        "    (\"Qwen2 Technical Report (2024)\", \"https://arxiv.org/pdf/2407.10671.pdf\"),\n",
        "    (\"Qwen2-VL (2024)\", \"https://arxiv.org/pdf/2409.12191.pdf\"),\n",
        "    (\"Qwen2-Audio (2024)\", \"https://arxiv.org/pdf/2407.10759.pdf\"),\n",
        "    (\"Qwen2.5 Technical Report (2024)\", \"https://arxiv.org/pdf/2412.15115.pdf\"),\n",
        "    (\"Qwen2.5-VL Technical Report (2025)\", \"https://arxiv.org/pdf/2502.13923.pdf\"),\n",
        "    (\"Qwen2.5-Omni Technical Report (2025)\", \"https://arxiv.org/pdf/2503.20215.pdf\"),\n",
        "    (\"Qwen3 Technical Report (2025)\", \"https://arxiv.org/pdf/2505.09388.pdf\"),\n",
        "    (\"DeepSeek-V2 (2024)\", \"https://arxiv.org/pdf/2405.04434.pdf\"),\n",
        "    (\"DeepSeek-V3 Technical Report (2024)\", \"https://arxiv.org/pdf/2412.19437.pdf\"),\n",
        "    (\"DeepSeek-R1 (2025)\", \"https://arxiv.org/pdf/2501.12948.pdf\"),\n",
        "    (\"DeepSeek-Coder (2024)\", \"https://arxiv.org/pdf/2401.14196.pdf\"),\n",
        "    (\"GLM-130B (2022)\", \"https://arxiv.org/pdf/2210.02414.pdf\"),\n",
        "    (\"InternLM2 Technical Report (2024)\", \"https://arxiv.org/pdf/2403.17297.pdf\"),\n",
        "    (\"InternVL 2.5 (2024)\", \"https://arxiv.org/pdf/2412.05271.pdf\"),\n",
        "    (\"Phi-3 Technical Report (2024)\", \"https://arxiv.org/pdf/2404.14219.pdf\"),\n",
        "    (\"Phi-3 Safety Post-Training (2024)\", \"https://arxiv.org/pdf/2407.13833.pdf\"),\n",
        "    (\"Jamba: Hybrid Transformer–Mamba (2024)\", \"https://arxiv.org/pdf/2403.19887.pdf\"),\n",
        "    (\"PanGu-Σ (2023)\", \"https://arxiv.org/pdf/2303.10845.pdf\"),\n",
        "    (\"Yi: Open Foundation Models (2024)\", \"https://arxiv.org/pdf/2403.04652.pdf\"),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36e8b5ae",
      "metadata": {},
      "source": [
        "### (A)  Here `download_pdfs()` will download all the PDF files.\n",
        "\n",
        "**Outputs:**\n",
        "- `data/raw_pdfs/` – all downloaded PDFs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5fc23f3a",
      "metadata": {},
      "outputs": [],
      "source": [
        "OUT_DIR = pathlib.Path('data')\n",
        "PDF_DIR = OUT_DIR / 'raw_pdfs'\n",
        "TXT_DIR = OUT_DIR / 'text'\n",
        "\n",
        "def ensure_dirs(): # To ensure directories exist.\n",
        "    PDF_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    TXT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "ensure_dirs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8d67012b",
      "metadata": {},
      "outputs": [],
      "source": [
        "USER_AGENT = 'HW1-Downloader/1.0 (+https://example.com)'\n",
        "def safe_filename(name: str) -> str: # To create a safe filename from the title.\n",
        "    name = re.sub(r\"[^\\w\\-. ]+\", \"_\", name).strip()\n",
        "    name = re.sub(r\"\\s+\", \"_\", name)\n",
        "    return name[:150]\n",
        "\n",
        "def download_pdfs(entries):\n",
        "    session = requests.Session()\n",
        "    retries = Retry(total=5, backoff_factor=0.8, status_forcelist=[429, 500, 502, 503, 504])\n",
        "    session.mount('https://', HTTPAdapter(max_retries=retries))\n",
        "    session.headers.update({'User-Agent': USER_AGENT})\n",
        "    records = []\n",
        "    for title, url in entries:\n",
        "        fname = safe_filename(title) + '.pdf'\n",
        "        fpath = PDF_DIR / fname\n",
        "        try:\n",
        "            if fpath.exists() and fpath.stat().st_size > 0:\n",
        "                logging.info(f'Skip (exists): {fname}')\n",
        "            else:\n",
        "                logging.info(f'Downloading: {url}')\n",
        "                with session.get(url, stream=True, timeout=60) as r:\n",
        "                    r.raise_for_status()\n",
        "                    with open(fpath, 'wb') as f:\n",
        "                        for chunk in r.iter_content(chunk_size=1048576):\n",
        "                            if chunk:\n",
        "                                f.write(chunk)\n",
        "            size_mb = fpath.stat().st_size / (1024*1024)\n",
        "            records.append((title, url, str(fpath), f'{size_mb:.2f}'))\n",
        "        except Exception as e:\n",
        "            logging.error(f'Failed: {title} — {e}')\n",
        "            records.append((title, url, str(fpath), 'ERROR'))\n",
        "    return records\n",
        "\n",
        "recs = download_pdfs(PDF_LIST) # To download the PDFs if needed."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bd1ba63",
      "metadata": {},
      "source": [
        "### B) Here `extract_text_one()` extract text from each PDF.\n",
        "\n",
        "### C) And `extract_all_to_combined()` uses `extract_text_one()` to combine everything into a single txt file `combined_raw.txt`.\n",
        "\n",
        "**Outputs:**\n",
        "- `data/text/combined_raw.txt` – concatenated raw text with document boundaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "41349ea5",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P2' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P0' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P2' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P3' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P4' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P5' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P6' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P0' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P2' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P3' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P4' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P5' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P6' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P7' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P8' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P9' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P10' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P11' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P12' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P13' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P14' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P15' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P16' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P17' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P18' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P19' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P20' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P21' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P22' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P23' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P24' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P25' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P26' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P27' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'H1' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'H1' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'H1' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'H1' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'H1' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'H1' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'H1' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P0' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P2' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P3' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P4' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P5' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P6' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'H1' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'H1' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'H1' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'H1' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'H1' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'H1' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'H1' is an invalid float value\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P14' is an invalid float value\n"
          ]
        }
      ],
      "source": [
        "COMBINED_TXT = TXT_DIR / 'combined_raw.txt'\n",
        "\n",
        "# logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s: %(message)s', datefmt='%H:%M:%S')\n",
        "# To extract text from each file. \n",
        "def extract_text_one(pdf_path: pathlib.Path) -> str: \n",
        "    try:\n",
        "        text = extract_text(str(pdf_path))\n",
        "        if text and text.strip():\n",
        "            return text\n",
        "    except Exception as e:\n",
        "        logging.warning(f'pdfminer failed on {pdf_path.name}: {e}')\n",
        "    try:\n",
        "        text_parts = []\n",
        "        with open(pdf_path, 'rb') as f:\n",
        "            reader = PyPDF2.PdfReader(f)\n",
        "            for i, page in enumerate(reader.pages):\n",
        "                try:\n",
        "                    text_parts.append(page.extract_text() or '')\n",
        "                except Exception as e:\n",
        "                    logging.warning(f'PyPDF2 failed on page {i} of {pdf_path.name}: {e}')\n",
        "        return '\\n'.join(text_parts)\n",
        "    except Exception as e:\n",
        "        logging.error(f'All extractors failed on {pdf_path.name}: {e}')\n",
        "        return ''\n",
        "\n",
        "# to extract text from all the files and save the to the combined txt file.\n",
        "def extract_all_to_combined(records):\n",
        "    with open(COMBINED_TXT, 'w', encoding='utf-8', errors='ignore') as out:\n",
        "        for title, url, fpath, status in records:\n",
        "            pdf_path = pathlib.Path(fpath)\n",
        "            if status == 'ERROR' or not pdf_path.exists() or pdf_path.stat().st_size == 0:\n",
        "                logging.warning(f'Skipping extraction (bad download): {title}')\n",
        "                continue\n",
        "            logging.info(f'Extracting: {pdf_path.name}')\n",
        "            text = extract_text_one(pdf_path) # To extract text from each file.\n",
        "            out.write('\\n\\n===== DOCUMENT START =====\\n')\n",
        "            out.write(f'TITLE: {title}\\nURL: {url}\\nFILE: {pdf_path.name}\\n\\n')\n",
        "            out.write(text)\n",
        "            out.write('\\n===== DOCUMENT END =====\\n')\n",
        "\n",
        "extract_all_to_combined(recs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "957a4edb",
      "metadata": {},
      "source": [
        "## Step 2 — Cleaning the Combined Text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3dff1fb6",
      "metadata": {},
      "source": [
        "### A) In this step, we cleaned the raw combined text from the PDFs to make it suitable for later tokenization and modeling."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34ecda2a",
      "metadata": {},
      "source": [
        "#### Functions Implemented:\n",
        "- **remove_header_footer_lines** → Remove repeating headers/footers and page numbers.\n",
        "- **fix_hyphenation** → Merge words split across lines with `-` (e.g., infor-\\nmation → information).\n",
        "- **normalize_unicode** → Fix ligatures (ﬁ → fi), normalize unicode.\n",
        "- **drop_toc_and_garbage** → Remove table of contents lines or other PDF-extracted garbage (e.g., “Contents”, “Figure list”, stray navigation text).\n",
        "- **unwrap_soft_linebreaks** → Convert soft newlines inside paragraphs to spaces, keep paragraph breaks.\n",
        "- **drop_figure_table_captions** → Remove figure/table/algorithm captions that are not natural text.\n",
        "- **trim_references_at_end** → Cut off “References” or “Bibliography” sections at the end of documents.\n",
        "- **drop_author_block_preserve_title** → Remove initial author/affiliation block while preserving the paper title.\n",
        "- **clean_one_doc** → Apply all cleaning steps in order to one document.\n",
        "- **split_into_docs_within_combined** → Split the raw combined file into individual documents for cleaning.\n",
        "- **strip_boundaries_and_metadata** → Drop START/END markers and metadata.\n",
        "- **drop_orphan_numeric_lines** → Remove lines that contain only numbers (page numbers, section counters).\n",
        "- **drop_artifact_lines** → Remove random short artifacts.\n",
        "- **remove_cjk** → Remove Chinese/Korean characters and punctuation.\n",
        "- **remove_links** → Strip out URLs.\n",
        "- **remove_empty_lines** → Collapse whitespace by removing empty or whitespace-only lines.\n",
        "\n",
        "**Outputs:**\n",
        "- `data/text/combined_clean.txt` – Cleaned combined text file (ready for tokenization)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "96f4c01a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import re, unicodedata\n",
        "from collections import Counter\n",
        "from typing import List\n",
        "\n",
        "# This function removes headers/footers from the text.\n",
        "def remove_header_footer_lines(text: str) -> str: \n",
        "    lines = text.splitlines()\n",
        "    cnt = Counter(lines)\n",
        "\n",
        "    # Lines that look like page numbers: \"12\", \"Page 3\", \"3 / 20\", \"p. 7\" will be removed.\n",
        "    def is_page_num(line: str) -> bool:\n",
        "        line_stripped = line.strip().lower()\n",
        "        if re.fullmatch(r\"\\d{1,4}\", line_stripped):\n",
        "            return True\n",
        "        if re.fullmatch(r\"(page|p\\.?)\\s*\\d{1,4}(\\s*(of|/)\\s*\\d{1,4})?\", line_stripped):\n",
        "            return True\n",
        "        if re.fullmatch(r\"\\d{1,4}\\s*/\\s*\\d{1,4}\", line_stripped):\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    # Lines that look like running headers: short, often repeated, not sentence-like, will be removed.\n",
        "    header_like = {\n",
        "        l for l, c in cnt.items()\n",
        "        if c >= 3                                  # appears many times\n",
        "        and 3 <= len(l.strip()) <= 80              # typically short\n",
        "        and not re.search(r\"[.!?]$\", l.strip())    # not ending like a sentence\n",
        "        and re.search(r\"[A-Za-z]\", l)              # ensure it has at least one letter\n",
        "    }\n",
        "\n",
        "    kept = []\n",
        "    for l in lines:\n",
        "        if is_page_num(l):          # drop page numbers\n",
        "            continue\n",
        "        if l in header_like:        # drop frequent headers/footers\n",
        "            continue\n",
        "        kept.append(l)\n",
        "    return \"\\n\".join(kept)\n",
        "\n",
        "# PDF line-wrapping often splits words with \"-\\n\". Merge only when next token is lowercase.\n",
        "# e.g., \"infor-\\nmation\" -> \"information\", but keep genuine hyphens like \"state-of-the-art\".\n",
        "def fix_hyphenation(text: str) -> str:\n",
        "    text = re.sub(r\"(?<=\\w)-\\n(?=[a-z])\", \"\", text)\n",
        "    return text\n",
        "\n",
        "# Normalize diacritics & ligatures (ﬁ, ﬂ, …) so tokens are consistent across PDFs.\n",
        "def normalize_unicode(s: str) -> str: \n",
        "    s = unicodedata.normalize(\"NFKC\", s)\n",
        "    s = s.replace(\"ﬁ\", \"fi\").replace(\"ﬂ\", \"fl\")\n",
        "    return s\n",
        "\n",
        "# Preserve paragraphs by templating them, then flatten remaining newlines to spaces.\n",
        "def unwrap_soft_linebreaks(text: str) -> str:\n",
        "    text = text.replace(\"\\r\\n\", \"\\n\")\n",
        "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)            # collapse 3+ newlines -> 2\n",
        "    text = text.replace(\"\\n\\n\", \"<PARA_BREAK>\")       # protect paragraph breaks\n",
        "    text = re.sub(r\"\\n+\", \" \", text)                  # single \\n -> space\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)               # extra spaces -> single\n",
        "    text = text.replace(\"<PARA_BREAK>\", \"\\n\\n\")       # restore paragraphs\n",
        "    return text\n",
        "\n",
        "# This function removes figure and table captions from the text.\n",
        "# Remove blocks that start with Figure/Fig./Table/Algorithm + number.\n",
        "# This targets captions that start at the beginning of a line, so in-sentence mentions like “as shown in Figure 1.1” are preserved.\n",
        "def drop_figure_table_captions(text: str) -> str:\n",
        "    header_pat = r\"(?:extended\\s+data\\s+figure|supplementary\\s+figure|figure|fig\\.|table|algorithm)\"\n",
        "    # ID can be: 2, 2.3, 2.3a, S1, or appendix style like C.1, C.1.2a\n",
        "    id_pat = r\"(?:s?\\d+(?:\\.\\d+)*[a-z]?|[A-Za-z]+(?:\\.\\d+)+[a-z]?)\"\n",
        "\n",
        "    block_pat = re.compile(\n",
        "        rf\"\"\"(?ims)                      # DOTALL + IGNORECASE + MULTILINE\n",
        "        ^\\s*{header_pat}\\s+{id_pat}      # e.g., Figure 1.1 / Table C.1\n",
        "        \\s*(?:[:.\\-–—]\\s*)?               # optional punctuation\n",
        "        .*?                               # caption text (greedy, but bounded by lookahead)\n",
        "        (?=\\n\\s*\\n                        # stop at blank line\n",
        "           |^\\s*{header_pat}\\s+           # or next caption header\n",
        "           |\\Z)                            # or end of text\n",
        "        \"\"\",\n",
        "        re.VERBOSE,\n",
        "    )\n",
        "\n",
        "    single_pat = re.compile(\n",
        "        rf\"\"\"(?im)\n",
        "        ^\\s*{header_pat}\\s+{id_pat}\\s*[:.\\-–—]\\s+.*$\n",
        "        \"\"\",\n",
        "        re.VERBOSE,\n",
        "    )\n",
        "\n",
        "    text = block_pat.sub(\"\\n\", text)\n",
        "    text = single_pat.sub(\"\", text)\n",
        "    return text\n",
        "\n",
        "# This function trims reference sections at the end of each PDF text.\n",
        "# Find a terminal heading like \"References\" / \"Bibliography\" near the end and cut everything after it.\n",
        "def trim_references_at_end(text: str) -> str:\n",
        "    refs = list(re.finditer(r\"(?mi)^\\s*(references|bibliography)\\s*$\", text))\n",
        "    if refs:\n",
        "        cut = refs[-1].start()\n",
        "        # Keep a small lead-in to avoid chopping mid-paragraph if false positive.\n",
        "        return text[:cut].rstrip()\n",
        "    return text\n",
        "\n",
        "# This function drops author blocks but preserves the title.\n",
        "def drop_author_block_preserve_title(text: str) -> str:\n",
        "    lines = text.lstrip().splitlines()\n",
        "    if not lines:\n",
        "        return text\n",
        "\n",
        "    # ---- keep the first non-empty line as the title ----\n",
        "    i = 0\n",
        "    while i < len(lines) and not lines[i].strip():\n",
        "        i += 1\n",
        "    if i >= len(lines):\n",
        "        return text\n",
        "    title = lines[i].strip()\n",
        "    i += 1  # start scanning after title\n",
        "\n",
        "    # ---- stopping points where body typically begins ----\n",
        "    marker_re = re.compile(\n",
        "        r'^(abstract|summary|introduction|keywords|key\\s*words|1[\\.\\)]?\\s|i[\\.\\)]?\\s|contents)\\b',\n",
        "        re.I,\n",
        "    )\n",
        "\n",
        "    # ---- heuristics for author/affiliation lines ----\n",
        "    footnote_syms = set(\"*†‡§#^\")\n",
        "    affil_kw = re.compile(\n",
        "        r'(university|institute|laborator|school|department|dept\\.|research|computer|science|engineering|ai|ml|cs|'\n",
        "        r'google|deepmind|meta|openai|microsoft|alibaba|nvidia|huawei|zhipu|epfl|stanford|mit|oxford|berkeley)',\n",
        "        re.I,\n",
        "    )\n",
        "\n",
        "    def looks_like_author_line(s: str) -> bool:\n",
        "        s = s.strip()\n",
        "        if not s:\n",
        "            return True  # blank lines between author lines\n",
        "        if '@' in s:\n",
        "            return True\n",
        "        if any(sym in s for sym in footnote_syms):\n",
        "            return True\n",
        "        if affil_kw.search(s):\n",
        "            return True\n",
        "        # short-ish and mostly Proper-Name tokens\n",
        "        if len(s) <= 80:\n",
        "            tokens = re.findall(r\"[A-Za-z\\.'\\-]+\", s)\n",
        "            if not tokens:\n",
        "                return True\n",
        "            caps = sum(1 for t in tokens if t[:1].isupper())\n",
        "            lower = sum(1 for t in tokens if t[:1].islower())\n",
        "            if caps >= max(2, int(0.6 * len(tokens))) and lower <= 3:\n",
        "                return True\n",
        "        # author lines like \"Tom B. Brown, Benjamin Mann, …\"\n",
        "        if re.match(r\"^[A-Z][A-Za-z\\.'\\-]+(?:\\s+[A-Z][A-Za-z\\.'\\-]+)*(?:\\s*,\\s*[A-Z][A-Za-z\\.'\\-]+.*)+$\", s):\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    # ---- skip author/affiliation block ----\n",
        "    j = i\n",
        "    while j < len(lines):\n",
        "        s = lines[j].strip()\n",
        "        if marker_re.match(s):\n",
        "            break\n",
        "        if looks_like_author_line(s):\n",
        "            j += 1\n",
        "            continue\n",
        "        # long prose -> likely body\n",
        "        if len(s) > 100 and sum(ch.islower() for ch in s) > 10:\n",
        "            break\n",
        "        # conservative stop\n",
        "        break\n",
        "\n",
        "    out = [title, \"\"] + lines[j:]\n",
        "    return \"\\n\".join(out)\n",
        "\n",
        "# This function removes table of contents, dotted leaders, and standalone junk lines.\n",
        "def drop_toc_and_garbage(text: str) -> str:\n",
        "    # First, strip out any block between 'Contents' and 'Introduction' (case-insensitive)\n",
        "    text = re.sub(\n",
        "        r'(?is)^\\s*contents\\s*.*?(?=^\\s*introduction\\s*$)',\n",
        "        '', text, flags=re.MULTILINE | re.DOTALL\n",
        "    )\n",
        "\n",
        "    # Safer TOC + junk removal (only early in the doc; keep normal prose)\n",
        "    lines = text.splitlines()\n",
        "    cleaned = []\n",
        "    in_toc = False\n",
        "    seen_nonempty = 0\n",
        "\n",
        "    # TOC patterns: dotted leaders **with a page number** at the end.\n",
        "    dotted_leader = re.compile(r'^[ \\t]*(?:\\d+(?:\\.\\d+)*)?[ \\t]*.+?\\.{3,}[ \\t]*\\d{1,4}[ \\t]*$')\n",
        "    # Junk line like: \"I u J\" (single letters separated by spaces)\n",
        "    spaced_single_letters = re.compile(r'^[A-Za-z](?:\\s+[A-Za-z]){1,6}$')\n",
        "    # Punctuation-only line\n",
        "    punct_only = re.compile(r'^[\\W_]+$')\n",
        "\n",
        "    for i, l in enumerate(lines):\n",
        "        s = l.strip()\n",
        "\n",
        "        # Count non-empty to detect the very beginning of the doc\n",
        "        if s:\n",
        "            seen_nonempty += 1\n",
        "\n",
        "        # Enter TOC only very early (first ~80 non-empty lines) if header is present\n",
        "        if not in_toc and seen_nonempty <= 80 and re.match(r'(?i)^(contents|table of contents)\\s*$', s):\n",
        "            in_toc = True\n",
        "            continue\n",
        "\n",
        "        if in_toc:\n",
        "            # Stay in TOC while lines look like TOC entries (dotted leaders with page nums)\n",
        "            if dotted_leader.match(s):\n",
        "                continue\n",
        "            # Exit TOC on the first blank or clearly non-TOC line\n",
        "            if not s or ('.' not in s and not dotted_leader.match(s)):\n",
        "                in_toc = False\n",
        "            if in_toc:\n",
        "                continue  # still skipping TOC lines\n",
        "\n",
        "        # Gentle junk filters:\n",
        "        if not s:\n",
        "            cleaned.append(l)\n",
        "            continue\n",
        "        if punct_only.match(s):\n",
        "            continue\n",
        "        if spaced_single_letters.match(s):\n",
        "            continue\n",
        "        # If the line has at least one word with >=3 letters, it's likely real text -> keep.\n",
        "        if re.search(r'\\b[A-Za-z]{3,}\\b', s):\n",
        "            cleaned.append(l)\n",
        "            continue\n",
        "        # Otherwise, keep by default (be conservative).\n",
        "        cleaned.append(l)\n",
        "\n",
        "    return \"\\n\".join(cleaned)\n",
        "\n",
        "# This function drops orphan numeric lines that are likely table/figure artifacts.\n",
        "def drop_orphan_numeric_lines(text: str) -> str:\n",
        "    ALPHA3_WORD_RE = re.compile(r'\\b[A-Za-z]{3,}\\b')                 # any normal word (>=3 letters)\n",
        "    HAS_DIGIT_RE   = re.compile(r'\\d')                               # contains a digit\n",
        "    ONLY_NUMERIC_CHARS_RE = re.compile(r'^[\\d\\.\\s,:%/()+-]+$')       # numbers & punctuation only\n",
        "    FRACTION_RE    = re.compile(r'^\\s*\\d+\\s*/\\s*\\d+\\s*$')            # like \"87 / 150\"\n",
        "    PERCENT_LINE_RE= re.compile(r'^\\s*\\d+(?:\\.\\d+)?\\s*%$')           # like \"75 %\"\n",
        "    SHORT_LABEL_RE = re.compile(r'^(?:\\d{1,2}[A-Za-z]{1,3}[+\\-]?|[A-Za-z]{1,3}\\d{1,2}[+\\-]?)$')\n",
        "    out_lines = []\n",
        "    for l in text.splitlines():\n",
        "        s = l.strip()\n",
        "        if not s:\n",
        "            out_lines.append(l); continue\n",
        "\n",
        "        # Keep immediately if there's a normal word (>=3 letters) or line is long\n",
        "        if ALPHA3_WORD_RE.search(s) or len(s) > 40 or not HAS_DIGIT_RE.search(s):\n",
        "            out_lines.append(l); continue\n",
        "\n",
        "        alpha_cnt = sum(c.isalpha() for c in s)\n",
        "        digit_cnt = sum(c.isdigit() for c in s)\n",
        "\n",
        "        # Candidate orphan numeric line: digits present, very few letters\n",
        "        if digit_cnt and alpha_cnt <= 2:\n",
        "            # Drop common table/figure leftovers\n",
        "            if (ONLY_NUMERIC_CHARS_RE.fullmatch(s)\n",
        "                or FRACTION_RE.fullmatch(s)\n",
        "                or PERCENT_LINE_RE.fullmatch(s)\n",
        "                or SHORT_LABEL_RE.fullmatch(s)):\n",
        "                continue  # drop\n",
        "        out_lines.append(l)\n",
        "    return \"\\n\".join(out_lines)\n",
        "\n",
        "# This function drops artifact lines that are likely non-content (short, non-prose artifacts).\n",
        "def drop_artifact_lines(text: str) -> str:\n",
        "    def is_artifact(s: str) -> bool:\n",
        "        s0 = s.strip()\n",
        "        if not s0:\n",
        "            return False\n",
        "\n",
        "        # Spaced letters like: \"A V a L L\"\n",
        "        if re.fullmatch(r'(?:[A-Za-z]\\s+){2,}[A-Za-z]', s0):\n",
        "            return True\n",
        "\n",
        "        alnum = re.sub(r'[^A-Za-z0-9]', '', s0)\n",
        "        if not alnum:\n",
        "            return False\n",
        "\n",
        "        if len(alnum) <= 12:\n",
        "            upper = sum(c.isupper() for c in alnum)\n",
        "            digits = sum(c.isdigit() for c in alnum)\n",
        "            vowels = sum(c in 'AEIOUaeiou' for c in alnum)\n",
        "            flips = sum(\n",
        "                (alnum[i].islower() and alnum[i-1].isupper()) or\n",
        "                (alnum[i].isupper() and alnum[i-1].islower())\n",
        "                for i in range(1, len(alnum))\n",
        "            )\n",
        "\n",
        "            upper_ratio = upper / len(alnum)\n",
        "            digit_ratio = digits / len(alnum)\n",
        "            vowel_ratio = vowels / len(alnum)\n",
        "            flip_ratio = flips / max(1, (len(alnum) - 1))\n",
        "\n",
        "            # All-caps heavy, digit-heavy, vowel-poor, or high case-flip noise\n",
        "            if upper_ratio > 0.7 or digit_ratio > 0.5 or vowel_ratio < 0.2 or flip_ratio >= 0.5:\n",
        "                return True\n",
        "\n",
        "            # Camel-case blips with few vowels (e.g., 'vviXra')\n",
        "            if re.fullmatch(r'[a-z]+[A-Z][a-z]+', alnum) and vowel_ratio < 0.35 and len(alnum) <= 8:\n",
        "                return True\n",
        "\n",
        "            # Short ALL-CAPS with ≤1 vowel (e.g., 'FGM', 'OTPG')\n",
        "            if re.fullmatch(r'[A-Z0-9]{3,}', alnum) and vowels <= 1:\n",
        "                return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    return \"\\n\".join(l for l in text.splitlines() if not is_artifact(l))\n",
        "\n",
        "# This function removes empty lines from the text.\n",
        "def remove_empty_lines(text: str) -> str:\n",
        "    return \"\\n\".join(line for line in text.splitlines() if line.strip())\n",
        "\n",
        "# This function removes Chinese and Korean characters from the text.\n",
        "def remove_cjk(text: str) -> str:\n",
        "    return re.sub(r'[\\u4e00-\\u9fff\\uac00-\\ud7af\\u1100-\\u11ff\\u3130-\\u318f\\u3000-\\u303F\\uFF00-\\uFFEF]+', '', text)\n",
        "\n",
        "# This function removes http/https URLs from the text.\n",
        "def remove_links(text: str) -> str:\n",
        "    return re.sub(r'http[s]?://\\S+', '', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2c935dba",
      "metadata": {},
      "outputs": [],
      "source": [
        "# This function applies all cleaning steps to a single document's text.\n",
        "def clean_one_doc(doc_text: str) -> str:\n",
        "    t = doc_text\n",
        "    t = normalize_unicode(t)                 # normalize first\n",
        "    t = drop_toc_and_garbage(t)              # drop TOC & junk lines\n",
        "    t = remove_header_footer_lines(t)        # drop repeating noise & page numbers\n",
        "    t = fix_hyphenation(t)                   # merge \"word-\\nwrap\"\n",
        "    t = drop_figure_table_captions(t)        # remove caption blocks\n",
        "    t = remove_cjk(t)                        # strip Chinese and Korean characters\n",
        "    t = remove_links(t)                      # strip links\n",
        "    t = drop_orphan_numeric_lines(t)         # drop orphan numeric lines that are likely table/figure artifacts\n",
        "    t = trim_references_at_end(t)            # cut terminal references section\n",
        "    t = unwrap_soft_linebreaks(t)            # unwrap paragraph-internal line breaks\n",
        "    t = drop_artifact_lines(t)               # drop other artifact lines\n",
        "    t = drop_author_block_preserve_title(t)  # drop author block but keep title\n",
        "    t = remove_empty_lines(t)                # drop empty/whitespace-only lines\n",
        "\n",
        "    # final tidy\n",
        "    t = re.sub(r\"[ \\t]+\\n\", \"\\n\", t)         # strip trailing spaces at line ends\n",
        "    t = re.sub(r\"\\n{3,}\", \"\\n\\n\", t)         # normalize blank lines\n",
        "    return t.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "c8714f96",
      "metadata": {},
      "outputs": [],
      "source": [
        "# This function splits the combined raw text into individual documents.\n",
        "def split_into_docs_within_combined(raw: str) -> List[str]:\n",
        "    parts = re.split(r\"(?m)^===== DOCUMENT START =====\\s*$\", raw)\n",
        "    docs = []\n",
        "    for p in parts:\n",
        "        if not p.strip():\n",
        "            continue\n",
        "        # each part ends before the corresponding END marker; keep within it\n",
        "        end_split = re.split(r\"(?m)^===== DOCUMENT END =====\\s*$\", p, maxsplit=1)\n",
        "        doc_chunk = end_split[0]\n",
        "        docs.append(doc_chunk.strip())\n",
        "    return docs\n",
        "\n",
        "# This function removes the STEP 1 scaffolding lines to keep only paper body.\n",
        "# Drop the START/END markers and TITLE/URL/FILE metadata lines.\n",
        "def strip_boundaries_and_metadata(block: str) -> str:\n",
        "    block = re.sub(r\"^===== DOCUMENT START =====\\s*$\", \"\", block, flags=re.M)\n",
        "    block = re.sub(r\"^===== DOCUMENT END =====\\s*$\", \"\", block, flags=re.M)\n",
        "    block = re.sub(r\"^(TITLE|URL|FILE):.*\\n?\", \"\", block, flags=re.M)\n",
        "    return block.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6934d76",
      "metadata": {},
      "source": [
        "Here, `data/text/combined_clean.txt` is created, which contains cleaned combined text file (ready for tokenization)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "daf57b14",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved cleaned txt file with 37 documents.\n"
          ]
        }
      ],
      "source": [
        "COMBINED_PATH = \"data/text/combined_raw.txt\"\n",
        "\n",
        "with open(COMBINED_PATH, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "    combined_raw = f.read()\n",
        "\n",
        "# Remove scaffolding & metadata, then clean each paper independently\n",
        "docs_raw = split_into_docs_within_combined(combined_raw)\n",
        "docs_raw = [strip_boundaries_and_metadata(d) for d in docs_raw]\n",
        "docs_cleaned = [clean_one_doc(d) for d in docs_raw]\n",
        "\n",
        "# Concatenate back to a single cleaned corpus (no file is written here)\n",
        "cleaned_corpus = (\"\\n\\n\" + (\"=\" * 80) + \"\\n\\n\").join(docs_cleaned)\n",
        "\n",
        "# Save to file\n",
        "CLEAN_PATH = pathlib.Path(\"data/text/combined_clean.txt\")\n",
        "\n",
        "CLEAN_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "with open(CLEAN_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(cleaned_corpus)\n",
        "\n",
        "print(f\"Saved cleaned txt file with {len(docs_cleaned)} documents.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17e5f04a",
      "metadata": {},
      "source": [
        "## Step 3 — Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5f68e10",
      "metadata": {},
      "source": [
        "### In this step, we explore and compare two different tokenization methods:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f7b8b31",
      "metadata": {},
      "source": [
        "### (A) Regex-based Tokenizer\n",
        "- Implemented by splitting text into **words, punctuation, and whitespace**.\n",
        "- Vocabulary is built directly from the tokens.\n",
        "- Includes special tokens: `<|endoftext|>` and `<|unk|>`.\n",
        "- Implemented as a simple class `SimpleTokenizerV2` with `encode` and `decode`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "7b7e099c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total regex tokens: 441490\n",
            "Regex vocabulary size: 26936\n",
            "Total regex IDs tokens: 441490\n"
          ]
        }
      ],
      "source": [
        "CLEAN_PATH = pathlib.Path(\"data/text/combined_clean.txt\")\n",
        "clean_text = CLEAN_PATH.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "# ====== Regex-based tokenizer ======\n",
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', clean_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(\"Total regex tokens:\", len(preprocessed))\n",
        "\n",
        "# Build vocabulary\n",
        "vocab = sorted(list(set(preprocessed)))\n",
        "vocab.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "vocab_size = len(vocab)\n",
        "print(\"Regex vocabulary size:\", vocab_size)\n",
        "\n",
        "class SimpleTokenizerV2:\n",
        "    def __init__(self, vocab):\n",
        "        self.tokens2ids = {token: id for id, token in enumerate(vocab)}\n",
        "        self.ids2tokens = {id: token for token, id in self.tokens2ids.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        tokens = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "        tokens = [item.strip() for item in tokens if item.strip()]\n",
        "        tokens = [t if t in self.tokens2ids else \"<|unk|>\" for t in tokens]\n",
        "        return [self.tokens2ids[t] for t in tokens]\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.ids2tokens[i] for i in ids])\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text\n",
        "\n",
        "# Initialize regex tokenizer\n",
        "tokenizer_v2 = SimpleTokenizerV2(vocab)\n",
        "\n",
        "# Encode the full text\n",
        "regex_ids = tokenizer_v2.encode(clean_text)\n",
        "print(\"Total regex IDs tokens:\", len(regex_ids))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1cc8499",
      "metadata": {},
      "source": [
        "### (B) Byte Pair Encoding (BPE) Tokenizer\n",
        "- Implemented using the pretrained **GPT-2** vocabulary.\n",
        "- Rare or long words are broken into multiple subword units."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "f961b393",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BPE vocabulary size: 50257\n",
            "Total BPE tokens: 564246\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "# ====== BPE Tokenizer ======\n",
        "tokenizer_gpt2 = tiktoken.get_encoding(\"gpt2\")  # GPT-2 vocab\n",
        "bpe_vocab_size = tokenizer_gpt2.n_vocab\n",
        "print(\"BPE vocabulary size:\", bpe_vocab_size)\n",
        "\n",
        "# Encode the full text\n",
        "bpe_ids = tokenizer_gpt2.encode(clean_text, allowed_special={\"<|endoftext|>\"})\n",
        "print(\"Total BPE tokens:\", len(bpe_ids))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9199c32",
      "metadata": {},
      "source": [
        "### (C) Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "0c3dd1c4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Regex vocabulary size: 26936\n",
            "BPE vocabulary size: 50257\n",
            "Vocabulary count difference (BPE - regex): 23321\n",
            "Total regex tokens: 441490\n",
            "Total BPE tokens: 564246\n",
            "Token count difference (BPE - regex): 122756\n",
            "\n",
            "Unknown word test:\n",
            "Regex IDs: [3458, 25743, 26935, 147]\n",
            "Regex decoded: Alice visited <|unk|>.\n",
            "BPE IDs: [44484, 8672, 2208, 86, 8623, 1044, 13]\n",
            "BPE decoded: Alice visited superwonderland.\n",
            "BPE ID: 44484 \tToken: Alice\n",
            "BPE ID: 8672 \tToken:  visited\n",
            "BPE ID: 2208 \tToken:  super\n",
            "BPE ID: 86 \tToken: w\n",
            "BPE ID: 8623 \tToken: onder\n",
            "BPE ID: 1044 \tToken: land\n",
            "BPE ID: 13 \tToken: .\n"
          ]
        }
      ],
      "source": [
        "print(\"Regex vocabulary size:\", vocab_size)\n",
        "print(\"BPE vocabulary size:\", bpe_vocab_size)\n",
        "print(\"Vocabulary count difference (BPE - regex):\",  bpe_vocab_size - vocab_size)\n",
        "\n",
        "print(\"Total regex tokens:\", len(regex_ids))\n",
        "print(\"Total BPE tokens:\", len(bpe_ids))\n",
        "print(\"Token count difference (BPE - regex):\", len(bpe_ids) - len(regex_ids))\n",
        "\n",
        "# Test with an unknown word.\n",
        "test_sentence = \"Alice visited superwonderland.\"\n",
        "\n",
        "print(\"\\nUnknown word test:\")\n",
        "print(\"Regex IDs:\", tokenizer_v2.encode(test_sentence))\n",
        "print(\"Regex decoded:\", tokenizer_v2.decode(tokenizer_v2.encode(test_sentence)))\n",
        "\n",
        "print(\"BPE IDs:\", tokenizer_gpt2.encode(test_sentence, allowed_special={\"<|endoftext|>\"}))\n",
        "print(\"BPE decoded:\", tokenizer_gpt2.decode(tokenizer_gpt2.encode(test_sentence, allowed_special={\"<|endoftext|>\"})))\n",
        "\n",
        "# Split wording test\n",
        "for i in tokenizer_gpt2.encode(test_sentence, allowed_special={\"<|endoftext|>\"}):\n",
        "    print(\"BPE ID:\", i, \"\\tToken:\", tokenizer_gpt2.decode([i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "761e9d07",
      "metadata": {},
      "source": [
        "- **Regex Tokenizer**  \n",
        "  - Simpler and preserves whole words.  \n",
        "  - text-dependent vocabulary → higher OOV rate when capped.  \n",
        "  - Assigns `<|unk|>` for unseen words.\n",
        "\n",
        "- **BPE Tokenizer**  \n",
        "  - Fixed vocabulary (pretrained, reusable across text).  \n",
        "  - Handles unknown words by splitting them into smaller pieces.  \n",
        "  - Produces more tokens for rare/complex words but avoids `<|unk|>`.  \n",
        "  \n",
        "  ### Insights:\n",
        "- Regex is straightforward and often yields fewer tokens for frequent words.  \n",
        "- BPE is more **robust** and **generalizable**, ensuring full coverage. \n",
        "- For training modern LLMs, **BPE is preferred** due to its balance of vocabulary size, coverage, and efficiency."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec6b5d28",
      "metadata": {},
      "source": [
        "## Step 4 — Dataset & DataLoader (Sliding Window)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "012f0bb2",
      "metadata": {},
      "source": [
        "\n",
        "We will:\n",
        "1) **Tokenize** the cleaned corpus with GPT-2 vocab.  \n",
        "2) **Window** the token stream into `(input, target)` pairs using a sliding window:\n",
        "   - `input = ids[i : i+context_size]`\n",
        "   - `target = ids[i+1 : i+context_size+1]`\n",
        "3) **Split** into train/test (90/10 on the token stream).  \n",
        "4) Wrap with **PyTorch `Dataset`** and **`DataLoader`** for batching.\n",
        "\n",
        "### Setup & load cleaned text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "55a19701",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total tokens in cleaned text: 564246\n"
          ]
        }
      ],
      "source": [
        "import pathlib, torch\n",
        "\n",
        "CLEAN_PATH = pathlib.Path(\"data/text/combined_clean.txt\")\n",
        "clean_text = CLEAN_PATH.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "# Encode entire text\n",
        "enc_text = tokenizer.encode(clean_text, allowed_special={\"<|endoftext|>\"})\n",
        "print(\"Total tokens in cleaned text:\", len(enc_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "519abcdf",
      "metadata": {},
      "source": [
        "### A) Here we prepare a dataset of input–target sequences (using a sliding window approach).\n",
        "### Sliding-window dataset\n",
        "\n",
        "- We use `GPTDatasetV1` structure.  \n",
        "- We will allow passing **pre-encoded token IDs** (so we can split train/test on the token stream precisely).  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "e2d35e5b",
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, data, tokenizer, context_size, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Accept raw text or already-encoded ids\n",
        "        if isinstance(data, str):\n",
        "            token_ids = tokenizer.encode(data, allowed_special={\"<|endoftext|>\"})\n",
        "        else:\n",
        "            token_ids = list(data)  # assume iterable of ints\n",
        "\n",
        "        assert len(token_ids) > context_size, \\\n",
        "            \"Token sequence must be longer than context_size+1.\"\n",
        "\n",
        "        # Sliding-window: step by `stride`\n",
        "        for i in range(0, len(token_ids) - context_size, stride):\n",
        "            x = token_ids[i : i + context_size]\n",
        "            y = token_ids[i + 1 : i + context_size + 1]\n",
        "            self.input_ids.append(torch.tensor(x, dtype=torch.long))\n",
        "            self.target_ids.append(torch.tensor(y, dtype=torch.long))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2adb94c5",
      "metadata": {},
      "source": [
        "### B) Here we implement a PyTorch DataLoader to batch the dataset for training.\n",
        "### Train/Test split on the token stream\n",
        "\n",
        "We split the encoded IDs (90/10) before windowing.  \n",
        "This ensures no leakage and preserves chronological order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "57d0f671",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train batches: 991 | Test batches: 110\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "context_size = 128  # Controls how many previous tokens the model can “see.”\n",
        "stride = 64         # Controls overlap between adjacent windows (smaller stride → more overlapping examples).\n",
        "batch_size = 8\n",
        "\n",
        "# Split 90/10 on the encoded token stream\n",
        "split_at = int(0.9 * len(enc_text))\n",
        "train_ids = enc_text[:split_at]\n",
        "test_ids  = enc_text[split_at:]\n",
        "\n",
        "# Build datasets (pass pre-encoded ids to reuse tokenizer once)\n",
        "train_ds = GPTDatasetV1(train_ids, tokenizer, context_size, stride)\n",
        "test_ds  = GPTDatasetV1(test_ids,  tokenizer, context_size, stride)\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_ds,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,      # better generalization\n",
        "    drop_last=True,\n",
        "    num_workers=0\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_ds,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,     # reproducible order\n",
        "    drop_last=False,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "print(f\"Train batches: {len(train_loader)} | Test batches: {len(test_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eeb2c3e6",
      "metadata": {},
      "source": [
        "**Check:** We want to reuse `iter(...)` and `next(...)` pattern to inspect `(inputs, targets)` pairs.  \n",
        "Each target is the next token of the corresponding input sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "81438e7b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch shapes -> inputs: torch.Size([8, 128]) targets: torch.Size([8, 128])\n",
            "\n",
            "First input IDs:\n",
            " tensor([ 8824, 18250, 12513,   198, 21947, 15168, 14435,  2700,  1377,   554,\n",
            "          257,  2829,  1339,   884,   355,   281,  2134, 19186,  2402,   257,\n",
            "         3084,    11,   262,  3487,  2700,   319,   262,  2134,   318,  4961,\n",
            "          475,   287,  6697,  4571,   284,   262, 29973,  2700,  5625,   319,\n",
            "          262,  2134,   357,   273,   262,  3463,   286,   262,  2134,   828,\n",
            "          326,   318,    11,   399,   796,   285,   308,   357,    59, 13812,\n",
            "         7635,   399,    28, 11296,   828,   810,   285,   318,  2347,    11,\n",
            "          290,   308,   318,   262, 29973,  2214,  4202,   357, 10755,   860,\n",
            "           13,  6659,   285,    14,    82,   319,  3668,   737,   994,  6870,\n",
            "          262,  2700,  5625,   416,   262,  3084,  1028,   262,  2134,   326,\n",
            "        15174,   340,   422, 27141,   832,   262,  3084,   290,  4433,   326,\n",
            "          262,  3084,   318,  2102,    11,   340, 28055,  1576,   284,  5203,\n",
            "          428,  3487,  2700,  1231,  7163,    13,   318,  2562])\n",
            "\n",
            "First target IDs:\n",
            " tensor([18250, 12513,   198, 21947, 15168, 14435,  2700,  1377,   554,   257,\n",
            "         2829,  1339,   884,   355,   281,  2134, 19186,  2402,   257,  3084,\n",
            "           11,   262,  3487,  2700,   319,   262,  2134,   318,  4961,   475,\n",
            "          287,  6697,  4571,   284,   262, 29973,  2700,  5625,   319,   262,\n",
            "         2134,   357,   273,   262,  3463,   286,   262,  2134,   828,   326,\n",
            "          318,    11,   399,   796,   285,   308,   357,    59, 13812,  7635,\n",
            "          399,    28, 11296,   828,   810,   285,   318,  2347,    11,   290,\n",
            "          308,   318,   262, 29973,  2214,  4202,   357, 10755,   860,    13,\n",
            "         6659,   285,    14,    82,   319,  3668,   737,   994,  6870,   262,\n",
            "         2700,  5625,   416,   262,  3084,  1028,   262,  2134,   326, 15174,\n",
            "          340,   422, 27141,   832,   262,  3084,   290,  4433,   326,   262,\n",
            "         3084,   318,  2102,    11,   340, 28055,  1576,   284,  5203,   428,\n",
            "         3487,  2700,  1231,  7163,    13,   318,  2562,   284])\n"
          ]
        }
      ],
      "source": [
        "data_iter = iter(train_loader)\n",
        "x, y = next(data_iter)\n",
        "print(\"Batch shapes -> inputs:\", x.shape, \"targets:\", y.shape)  # [B, T] each\n",
        "\n",
        "# Show first sequence pair\n",
        "print(\"\\nFirst input IDs:\\n\", x[0])\n",
        "print(\"\\nFirst target IDs:\\n\", y[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d8556c0",
      "metadata": {},
      "source": [
        "### Token & Position Embeddings.\n",
        "\n",
        "Here we embed tokens (`nn.Embedding(vocab_size, d_model)`),  \n",
        "and add position embeddings (`nn.Embedding(context_size, d_model)`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "bfcfb191",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input embeddings shape: torch.Size([8, 128, 256])\n"
          ]
        }
      ],
      "source": [
        "vocab_size = 50257          # GPT-2 vocab size\n",
        "d_model = 256               # embedding dimension (example)\n",
        "\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, d_model)\n",
        "pos_embedding_layer   = torch.nn.Embedding(context_size, d_model)\n",
        "\n",
        "# Get one batch\n",
        "data_iter = iter(train_loader)\n",
        "inputs, targets = next(data_iter)   # [B, T]\n",
        "\n",
        "# Token embeddings: [B, T, d_model]\n",
        "token_embeddings = token_embedding_layer(inputs)\n",
        "\n",
        "# Position indices [0..T-1], shape [T], then broadcast to [B, T]\n",
        "positions = torch.arange(inputs.size(1), dtype=torch.long)\n",
        "pos_embeddings = pos_embedding_layer(positions)       # [T, d_model]\n",
        "pos_embeddings = pos_embeddings.unsqueeze(0)          # [1, T, d_model]\n",
        "\n",
        "# Sum token + position embeddings -> final input embeddings\n",
        "input_embeddings = token_embeddings + pos_embeddings   # [B, T, d_model]\n",
        "print(\"Input embeddings shape:\", input_embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "960fe348",
      "metadata": {},
      "source": [
        "## Step 5 —"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "138cd51e",
      "metadata": {},
      "source": [
        "\n",
        "### A) Documents counts\n",
        "We’ll load the **raw combined** file and the **cleaned** file.  \n",
        "Doc counts are computed as:\n",
        "- **Raw**: number of `===== DOCUMENT START =====` markers.  \n",
        "- **Cleaned**: split on the `================================================================================` divider we used when concatenating docs (fallback to 1 if not found)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "14c27499",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Documents — raw: 37 | clean: 37\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "RAW_PATH = Path(\"data/text/combined_raw.txt\")\n",
        "CLEAN_PATH = Path(\"data/text/combined_clean.txt\")\n",
        "\n",
        "raw_text = RAW_PATH.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "clean_text = CLEAN_PATH.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "# Doc count in RAW (based on markers from Step 1)\n",
        "raw_docs = re.split(r\"(?m)^===== DOCUMENT START =====\\s*$\", raw_text)\n",
        "raw_docs = [d for d in raw_docs if d.strip()]\n",
        "n_docs_raw = len(raw_docs)\n",
        "\n",
        "# Doc count in CLEAN (based on 80 '=' chars divider used in Step 2)\n",
        "clean_docs = clean_text.split(\"=\" * 80)  # 80 '=' chars\n",
        "clean_docs = [d.strip() for d in clean_docs if d.strip()]\n",
        "n_docs_clean = max(1, len(clean_docs))   # fallback to 1 if no divider found\n",
        "\n",
        "print(f\"Documents — raw: {n_docs_raw} | clean: {n_docs_clean}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66914306",
      "metadata": {},
      "source": [
        "### A) Total tokens & average tokens per doc (BPE & Regex)\n",
        "\n",
        "We compute:\n",
        "- Total tokens (sum over all documents) - For raw and clean versions.\n",
        "- Average tokens per document - For raw and clean versions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "bbc6502c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "— BPE —\n",
            "Vocabulary size:        50257\n",
            "Total tokens (raw):     1500507\n",
            "Total tokens (clean):   564246\n",
            "Avg tokens/doc (raw):   40554\n",
            "Avg tokens/doc (clean): 15250\n",
            "\n",
            "— Regex —\n",
            "Vocabulary size:        26936\n",
            "Total tokens (raw):     978958\n",
            "Total tokens (clean):   441490\n",
            "Avg tokens/doc (raw):   26458\n",
            "Avg tokens/doc (clean): 11932\n"
          ]
        }
      ],
      "source": [
        "# Tokenize with existing tokenizers\n",
        "raw_bpe_ids   = tokenizer_gpt2.encode(raw_text,  allowed_special={\"<|endoftext|>\"})\n",
        "clean_bpe_ids = tokenizer_gpt2.encode(clean_text, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "raw_regex_ids   = tokenizer_v2.encode(raw_text)\n",
        "clean_regex_ids = tokenizer_v2.encode(clean_text)\n",
        "\n",
        "# Totals\n",
        "total_bpe_raw   = len(raw_bpe_ids)\n",
        "total_bpe_clean = len(clean_bpe_ids)\n",
        "total_regex_raw   = len(raw_regex_ids)\n",
        "total_regex_clean = len(clean_regex_ids)\n",
        "\n",
        "# Vocabulary sizes\n",
        "vocab_bpe   = tokenizer_gpt2.n_vocab\n",
        "vocab_regex = len(tokenizer_v2.tokens2ids)\n",
        "\n",
        "# Averages per document\n",
        "avg_bpe_per_doc_raw   = total_bpe_raw   / n_docs_raw\n",
        "avg_bpe_per_doc_clean = total_bpe_clean / n_docs_clean\n",
        "avg_regex_per_doc_raw   = total_regex_raw   / n_docs_raw\n",
        "avg_regex_per_doc_clean = total_regex_clean / n_docs_clean\n",
        "\n",
        "print(\"— BPE —\")\n",
        "print(f\"Vocabulary size:        {vocab_bpe}\")\n",
        "print(f\"Total tokens (raw):     {total_bpe_raw}\")\n",
        "print(f\"Total tokens (clean):   {total_bpe_clean}\")\n",
        "print(f\"Avg tokens/doc (raw):   {avg_bpe_per_doc_raw:.0f}\")\n",
        "print(f\"Avg tokens/doc (clean): {avg_bpe_per_doc_clean:.0f}\")\n",
        "\n",
        "print(\"\\n— Regex —\")\n",
        "print(f\"Vocabulary size:        {vocab_regex}\")\n",
        "print(f\"Total tokens (raw):     {total_regex_raw}\")\n",
        "print(f\"Total tokens (clean):   {total_regex_clean}\")\n",
        "print(f\"Avg tokens/doc (raw):   {avg_regex_per_doc_raw:.0f}\")\n",
        "print(f\"Avg tokens/doc (clean): {avg_regex_per_doc_clean:.0f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "326928d2",
      "metadata": {},
      "source": [
        "### B) Before vs after cleaning — how much text was removed\n",
        "\n",
        "We compute:\n",
        "- Removal stats (chars + tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "c0afd7f9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "— Characters —\n",
            "Chars (raw):   4824596\n",
            "Chars (clean): 2383987\n",
            "Removed chars: 2440609  (50.59% reduction)\n",
            "\n",
            "— Tokens (BPE) —\n",
            "Tokens (raw):   1500507\n",
            "Tokens (clean): 564246\n",
            "Removed tokens: 936261  (62.40% reduction)\n",
            "\n",
            "— Tokens (Regex) —\n",
            "Tokens (raw):   978958\n",
            "Tokens (clean): 441490\n",
            "Removed tokens: 537468  (54.90% reduction)\n"
          ]
        }
      ],
      "source": [
        "# Character-level change\n",
        "chars_raw   = len(raw_text)\n",
        "chars_clean = len(clean_text)\n",
        "chars_removed = chars_raw - chars_clean\n",
        "pct_chars_removed = (chars_removed / chars_raw * 100.0) if chars_raw > 0 else 0.0\n",
        "\n",
        "# Token-level change (BPE)\n",
        "bpe_removed = len(raw_bpe_ids) - len(clean_bpe_ids)\n",
        "pct_bpe_removed = (bpe_removed / len(raw_bpe_ids) * 100.0) if len(raw_bpe_ids) > 0 else 0.0\n",
        "\n",
        "# Token-level change (Regex)\n",
        "regex_removed = len(raw_regex_ids) - len(clean_regex_ids)\n",
        "pct_regex_removed = (regex_removed / len(raw_regex_ids) * 100.0) if len(raw_regex_ids) > 0 else 0.0\n",
        "\n",
        "print(\"— Characters —\")\n",
        "print(f\"Chars (raw):   {chars_raw}\")\n",
        "print(f\"Chars (clean): {chars_clean}\")\n",
        "print(f\"Removed chars: {chars_removed}  ({pct_chars_removed:.2f}% reduction)\")\n",
        "\n",
        "print(\"\\n— Tokens (BPE) —\")\n",
        "print(f\"Tokens (raw):   {len(raw_bpe_ids)}\")\n",
        "print(f\"Tokens (clean): {len(clean_bpe_ids)}\")\n",
        "print(f\"Removed tokens: {bpe_removed}  ({pct_bpe_removed:.2f}% reduction)\")\n",
        "\n",
        "print(\"\\n— Tokens (Regex) —\")\n",
        "print(f\"Tokens (raw):   {len(raw_regex_ids)}\")\n",
        "print(f\"Tokens (clean): {len(clean_regex_ids)}\")\n",
        "print(f\"Removed tokens: {regex_removed}  ({pct_regex_removed:.2f}% reduction)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4a1a175",
      "metadata": {},
      "source": [
        "# Secondly: End-To-End Training and Instruction Fine-Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d07a55c3",
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "import pathlib, re, logging, requests, torch, tiktoken, random, json\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "from pathlib import Path\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "gpt2 = False\n",
        "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d954448",
      "metadata": {},
      "source": [
        "### Step 1: Load the cleaned and raw text files\n",
        "Load both the raw and cleaned text files from disk to prepare the dataset for pretraining."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6c0ef388",
      "metadata": {},
      "outputs": [],
      "source": [
        "RAW_PATH = Path(\"data/text/combined_raw.txt\")\n",
        "CLEAN_PATH = Path(\"data/text/combined_clean.txt\")\n",
        "\n",
        "raw_text = RAW_PATH.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "clean_text = CLEAN_PATH.read_text(encoding=\"utf-8\", errors=\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df8bcee5",
      "metadata": {},
      "source": [
        "### Step 2: Re-create the custom regex-based tokenizer\n",
        "Tokenize the cleaned text using a regex-based tokenizer, build its vocabulary, and define a simple tokenizer class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "6ede5c03",
      "metadata": {},
      "outputs": [],
      "source": [
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', clean_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "\n",
        "vocab = sorted(list(set(preprocessed)))\n",
        "vocab.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "\n",
        "class SimpleTokenizerV2:\n",
        "    def __init__(self, vocab):\n",
        "        self.tokens2ids = {token: id for id, token in enumerate(vocab)}\n",
        "        self.ids2tokens = {id: token for token, id in self.tokens2ids.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        tokens = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "        tokens = [item.strip() for item in tokens if item.strip()]\n",
        "        tokens = [t if t in self.tokens2ids else \"<|unk|>\" for t in tokens]\n",
        "        return [self.tokens2ids[t] for t in tokens]\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.ids2tokens[i] for i in ids])\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c736643",
      "metadata": {},
      "source": [
        "## Part #1: Pre-Training "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18c96f12",
      "metadata": {},
      "source": [
        "### Step 3: Define GPT model architecture and helper functions\n",
        "This includes the Transformer blocks, attention layers, dataset loader, training loop, evaluation, and text generation utilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "84e7afbd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# This class creates causal multi-head self-attention module.\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec) # optional projection\n",
        "\n",
        "        return context_vec\n",
        "\n",
        "# This class implements the feed-forward network used in transformer blocks.\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "# This class defines a single transformer block with attention and feed-forward layers.\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"])\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = nn.LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = nn.LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        shortcut = x # Save input for residual connection\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        return x\n",
        "\n",
        "# This class defines the overall GPT model architecture.\n",
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = nn.LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits\n",
        "\n",
        "# This class creates a dataset for GPT training using sliding windows.\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, gpt2, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        if gpt2:\n",
        "          token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "        else:\n",
        "          token_ids = tokenizer.encode(txt)\n",
        "\n",
        "        # Use a sliding window to chunk the data into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]\n",
        "\n",
        "# This function creates a DataLoader for the GPT dataset.\n",
        "def create_dataloader_v1(txt, batch_size=4, max_length=32,\n",
        "                         stride=16, shuffle=True, drop_last=True, num_workers=0, tokenizer=None):\n",
        "    \n",
        "    dataset = GPTDatasetV1(txt, gpt2, tokenizer, max_length, stride)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "# This function calculates the cross-entropy loss for a single batch.\n",
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "    return loss\n",
        "\n",
        "# This function calculates the average loss over a data loader.\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches\n",
        "\n",
        "\n",
        "# This function generates text using the model in a simple greedy manner.\n",
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
        "    return idx\n",
        "\n",
        "# This function encodes text to token IDs.\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "  if gpt2:\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "  else:\n",
        "    encoded = tokenizer.encode(text)\n",
        "  encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
        "  return encoded_tensor\n",
        "\n",
        "# This function decodes token IDs back to text.\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "  flat = token_ids.squeeze(0) # remove batch dimension\n",
        "  return tokenizer.decode(flat.tolist())\n",
        "\n",
        "# This function trains the GPT model with periodic evaluation and sample generation.\n",
        "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                       eval_freq, eval_iter, start_context, tokenizer):\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward() # Calculate loss gradients\n",
        "            optimizer.step() # Update model weights using loss gradients\n",
        "            tokens_seen += input_batch.numel()\n",
        "            global_step += 1\n",
        "\n",
        "            # Optional evaluation step\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # Print a sample text after each epoch\n",
        "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen\n",
        "\n",
        "# This function evaluates the model on training and validation sets.\n",
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss\n",
        "\n",
        "# This function generates and prints a sample text from the model.\n",
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate_text_simple(model=model, idx=encoded, max_new_tokens=50, context_size=context_size)\n",
        "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
        "    model.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "902a3233",
      "metadata": {},
      "source": [
        "### Step 4: Define the text generation function\n",
        "Implements top-k sampling and temperature-based generation for more diverse outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "295869df",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        if top_k is not None:\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
        "\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "            logits = logits - logits.max(dim=-1, keepdim=True).values\n",
        "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
        "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
        "            break\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
        "        \n",
        "    return idx\n",
        "\n",
        "# This function plots training and validation losses over epochs and tokens seen.\n",
        "import matplotlib.pyplot as plt\n",
        "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
        "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
        "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.legend(loc=\"upper right\")\n",
        "    ax2 = ax1.twiny() #A\n",
        "    ax2.plot(tokens_seen, train_losses, alpha=0) #B\n",
        "    ax2.set_xlabel(\"Tokens seen\")\n",
        "    fig.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b272af8e",
      "metadata": {},
      "source": [
        "### Step 5: Pretrain GPT model using the custom regex tokenizer\n",
        "Train GPT on your cleaned text using the regex vocabulary, then save the trained model and optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc0ec1a7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep 1 (Step 000000): Train loss 9.903, Val loss 9.947\n",
            "Ep 1 (Step 000010): Train loss 8.360, Val loss 8.413\n",
            "Ep 1 (Step 000020): Train loss 7.742, Val loss 7.904\n",
            "Ep 1 (Step 000030): Train loss 7.396, Val loss 7.515\n",
            "Ep 1 (Step 000040): Train loss 7.007, Val loss 7.304\n",
            "Ep 1 (Step 000050): Train loss 7.010, Val loss 7.226\n",
            "Ep 1 (Step 000060): Train loss 6.990, Val loss 7.155\n",
            "Ep 1 (Step 000070): Train loss 6.785, Val loss 7.123\n",
            "Ep 1 (Step 000080): Train loss 6.737, Val loss 7.082\n",
            "Ep 1 (Step 000090): Train loss 6.522, Val loss 7.037\n",
            "Ep 1 (Step 000100): Train loss 6.855, Val loss 7.098\n",
            "Ep 1 (Step 000110): Train loss 6.654, Val loss 7.009\n",
            "Ep 1 (Step 000120): Train loss 6.515, Val loss 7.016\n",
            "Ep 1 (Step 000130): Train loss 6.587, Val loss 6.957\n",
            "Ep 1 (Step 000140): Train loss 6.395, Val loss 6.928\n",
            "Ep 1 (Step 000150): Train loss 6.488, Val loss 6.904\n",
            "Ep 1 (Step 000160): Train loss 6.427, Val loss 6.868\n",
            "Ep 1 (Step 000170): Train loss 6.473, Val loss 6.852\n",
            "Ep 1 (Step 000180): Train loss 6.390, Val loss 6.904\n",
            "Ep 1 (Step 000190): Train loss 6.271, Val loss 6.829\n",
            "large language models are model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to\n",
            "Ep 2 (Step 000200): Train loss 6.423, Val loss 6.835\n",
            "Ep 2 (Step 000210): Train loss 6.253, Val loss 6.799\n",
            "Ep 2 (Step 000220): Train loss 6.282, Val loss 6.794\n",
            "Ep 2 (Step 000230): Train loss 6.172, Val loss 6.799\n",
            "Ep 2 (Step 000240): Train loss 6.143, Val loss 6.784\n",
            "Ep 2 (Step 000250): Train loss 6.158, Val loss 6.780\n",
            "Ep 2 (Step 000260): Train loss 6.101, Val loss 6.751\n",
            "Ep 2 (Step 000270): Train loss 6.071, Val loss 6.761\n",
            "Ep 2 (Step 000280): Train loss 6.177, Val loss 6.730\n",
            "Ep 2 (Step 000290): Train loss 6.252, Val loss 6.703\n",
            "Ep 2 (Step 000300): Train loss 6.172, Val loss 6.718\n",
            "Ep 2 (Step 000310): Train loss 6.086, Val loss 6.693\n",
            "Ep 2 (Step 000320): Train loss 6.007, Val loss 6.682\n",
            "Ep 2 (Step 000330): Train loss 5.850, Val loss 6.673\n",
            "Ep 2 (Step 000340): Train loss 6.033, Val loss 6.689\n",
            "Ep 2 (Step 000350): Train loss 5.897, Val loss 6.669\n",
            "Ep 2 (Step 000360): Train loss 5.940, Val loss 6.675\n",
            "Ep 2 (Step 000370): Train loss 6.021, Val loss 6.653\n",
            "Ep 2 (Step 000380): Train loss 5.930, Val loss 6.643\n",
            "large language models are not be used in the model to the model, and the model, and the model, we use the model, and the model to the model, we use the model, we use the model, we use the model, and the model, and\n",
            "Ep 3 (Step 000390): Train loss 5.887, Val loss 6.641\n",
            "Ep 3 (Step 000400): Train loss 5.998, Val loss 6.637\n",
            "Ep 3 (Step 000410): Train loss 5.858, Val loss 6.631\n",
            "Ep 3 (Step 000420): Train loss 5.737, Val loss 6.621\n",
            "Ep 3 (Step 000430): Train loss 5.850, Val loss 6.613\n",
            "Ep 3 (Step 000440): Train loss 5.956, Val loss 6.612\n",
            "Ep 3 (Step 000450): Train loss 5.645, Val loss 6.615\n",
            "Ep 3 (Step 000460): Train loss 5.684, Val loss 6.603\n",
            "Ep 3 (Step 000470): Train loss 5.918, Val loss 6.583\n",
            "Ep 3 (Step 000480): Train loss 5.844, Val loss 6.605\n",
            "Ep 3 (Step 000490): Train loss 5.772, Val loss 6.600\n",
            "Ep 3 (Step 000500): Train loss 5.641, Val loss 6.589\n",
            "Ep 3 (Step 000510): Train loss 5.713, Val loss 6.575\n",
            "Ep 3 (Step 000520): Train loss 5.654, Val loss 6.568\n",
            "Ep 3 (Step 000530): Train loss 5.580, Val loss 6.559\n",
            "Ep 3 (Step 000540): Train loss 5.712, Val loss 6.570\n",
            "Ep 3 (Step 000550): Train loss 5.653, Val loss 6.574\n",
            "Ep 3 (Step 000560): Train loss 5.640, Val loss 6.540\n",
            "Ep 3 (Step 000570): Train loss 5.447, Val loss 6.524\n",
            "Ep 3 (Step 000580): Train loss 5.518, Val loss 6.540\n",
            "large language models are not only a large language models, and the model to the model to the model. We use the model. The model. The model.......................\n",
            "Ep 4 (Step 000590): Train loss 5.654, Val loss 6.525\n",
            "Ep 4 (Step 000600): Train loss 5.377, Val loss 6.538\n",
            "Ep 4 (Step 000610): Train loss 5.491, Val loss 6.518\n",
            "Ep 4 (Step 000620): Train loss 5.609, Val loss 6.537\n",
            "Ep 4 (Step 000630): Train loss 5.500, Val loss 6.540\n",
            "Ep 4 (Step 000640): Train loss 5.564, Val loss 6.538\n",
            "Ep 4 (Step 000650): Train loss 5.611, Val loss 6.502\n",
            "Ep 4 (Step 000660): Train loss 5.500, Val loss 6.508\n",
            "Ep 4 (Step 000670): Train loss 5.471, Val loss 6.527\n",
            "Ep 4 (Step 000680): Train loss 5.609, Val loss 6.512\n",
            "Ep 4 (Step 000690): Train loss 5.418, Val loss 6.516\n",
            "Ep 4 (Step 000700): Train loss 5.572, Val loss 6.510\n",
            "Ep 4 (Step 000710): Train loss 5.507, Val loss 6.492\n",
            "Ep 4 (Step 000720): Train loss 5.242, Val loss 6.493\n",
            "Ep 4 (Step 000730): Train loss 5.501, Val loss 6.518\n",
            "Ep 4 (Step 000740): Train loss 5.289, Val loss 6.505\n",
            "Ep 4 (Step 000750): Train loss 5.406, Val loss 6.507\n",
            "Ep 4 (Step 000760): Train loss 5.487, Val loss 6.503\n",
            "Ep 4 (Step 000770): Train loss 5.323, Val loss 6.517\n",
            "large language models are not only a new. We use the model. We also use the model is a large language models. We also have a large language models. 5 Pro. 5. 5. 5 Pro. 5. 5. 5. 5 Pro. 5.\n",
            "Ep 5 (Step 000780): Train loss 5.452, Val loss 6.481\n",
            "Ep 5 (Step 000790): Train loss 5.378, Val loss 6.484\n",
            "Ep 5 (Step 000800): Train loss 5.404, Val loss 6.509\n",
            "Ep 5 (Step 000810): Train loss 5.352, Val loss 6.494\n",
            "Ep 5 (Step 000820): Train loss 5.471, Val loss 6.516\n",
            "Ep 5 (Step 000830): Train loss 5.264, Val loss 6.476\n",
            "Ep 5 (Step 000840): Train loss 5.218, Val loss 6.496\n",
            "Ep 5 (Step 000850): Train loss 5.348, Val loss 6.505\n",
            "Ep 5 (Step 000860): Train loss 5.277, Val loss 6.509\n",
            "Ep 5 (Step 000870): Train loss 5.333, Val loss 6.477\n",
            "Ep 5 (Step 000880): Train loss 5.197, Val loss 6.488\n",
            "Ep 5 (Step 000890): Train loss 5.304, Val loss 6.454\n",
            "Ep 5 (Step 000900): Train loss 5.424, Val loss 6.458\n",
            "Ep 5 (Step 000910): Train loss 5.157, Val loss 6.492\n",
            "Ep 5 (Step 000920): Train loss 5.218, Val loss 6.495\n",
            "Ep 5 (Step 000930): Train loss 5.188, Val loss 6.475\n",
            "Ep 5 (Step 000940): Train loss 5.334, Val loss 6.461\n",
            "Ep 5 (Step 000950): Train loss 5.164, Val loss 6.429\n",
            "Ep 5 (Step 000960): Train loss 5.148, Val loss 6.464\n",
            "large language models are the model. We use the model, we use the model, we use the model, we use the model to the model, we also use the model, we use the model, we use the model, we also be used in the model,\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "gpt2 = False  # custom tokenizer mode\n",
        "tokenizer_v2 = SimpleTokenizerV2(vocab)  # regex tokenizer\n",
        "vocab_size = len(tokenizer_v2.tokens2ids)\n",
        "\n",
        "# === Config ===\n",
        "GPT_CONFIG = {\n",
        "    \"vocab_size\": vocab_size,\n",
        "    \"context_length\": 1024,\n",
        "    \"emb_dim\": 768,\n",
        "    \"n_heads\": 12,\n",
        "    \"n_layers\": 12,\n",
        "    \"drop_rate\": 0.3,\n",
        "    \"qkv_bias\": False,\n",
        "}\n",
        "\n",
        "# === Dataset ===\n",
        "train_ratio = 0.9\n",
        "split = int(train_ratio * len(clean_text))\n",
        "train_data, val_data = clean_text[:split], clean_text[split:]\n",
        "\n",
        "train_loader = create_dataloader_v1(train_data, batch_size=2, max_length=GPT_CONFIG[\"context_length\"], stride=GPT_CONFIG[\"context_length\"], tokenizer=tokenizer_v2)\n",
        "val_loader   = create_dataloader_v1(val_data,   batch_size=2, max_length=GPT_CONFIG[\"context_length\"], stride=GPT_CONFIG[\"context_length\"], shuffle=False, tokenizer=tokenizer_v2)\n",
        "\n",
        "# === Model + training ===\n",
        "model_regex = GPTModel(GPT_CONFIG).to(device)\n",
        "optimizer = torch.optim.AdamW(model_regex.parameters(), lr=1e-4, weight_decay=0.1)\n",
        "\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model_regex, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=5, eval_freq=10, eval_iter=20,\n",
        "    start_context=\"large language models are\", tokenizer=tokenizer_v2)\n",
        "\n",
        "torch.save(model_regex.state_dict(), \"models/regex_pretrained.pth\")\n",
        "epochs_tensor = torch.linspace(0, 5, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb96c885",
      "metadata": {},
      "source": [
        "### Step 6: Pretrain GPT model using the GPT-2 tokenizer\n",
        "Repeat the same pretraining pipeline, this time with the GPT-2 vocabulary and tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "5f937db1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep 1 (Step 000000): Train loss 10.659, Val loss 10.711\n",
            "Ep 1 (Step 000010): Train loss 9.135, Val loss 9.193\n",
            "Ep 1 (Step 000020): Train loss 8.563, Val loss 8.474\n",
            "Ep 1 (Step 000030): Train loss 7.811, Val loss 7.942\n",
            "Ep 1 (Step 000040): Train loss 7.533, Val loss 7.632\n",
            "Ep 1 (Step 000050): Train loss 7.393, Val loss 7.482\n",
            "Ep 1 (Step 000060): Train loss 7.103, Val loss 7.363\n",
            "Ep 1 (Step 000070): Train loss 6.914, Val loss 7.286\n",
            "Ep 1 (Step 000080): Train loss 6.916, Val loss 7.201\n",
            "Ep 1 (Step 000090): Train loss 6.862, Val loss 7.148\n",
            "Ep 1 (Step 000100): Train loss 6.875, Val loss 7.135\n",
            "Ep 1 (Step 000110): Train loss 6.879, Val loss 7.052\n",
            "Ep 1 (Step 000120): Train loss 6.499, Val loss 7.003\n",
            "Ep 1 (Step 000130): Train loss 6.682, Val loss 6.985\n",
            "Ep 1 (Step 000140): Train loss 6.513, Val loss 6.933\n",
            "Ep 1 (Step 000150): Train loss 6.297, Val loss 6.890\n",
            "Ep 1 (Step 000160): Train loss 6.559, Val loss 6.839\n",
            "Ep 1 (Step 000170): Train loss 6.435, Val loss 6.816\n",
            "Ep 1 (Step 000180): Train loss 6.380, Val loss 6.755\n",
            "Ep 1 (Step 000190): Train loss 6.137, Val loss 6.720\n",
            "Ep 1 (Step 000200): Train loss 6.376, Val loss 6.714\n",
            "Ep 1 (Step 000210): Train loss 6.499, Val loss 6.693\n",
            "Ep 1 (Step 000220): Train loss 6.086, Val loss 6.677\n",
            "Ep 1 (Step 000230): Train loss 6.350, Val loss 6.662\n",
            "Ep 1 (Step 000240): Train loss 6.135, Val loss 6.629\n",
            "large language models are a large-shot and the model to the model’s the model’s the model’s the model’s the model is a large-shot and the model is a model’s the model is a large-\n",
            "Ep 2 (Step 000250): Train loss 6.192, Val loss 6.610\n",
            "Ep 2 (Step 000260): Train loss 6.109, Val loss 6.585\n",
            "Ep 2 (Step 000270): Train loss 6.056, Val loss 6.568\n",
            "Ep 2 (Step 000280): Train loss 5.941, Val loss 6.559\n",
            "Ep 2 (Step 000290): Train loss 6.007, Val loss 6.536\n",
            "Ep 2 (Step 000300): Train loss 5.989, Val loss 6.498\n",
            "Ep 2 (Step 000310): Train loss 6.189, Val loss 6.479\n",
            "Ep 2 (Step 000320): Train loss 5.988, Val loss 6.474\n",
            "Ep 2 (Step 000330): Train loss 6.055, Val loss 6.471\n",
            "Ep 2 (Step 000340): Train loss 5.984, Val loss 6.424\n",
            "Ep 2 (Step 000350): Train loss 5.692, Val loss 6.421\n",
            "Ep 2 (Step 000360): Train loss 5.725, Val loss 6.417\n",
            "Ep 2 (Step 000370): Train loss 5.794, Val loss 6.388\n",
            "Ep 2 (Step 000380): Train loss 5.812, Val loss 6.392\n",
            "Ep 2 (Step 000390): Train loss 5.587, Val loss 6.377\n",
            "Ep 2 (Step 000400): Train loss 5.640, Val loss 6.350\n",
            "Ep 2 (Step 000410): Train loss 5.548, Val loss 6.352\n",
            "Ep 2 (Step 000420): Train loss 5.462, Val loss 6.357\n",
            "Ep 2 (Step 000430): Train loss 5.652, Val loss 6.314\n",
            "Ep 2 (Step 000440): Train loss 5.592, Val loss 6.322\n",
            "Ep 2 (Step 000450): Train loss 5.536, Val loss 6.319\n",
            "Ep 2 (Step 000460): Train loss 5.542, Val loss 6.287\n",
            "Ep 2 (Step 000470): Train loss 5.593, Val loss 6.291\n",
            "Ep 2 (Step 000480): Train loss 5.721, Val loss 6.276\n",
            "Ep 2 (Step 000490): Train loss 5.458, Val loss 6.273\n",
            "large language models are not the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the\n",
            "Ep 3 (Step 000500): Train loss 5.554, Val loss 6.266\n",
            "Ep 3 (Step 000510): Train loss 5.564, Val loss 6.240\n",
            "Ep 3 (Step 000520): Train loss 5.589, Val loss 6.243\n",
            "Ep 3 (Step 000530): Train loss 5.399, Val loss 6.258\n",
            "Ep 3 (Step 000540): Train loss 5.659, Val loss 6.249\n",
            "Ep 3 (Step 000550): Train loss 5.626, Val loss 6.214\n",
            "Ep 3 (Step 000560): Train loss 5.524, Val loss 6.241\n",
            "Ep 3 (Step 000570): Train loss 5.398, Val loss 6.185\n",
            "Ep 3 (Step 000580): Train loss 5.286, Val loss 6.182\n",
            "Ep 3 (Step 000590): Train loss 5.331, Val loss 6.150\n",
            "Ep 3 (Step 000600): Train loss 5.360, Val loss 6.178\n",
            "Ep 3 (Step 000610): Train loss 5.563, Val loss 6.148\n",
            "Ep 3 (Step 000620): Train loss 5.532, Val loss 6.159\n",
            "Ep 3 (Step 000630): Train loss 5.386, Val loss 6.112\n",
            "Ep 3 (Step 000640): Train loss 5.412, Val loss 6.108\n",
            "Ep 3 (Step 000650): Train loss 5.042, Val loss 6.094\n",
            "Ep 3 (Step 000660): Train loss 5.271, Val loss 6.119\n",
            "Ep 3 (Step 000670): Train loss 5.255, Val loss 6.079\n",
            "Ep 3 (Step 000680): Train loss 5.298, Val loss 6.082\n",
            "Ep 3 (Step 000690): Train loss 5.278, Val loss 6.084\n",
            "Ep 3 (Step 000700): Train loss 5.245, Val loss 6.085\n",
            "Ep 3 (Step 000710): Train loss 5.323, Val loss 6.095\n",
            "Ep 3 (Step 000720): Train loss 5.132, Val loss 6.067\n",
            "Ep 3 (Step 000730): Train loss 5.427, Val loss 6.018\n",
            "Ep 3 (Step 000740): Train loss 5.112, Val loss 6.032\n",
            "large language models are a model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model to the model\n",
            "Ep 4 (Step 000750): Train loss 5.334, Val loss 6.053\n",
            "Ep 4 (Step 000760): Train loss 5.168, Val loss 6.028\n",
            "Ep 4 (Step 000770): Train loss 5.145, Val loss 6.049\n",
            "Ep 4 (Step 000780): Train loss 5.272, Val loss 6.036\n",
            "Ep 4 (Step 000790): Train loss 5.142, Val loss 6.024\n",
            "Ep 4 (Step 000800): Train loss 5.323, Val loss 6.008\n",
            "Ep 4 (Step 000810): Train loss 5.344, Val loss 6.016\n",
            "Ep 4 (Step 000820): Train loss 5.327, Val loss 6.034\n",
            "Ep 4 (Step 000830): Train loss 5.023, Val loss 6.033\n",
            "Ep 4 (Step 000840): Train loss 5.213, Val loss 6.042\n",
            "Ep 4 (Step 000850): Train loss 5.138, Val loss 5.993\n",
            "Ep 4 (Step 000860): Train loss 5.220, Val loss 5.997\n",
            "Ep 4 (Step 000870): Train loss 5.082, Val loss 5.984\n",
            "Ep 4 (Step 000880): Train loss 5.012, Val loss 5.964\n",
            "Ep 4 (Step 000890): Train loss 4.820, Val loss 5.971\n",
            "Ep 4 (Step 000900): Train loss 5.176, Val loss 5.958\n",
            "Ep 4 (Step 000910): Train loss 5.087, Val loss 5.957\n",
            "Ep 4 (Step 000920): Train loss 5.144, Val loss 5.944\n",
            "Ep 4 (Step 000930): Train loss 4.921, Val loss 5.921\n",
            "Ep 4 (Step 000940): Train loss 5.019, Val loss 5.920\n",
            "Ep 4 (Step 000950): Train loss 5.091, Val loss 5.934\n",
            "Ep 4 (Step 000960): Train loss 4.846, Val loss 5.913\n",
            "Ep 4 (Step 000970): Train loss 4.957, Val loss 5.920\n",
            "Ep 4 (Step 000980): Train loss 4.942, Val loss 5.918\n",
            "Ep 4 (Step 000990): Train loss 4.774, Val loss 5.910\n",
            "large language models are shown in the model. We also the model. We also the model. We evaluate the model to the model. We also the model to the model. In this task. In this task. In this section,\n",
            "Ep 5 (Step 001000): Train loss 4.800, Val loss 5.918\n",
            "Ep 5 (Step 001010): Train loss 4.928, Val loss 5.928\n",
            "Ep 5 (Step 001020): Train loss 4.899, Val loss 5.924\n",
            "Ep 5 (Step 001030): Train loss 4.732, Val loss 5.947\n",
            "Ep 5 (Step 001040): Train loss 4.935, Val loss 5.919\n",
            "Ep 5 (Step 001050): Train loss 5.075, Val loss 5.933\n",
            "Ep 5 (Step 001060): Train loss 5.087, Val loss 5.917\n",
            "Ep 5 (Step 001070): Train loss 4.942, Val loss 5.908\n",
            "Ep 5 (Step 001080): Train loss 5.015, Val loss 5.893\n",
            "Ep 5 (Step 001090): Train loss 4.954, Val loss 5.895\n",
            "Ep 5 (Step 001100): Train loss 4.944, Val loss 5.910\n",
            "Ep 5 (Step 001110): Train loss 4.761, Val loss 5.880\n",
            "Ep 5 (Step 001120): Train loss 4.762, Val loss 5.875\n",
            "Ep 5 (Step 001130): Train loss 4.797, Val loss 5.884\n",
            "Ep 5 (Step 001140): Train loss 4.951, Val loss 5.874\n",
            "Ep 5 (Step 001150): Train loss 4.783, Val loss 5.854\n",
            "Ep 5 (Step 001160): Train loss 4.762, Val loss 5.860\n",
            "Ep 5 (Step 001170): Train loss 4.749, Val loss 5.856\n",
            "Ep 5 (Step 001180): Train loss 4.725, Val loss 5.853\n",
            "Ep 5 (Step 001190): Train loss 4.716, Val loss 5.870\n",
            "Ep 5 (Step 001200): Train loss 4.826, Val loss 5.850\n",
            "Ep 5 (Step 001210): Train loss 4.776, Val loss 5.856\n",
            "Ep 5 (Step 001220): Train loss 4.604, Val loss 5.839\n",
            "Ep 5 (Step 001230): Train loss 4.704, Val loss 5.833\n",
            "large language models are shown in the model to the model to the model to the model to the model to the model to the model to the model, and the model to the model to the model to the model to the model to the model to the model to the model\n",
            "Ep 6 (Step 001240): Train loss 4.684, Val loss 5.834\n",
            "Ep 6 (Step 001250): Train loss 4.729, Val loss 5.840\n",
            "Ep 6 (Step 001260): Train loss 4.648, Val loss 5.868\n",
            "Ep 6 (Step 001270): Train loss 4.647, Val loss 5.846\n",
            "Ep 6 (Step 001280): Train loss 4.655, Val loss 5.829\n",
            "Ep 6 (Step 001290): Train loss 4.671, Val loss 5.838\n",
            "Ep 6 (Step 001300): Train loss 4.543, Val loss 5.817\n",
            "Ep 6 (Step 001310): Train loss 4.641, Val loss 5.834\n",
            "Ep 6 (Step 001320): Train loss 4.631, Val loss 5.829\n",
            "Ep 6 (Step 001330): Train loss 4.574, Val loss 5.821\n",
            "Ep 6 (Step 001340): Train loss 4.717, Val loss 5.821\n",
            "Ep 6 (Step 001350): Train loss 4.703, Val loss 5.793\n",
            "Ep 6 (Step 001360): Train loss 4.668, Val loss 5.802\n",
            "Ep 6 (Step 001370): Train loss 4.587, Val loss 5.815\n",
            "Ep 6 (Step 001380): Train loss 4.665, Val loss 5.835\n",
            "Ep 6 (Step 001390): Train loss 4.439, Val loss 5.811\n",
            "Ep 6 (Step 001400): Train loss 4.643, Val loss 5.823\n",
            "Ep 6 (Step 001410): Train loss 4.551, Val loss 5.805\n",
            "Ep 6 (Step 001420): Train loss 4.586, Val loss 5.806\n",
            "Ep 6 (Step 001430): Train loss 4.542, Val loss 5.794\n",
            "Ep 6 (Step 001440): Train loss 4.488, Val loss 5.808\n",
            "Ep 6 (Step 001450): Train loss 4.455, Val loss 5.795\n",
            "Ep 6 (Step 001460): Train loss 4.571, Val loss 5.802\n",
            "Ep 6 (Step 001470): Train loss 4.534, Val loss 5.777\n",
            "Ep 6 (Step 001480): Train loss 4.507, Val loss 5.777\n",
            "large language models are shown in the model, and the model, and the model, and the model, and the model, and the model, and the model, and the model, and the model, and the model, and the model, and the model, and\n",
            "Ep 7 (Step 001490): Train loss 4.425, Val loss 5.785\n",
            "Ep 7 (Step 001500): Train loss 4.403, Val loss 5.781\n",
            "Ep 7 (Step 001510): Train loss 4.329, Val loss 5.786\n",
            "Ep 7 (Step 001520): Train loss 4.478, Val loss 5.776\n",
            "Ep 7 (Step 001530): Train loss 4.404, Val loss 5.777\n",
            "Ep 7 (Step 001540): Train loss 4.524, Val loss 5.784\n",
            "Ep 7 (Step 001550): Train loss 4.488, Val loss 5.796\n",
            "Ep 7 (Step 001560): Train loss 4.467, Val loss 5.795\n",
            "Ep 7 (Step 001570): Train loss 4.486, Val loss 5.790\n",
            "Ep 7 (Step 001580): Train loss 4.255, Val loss 5.759\n",
            "Ep 7 (Step 001590): Train loss 4.205, Val loss 5.754\n",
            "Ep 7 (Step 001600): Train loss 4.334, Val loss 5.777\n",
            "Ep 7 (Step 001610): Train loss 4.289, Val loss 5.762\n",
            "Ep 7 (Step 001620): Train loss 4.278, Val loss 5.776\n",
            "Ep 7 (Step 001630): Train loss 4.476, Val loss 5.774\n",
            "Ep 7 (Step 001640): Train loss 4.312, Val loss 5.768\n",
            "Ep 7 (Step 001650): Train loss 4.447, Val loss 5.757\n",
            "Ep 7 (Step 001660): Train loss 4.523, Val loss 5.777\n",
            "Ep 7 (Step 001670): Train loss 4.524, Val loss 5.752\n",
            "Ep 7 (Step 001680): Train loss 4.533, Val loss 5.771\n",
            "Ep 7 (Step 001690): Train loss 4.241, Val loss 5.758\n",
            "Ep 7 (Step 001700): Train loss 4.309, Val loss 5.732\n",
            "Ep 7 (Step 001710): Train loss 4.252, Val loss 5.734\n",
            "Ep 7 (Step 001720): Train loss 4.143, Val loss 5.750\n",
            "Ep 7 (Step 001730): Train loss 4.322, Val loss 5.726\n",
            "large language models are shown in the model’s ability to the model’s. We also evaluate the model’s in the model’s in the model’s in the model’s in the model’s in the\n",
            "Ep 8 (Step 001740): Train loss 4.390, Val loss 5.756\n",
            "Ep 8 (Step 001750): Train loss 4.209, Val loss 5.728\n",
            "Ep 8 (Step 001760): Train loss 4.330, Val loss 5.788\n",
            "Ep 8 (Step 001770): Train loss 4.459, Val loss 5.786\n",
            "Ep 8 (Step 001780): Train loss 4.329, Val loss 5.760\n",
            "Ep 8 (Step 001790): Train loss 4.218, Val loss 5.743\n",
            "Ep 8 (Step 001800): Train loss 4.229, Val loss 5.771\n",
            "Ep 8 (Step 001810): Train loss 4.287, Val loss 5.734\n",
            "Ep 8 (Step 001820): Train loss 4.300, Val loss 5.751\n",
            "Ep 8 (Step 001830): Train loss 4.133, Val loss 5.749\n",
            "Ep 8 (Step 001840): Train loss 4.274, Val loss 5.737\n",
            "Ep 8 (Step 001850): Train loss 4.101, Val loss 5.748\n",
            "Ep 8 (Step 001860): Train loss 4.184, Val loss 5.748\n",
            "Ep 8 (Step 001870): Train loss 4.135, Val loss 5.743\n",
            "Ep 8 (Step 001880): Train loss 4.124, Val loss 5.725\n",
            "Ep 8 (Step 001890): Train loss 4.161, Val loss 5.746\n",
            "Ep 8 (Step 001900): Train loss 4.292, Val loss 5.745\n",
            "Ep 8 (Step 001910): Train loss 4.313, Val loss 5.729\n",
            "Ep 8 (Step 001920): Train loss 4.078, Val loss 5.770\n",
            "Ep 8 (Step 001930): Train loss 4.196, Val loss 5.738\n",
            "Ep 8 (Step 001940): Train loss 4.116, Val loss 5.741\n",
            "Ep 8 (Step 001950): Train loss 4.232, Val loss 5.712\n",
            "Ep 8 (Step 001960): Train loss 4.125, Val loss 5.743\n",
            "Ep 8 (Step 001970): Train loss 4.261, Val loss 5.735\n",
            "Ep 8 (Step 001980): Train loss 4.153, Val loss 5.722\n",
            "large language models are shown in the model. We evaluate the model to the model to the model. We evaluate the model to the model. We evaluate the model to the model. We evaluate the model to the model to the model to the model.\n",
            "Ep 9 (Step 001990): Train loss 4.017, Val loss 5.732\n",
            "Ep 9 (Step 002000): Train loss 4.162, Val loss 5.731\n",
            "Ep 9 (Step 002010): Train loss 4.130, Val loss 5.715\n",
            "Ep 9 (Step 002020): Train loss 4.116, Val loss 5.742\n",
            "Ep 9 (Step 002030): Train loss 4.068, Val loss 5.735\n",
            "Ep 9 (Step 002040): Train loss 4.056, Val loss 5.729\n",
            "Ep 9 (Step 002050): Train loss 4.122, Val loss 5.705\n",
            "Ep 9 (Step 002060): Train loss 4.113, Val loss 5.723\n",
            "Ep 9 (Step 002070): Train loss 4.077, Val loss 5.714\n",
            "Ep 9 (Step 002080): Train loss 4.031, Val loss 5.717\n",
            "Ep 9 (Step 002090): Train loss 4.057, Val loss 5.718\n",
            "Ep 9 (Step 002100): Train loss 3.938, Val loss 5.718\n",
            "Ep 9 (Step 002110): Train loss 3.900, Val loss 5.712\n",
            "Ep 9 (Step 002120): Train loss 4.015, Val loss 5.717\n",
            "Ep 9 (Step 002130): Train loss 3.960, Val loss 5.739\n",
            "Ep 9 (Step 002140): Train loss 4.086, Val loss 5.716\n",
            "Ep 9 (Step 002150): Train loss 3.971, Val loss 5.723\n",
            "Ep 9 (Step 002160): Train loss 3.965, Val loss 5.712\n",
            "Ep 9 (Step 002170): Train loss 3.958, Val loss 5.711\n",
            "Ep 9 (Step 002180): Train loss 4.154, Val loss 5.746\n",
            "Ep 9 (Step 002190): Train loss 3.880, Val loss 5.735\n",
            "Ep 9 (Step 002200): Train loss 3.903, Val loss 5.700\n",
            "Ep 9 (Step 002210): Train loss 3.971, Val loss 5.709\n",
            "Ep 9 (Step 002220): Train loss 3.918, Val loss 5.733\n",
            "Ep 9 (Step 002230): Train loss 3.947, Val loss 5.718\n",
            "large language models are shown in the model is a large language models. We evaluate our models. We evaluate our models. We evaluate our models on a model to the model to improve model to the model to the model to the model to the model to the\n",
            "Ep 10 (Step 002240): Train loss 3.780, Val loss 5.714\n",
            "Ep 10 (Step 002250): Train loss 3.931, Val loss 5.703\n",
            "Ep 10 (Step 002260): Train loss 4.067, Val loss 5.740\n",
            "Ep 10 (Step 002270): Train loss 3.924, Val loss 5.717\n",
            "Ep 10 (Step 002280): Train loss 3.950, Val loss 5.739\n",
            "Ep 10 (Step 002290): Train loss 4.019, Val loss 5.727\n",
            "Ep 10 (Step 002300): Train loss 3.943, Val loss 5.724\n",
            "Ep 10 (Step 002310): Train loss 3.877, Val loss 5.716\n",
            "Ep 10 (Step 002320): Train loss 3.929, Val loss 5.737\n",
            "Ep 10 (Step 002330): Train loss 4.017, Val loss 5.750\n",
            "Ep 10 (Step 002340): Train loss 3.876, Val loss 5.735\n",
            "Ep 10 (Step 002350): Train loss 3.885, Val loss 5.711\n",
            "Ep 10 (Step 002360): Train loss 3.797, Val loss 5.733\n",
            "Ep 10 (Step 002370): Train loss 3.868, Val loss 5.715\n",
            "Ep 10 (Step 002380): Train loss 3.934, Val loss 5.703\n",
            "Ep 10 (Step 002390): Train loss 3.797, Val loss 5.722\n",
            "Ep 10 (Step 002400): Train loss 4.012, Val loss 5.729\n",
            "Ep 10 (Step 002410): Train loss 3.741, Val loss 5.710\n",
            "Ep 10 (Step 002420): Train loss 3.750, Val loss 5.719\n",
            "Ep 10 (Step 002430): Train loss 3.784, Val loss 5.725\n",
            "Ep 10 (Step 002440): Train loss 3.745, Val loss 5.700\n",
            "Ep 10 (Step 002450): Train loss 3.727, Val loss 5.726\n",
            "Ep 10 (Step 002460): Train loss 3.899, Val loss 5.737\n",
            "Ep 10 (Step 002470): Train loss 3.865, Val loss 5.736\n",
            "large language models are not have a large language models. Gemini 1.5 Pro model performance on the model performance on the model, we have a model performance on the model, and Gemini models, and Gemini models, and Gemini 1.5 Pro on Gemini\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "gpt2 = True\n",
        "tokenizer_gpt2 = tiktoken.get_encoding(\"gpt2\")\n",
        "vocab_size = tokenizer_gpt2.n_vocab\n",
        "GPT_CONFIG[\"vocab_size\"] = vocab_size\n",
        "\n",
        "# Recreate model & loaders (same data)\n",
        "train_loader = create_dataloader_v1(train_data, batch_size=2, max_length=GPT_CONFIG[\"context_length\"], stride=GPT_CONFIG[\"context_length\"], tokenizer=tokenizer_gpt2)\n",
        "val_loader   = create_dataloader_v1(val_data,   batch_size=2, max_length=GPT_CONFIG[\"context_length\"], stride=GPT_CONFIG[\"context_length\"], shuffle=False, tokenizer=tokenizer_gpt2)\n",
        "\n",
        "model_gpt2 = GPTModel(GPT_CONFIG).to(device)\n",
        "optimizer = torch.optim.AdamW(model_gpt2.parameters(), lr=1e-4, weight_decay=0.1)\n",
        "\n",
        "train_losses_gpt2, val_losses_gpt2, tokens_seen_gpt2 = train_model_simple(\n",
        "    model_gpt2, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=10, eval_freq=10, eval_iter=20,\n",
        "    start_context=\"large language models are\", tokenizer=tokenizer_gpt2)\n",
        "\n",
        "torch.save(model_gpt2.state_dict(), \"models/gpt2_pretrained.pth\")\n",
        "epochs_tensor = torch.linspace(0, 10, len(train_losses_gpt2))\n",
        "plot_losses(epochs_tensor, tokens_seen_gpt2, train_losses_gpt2, val_losses_gpt2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd024798",
      "metadata": {},
      "source": [
        "### Step 7: Load the pretrained Regex and GPT-2 models from saved checkpoints\n",
        "\n",
        "This step restores the previously trained models (`gpt2_pretrained.pth` and `regex_pretrained.pth`) so they can be used later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "abe81339",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🟦 Regex tokenizer model loaded.\n",
            "🟨 GPT-2 tokenizer model loaded.\n"
          ]
        }
      ],
      "source": [
        "GPT_CONFIG = {\n",
        "    \"vocab_size\": 1000,  # placeholder, will be updated\n",
        "    \"context_length\": 1024,\n",
        "    \"emb_dim\": 768,\n",
        "    \"n_heads\": 12,\n",
        "    \"n_layers\": 12,\n",
        "    \"drop_rate\": 0.3,\n",
        "    \"qkv_bias\": False}\n",
        "\n",
        "# ---- Load Regex tokenizer model ----\n",
        "gpt2 = False\n",
        "tokenizer_v2 = SimpleTokenizerV2(vocab)\n",
        "vocab_size = len(tokenizer_v2.tokens2ids)\n",
        "GPT_CONFIG[\"vocab_size\"] = vocab_size\n",
        "\n",
        "model_regex = GPTModel(GPT_CONFIG).to(device)\n",
        "model_regex.load_state_dict(torch.load(\"models/regex_pretrained.pth\", map_location=device))\n",
        "model_regex.eval()\n",
        "print(\"🟦 Regex tokenizer model loaded.\")\n",
        "\n",
        "# ---- Load GPT-2 tokenizer model ----\n",
        "gpt2 = True\n",
        "tokenizer_gpt2 = tiktoken.get_encoding(\"gpt2\")\n",
        "vocab_size = tokenizer_gpt2.n_vocab\n",
        "GPT_CONFIG[\"vocab_size\"] = vocab_size\n",
        "\n",
        "model_gpt2 = GPTModel(GPT_CONFIG).to(device)\n",
        "model_gpt2.load_state_dict(torch.load(\"models/gpt2_pretrained.pth\", map_location=device))\n",
        "model_gpt2.eval()\n",
        "print(\"🟨 GPT-2 tokenizer model loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26a45134",
      "metadata": {},
      "source": [
        "### Step 8: Generate sample text using both trained models (Regex & GPT-2)\n",
        "\n",
        "Here we use the same `generate()` function to compare how both models (regex-tokenizer and GPT-2 tokenizer) generate text.  \n",
        " - Adjust `top_k` and `temperature` for creativity control."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "cc9770ce",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🟦 Regex tokenizer model output:\n",
            " large language models are reported in a small, and training of large language models have been expected for all results were crucial and reasoning in the model in Figure 2. • We report a single dataset. We have significantly that is an efficient data that are presented in an internal the\n",
            "\n",
            "🟨 GPT-2 tokenizer model output:\n",
            " large language models are the performance of the original previous work, where it has been significantly better than 2-attention-3 achieves strong results on the models with a large margin, and human written, GPT-3 demonstrates that of two tasks like GPT-2\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "start_text = \"large language models are\"\n",
        "\n",
        "# === Regex tokenizer model ===\n",
        "gpt2 = False\n",
        "vocab_size = len(tokenizer_v2.tokens2ids)\n",
        "GPT_CONFIG[\"vocab_size\"] = vocab_size\n",
        "token_ids = generate(\n",
        "    model=model_regex,\n",
        "    idx=text_to_token_ids(start_text, tokenizer_v2).to(device),\n",
        "    max_new_tokens=50,\n",
        "    context_size=GPT_CONFIG[\"context_length\"],\n",
        "    top_k=25,\n",
        "    temperature=1.4)\n",
        "print(\"🟦 Regex tokenizer model output:\\n\", token_ids_to_text(token_ids, tokenizer_v2))\n",
        "\n",
        "# === GPT-2 tokenizer model ===\n",
        "gpt2 = True\n",
        "vocab_size = tokenizer_gpt2.n_vocab\n",
        "GPT_CONFIG[\"vocab_size\"] = vocab_size\n",
        "token_ids = generate(\n",
        "    model=model_gpt2,\n",
        "    idx=text_to_token_ids(start_text, tokenizer_gpt2).to(device),\n",
        "    max_new_tokens=50,\n",
        "    context_size=GPT_CONFIG[\"context_length\"],\n",
        "    top_k=25,\n",
        "    temperature=1.4)\n",
        "print(\"\\n🟨 GPT-2 tokenizer model output:\\n\", token_ids_to_text(token_ids, tokenizer_gpt2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e60626fa",
      "metadata": {},
      "source": [
        "## Part 2: Instruction–Response Pair Generation\n",
        "\n",
        "Here we automates the creation of an instruction-following dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f60d123",
      "metadata": {},
      "source": [
        "### Step 1: Split Documents text into Subsections \n",
        "\n",
        " - Prepare Source Documents: We split firstly the text into document units.\n",
        " - The cleaned text already uses (`=` * 80) as a delimiter.  \n",
        "\n",
        "Each document may contain multiple paragraphs or sections, and using only full documents limits the number of possible instruction–response pairs.  \n",
        "\n",
        "The `split_into_subsections()` function increases dataset richness by dividing each document into smaller **subsections** based on the following criteria:\n",
        "\n",
        "- **Splitting rule:** It separates text wherever there is **one or more newline characters (`\\n`)**.  \n",
        "- **Filtering rule:** It keeps only those subsections containing **at least 80 words** (`min_words=80`) to ensure that each segment is substantial enough."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "b22b5562",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detected 37 documents.\n",
            "Total sections: 1618\n"
          ]
        }
      ],
      "source": [
        "def split_into_subsections(docs, min_words=80): \n",
        "    # Previous configuration: min_words=117 was choosen manually to have = 2028 pairs\n",
        "    # New configuration: min_words=80 to have more sections for training = 4840 pairs\n",
        "    sections = []\n",
        "    for d in docs:\n",
        "        parts = re.split(r'\\n{1,}', d)\n",
        "        for p in parts:\n",
        "            if len(p.split()) >= min_words:\n",
        "                sections.append(p.strip())\n",
        "    return sections\n",
        "\n",
        "docs = [d.strip() for d in clean_text.split(\"=\" * 80) if d.strip()]\n",
        "print(f\"Detected {len(docs)} documents.\")\n",
        "\n",
        "sections = split_into_subsections(docs)\n",
        "print(f\"Total sections: {len(sections)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e330447d",
      "metadata": {},
      "source": [
        "### Step 2: Loads a pretrained instruction-tuned model\n",
        "   - Uses the free and open-source **`Qwen/Qwen2.5-7B-Instruct`** model from Hugging Face.  \n",
        "   - Automatically initializes a text-generation pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "078f6d3d",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu126 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "869cfc640c5b4a49af706326b24631f9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Device set to use cuda:2\n"
          ]
        }
      ],
      "source": [
        "import re, random, json\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# ===== Load Mistral-7B-Instruct model =====\n",
        "# model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "device = \"cuda:2\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token  # prevent padding warnings\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(\"cuda:2\")\n",
        "\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=2,  # explicitly pick cuda:2\n",
        "    torch_dtype=torch.float16,\n",
        "    max_new_tokens=256,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    batch_size=12)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0d67bf4",
      "metadata": {},
      "source": [
        "### Step 3: Creates multiple types of instruction–input pairs per paragraph\n",
        "  - Each paragraph is lightly cleaned and truncated to stay within the **1024-token context window**.\n",
        "  - For each text chunk, three types of tasks are created:\n",
        "     - **Summarization** — e.g., “Summarize the following passage in 1–3 sentences.”\n",
        "     - **Explanation** — e.g., “Explain the main mechanism or method described.”\n",
        "     - **Q&A** — e.g., “What is [Key Term] in this context?” (automatically picks a capitalized term from the text).\n",
        "   - Each instruction and paragraph are wrapped in **Alpaca-style format**:\n",
        "     ```\n",
        "     Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "     ### Instruction:\n",
        "     [Instruction text]\n",
        "\n",
        "     ### Input:\n",
        "     [Paragraph text]\n",
        "\n",
        "     ### Response:\n",
        "     ```\n",
        "\n",
        "### Step 4: Generates real model responses automatically\n",
        "   - The Mistral model reads each formatted prompt and writes an appropriate answer.  \n",
        "   - The generated response is extracted from the output and stored as `\"Response\"`.\n",
        "\n",
        "### Step 5: Assembles and saves a full JSON dataset\n",
        "   - The result is a list of dictionaries:\n",
        "     ```json\n",
        "     {\n",
        "       \"Instruction\": \"...\",\n",
        "       \"Input\": \"...\",\n",
        "       \"Response\": \"...\"\n",
        "     }\n",
        "     ```\n",
        "   - Saved to `instruction_data_from_pdfs.json`, ready for supervised fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating instruction–response pairs:   1%|          | 28/4860 [01:24<3:44:46,  2.79s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "Generating instruction–response pairs: 100%|█████████▉| 4840/4860 [3:20:16<00:49,  2.48s/it]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated 4840 instruction–response pairs.\n",
            "Saved → data\\instruction_data_from_pdfs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ===== Generate instruction–response pairs =====\n",
        "def make_pairs_from_paragraphs(paragraphs, target=2000, seed=123, max_chars=1500):\n",
        "    random.seed(seed)\n",
        "    random.shuffle(paragraphs)\n",
        "\n",
        "    pairs = []\n",
        "    progress = tqdm(total=target, desc=\"Generating instruction–response pairs\")\n",
        "\n",
        "    for p in paragraphs:\n",
        "        text = re.sub(r\"\\s+\", \" \", p).strip()\n",
        "        if len(text.split()) < 40:\n",
        "            continue\n",
        "\n",
        "        short_text = text[:max_chars]\n",
        "\n",
        "        # Define 3 instruction types\n",
        "        tasks = [\n",
        "            (\"Summarize the following passage in 1–3 sentences.\", short_text),\n",
        "            (\"Explain the main mechanism or method described.\", short_text)\n",
        "        ]\n",
        "\n",
        "        # Try to create a context-specific Q&A\n",
        "        tokens = re.findall(r\"[A-Za-z][A-Za-z\\-]+\", text)\n",
        "        topic = next((t for t in tokens if t[0].isupper() and len(t) > 3), None)\n",
        "        if topic:\n",
        "            tasks.append((f\"What is {topic} in this context?\", short_text))\n",
        "\n",
        "        # Batch prompts for efficiency\n",
        "        prompts = [\n",
        "            f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n\"\n",
        "            f\"### Instruction:\\n{inst}\\n\\n### Input:\\n{inp}\\n\\n### Response:\\n\"\n",
        "            for inst, inp in tasks\n",
        "        ]\n",
        "\n",
        "        outputs = generator(prompts)\n",
        "\n",
        "        for prompt, out in zip(prompts, outputs):\n",
        "            response = out[0][\"generated_text\"].split(\"### Response:\")[-1].strip()\n",
        "            instruction = re.search(r\"### Instruction:\\n(.*?)\\n\\n### Input:\", prompt, re.S).group(1)\n",
        "            input_text = re.search(r\"### Input:\\n(.*)\\n\\n### Response:\", prompt, re.S).group(1)\n",
        "\n",
        "            pairs.append({\n",
        "                \"Instruction\": instruction,\n",
        "                \"Input\": input_text,\n",
        "                \"Response\": response\n",
        "            })\n",
        "\n",
        "            progress.update(1)\n",
        "            if len(pairs) >= target:\n",
        "                progress.close()\n",
        "                return pairs[:target]\n",
        "\n",
        "    progress.close()\n",
        "    return pairs[:target]\n",
        "\n",
        "# debug_subset = sections[:15]  # use only first 15 sections\n",
        "# pairs = make_pairs_from_paragraphs(debug_subset, target=10)\n",
        "\n",
        "target0 = 4860 \n",
        "# In a previous configuration, I set min_words=117 to get exactly 2028 pairs. \n",
        "# Now with min_words=80, I can get 4860 pairs.\n",
        "\n",
        "pairs = make_pairs_from_paragraphs(sections, target=target0)\n",
        "print(f\"Generated {len(pairs)} instruction–response pairs.\")\n",
        "\n",
        "# ===== Save =====\n",
        "with open(\"responces/instruction_data_from_pdfs.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(pairs, f, ensure_ascii=False, indent=2)\n",
        "print(\"Saved → responces/instruction_data_from_pdfs.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ee2dd33",
      "metadata": {},
      "source": [
        "### Step 6: Load the instruction dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0aa6302e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 4840 instruction–response pairs.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "with open(\"responces/instruction_data_from_pdfs.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "print(f\"Loaded {len(data)} instruction–response pairs.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecf6d6db",
      "metadata": {},
      "source": [
        "### Step 7: Performs a Train/Validation split\n",
        "   - Randomly shuffles and splits the dataset into:\n",
        "     - *4340 training samples*\n",
        "     - *500 validation samples*\n",
        "  - In a previous configuration, I made *1528 training samples* and *500 validation samples*, and the performance was less compared to this new configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "718ae127",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train: 4340 | val: 500 | total: 4840\n"
          ]
        }
      ],
      "source": [
        "pairs = data.copy()\n",
        "random.shuffle(pairs)\n",
        "val_set = pairs[:500]\n",
        "train_set = pairs[500:]\n",
        "\n",
        "print(f\"train: {len(train_set)} | val: {len(val_set)} | total: {len(pairs)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cba3f46d",
      "metadata": {},
      "source": [
        "## Part 3 — Fine-Tuning The Pretrained Models\n",
        "Here we fine-tune both pretrained models (Regex and GPT-2 versions) on the generated instruction dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5136f12",
      "metadata": {},
      "source": [
        "### Step 1: Prepare datasets and dataloaders\n",
        "\n",
        "- `InstructionDataset()` and `format_input()` are used to prepare the dataset for instruction fine-tuning.  \n",
        "    - They convert each JSON entry (with keys `\"Instruction\"`, `\"Input\"`, and `\"Response\"`) into an **Alpaca-style prompt–response format** suitable for LLM models.\n",
        "    - **`format_input()`:** Formats each record into the Alpaca-style text.\n",
        "    - **`InstructionDataset()`:** loads the pre-generated *Instruction–Input–Response* triplets and tokenizes them into input–target pairs.\n",
        "\n",
        "- We use **`custom_collate_fn()`** to pad and align input–target sequences within each batch.  \n",
        "    - Padding tokens (`<|endoftext|>`) are ignored in the loss function using `ignore_index = -100`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "3aeaf59f",
      "metadata": {},
      "outputs": [],
      "source": [
        "class InstructionDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.data = data\n",
        "        self.encoded_texts = []\n",
        "        for entry in data:\n",
        "            instruction_plus_input = format_input(entry)\n",
        "            response_text = f\"\\n\\n### Response:\\n{entry['Response']}\"\n",
        "            full_text = instruction_plus_input + response_text\n",
        "            self.encoded_texts.append(tokenizer.encode(full_text))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.encoded_texts[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "def format_input(entry):\n",
        "    instruction_text = (\n",
        "        f\"Below is an instruction that describes a task. \"\n",
        "        f\"Write a response that appropriately completes the request.\"\n",
        "        f\"\\n\\n### Instruction:\\n{entry['Instruction']}\"\n",
        "    )\n",
        "    input_text = f\"\\n\\n### Input:\\n{entry['Input']}\" if entry[\"Input\"] else \"\"\n",
        "    return instruction_text + input_text\n",
        "\n",
        "def custom_collate_fn(batch, pad_token_id, ignore_index=-100, allowed_max_length=None, device=\"cpu\"):\n",
        "    # Find the longest sequence in the batch\n",
        "    batch_max_length = max(len(item)+1 for item in batch)\n",
        "    inputs_lst, targets_lst = [], []\n",
        "    for item in batch:\n",
        "        new_item = item.copy()\n",
        "        # Add an <|endoftext|> token\n",
        "        new_item += [pad_token_id]\n",
        "        # Pad sequences to max_length\n",
        "        padded = (\n",
        "            new_item + [pad_token_id] *\n",
        "            (batch_max_length - len(new_item)))\n",
        "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
        "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
        "        # New: Replace all but the first padding tokens in targets by ignore_index\n",
        "        mask = targets == pad_token_id\n",
        "        indices = torch.nonzero(mask).squeeze()\n",
        "        if indices.numel() > 1:\n",
        "            targets[indices[1:]] = ignore_index\n",
        "\n",
        "        # New: Optionally truncate to maximum sequence length\n",
        "        if allowed_max_length is not None:\n",
        "            inputs = inputs[:allowed_max_length]\n",
        "            targets = targets[:allowed_max_length]\n",
        "\n",
        "        inputs_lst.append(inputs)\n",
        "        targets_lst.append(targets)\n",
        "\n",
        "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
        "    targets_tensor = torch.stack(targets_lst).to(device)\n",
        "    return inputs_tensor, targets_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "6cde3f9e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🟦 Regex loaders ready.\n",
            "🟨 GPT-2 loaders ready.\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from functools import partial\n",
        "\n",
        "# === Regex tokenizer model ===\n",
        "regex_pad_token = \"<|endoftext|>\" # \"<|endoftext|>\" is the second last token.\n",
        "regex_pad_id = tokenizer_v2.tokens2ids.get(regex_pad_token, len(tokenizer_v2.tokens2ids) - 1)\n",
        "customized_collate_regex = partial(custom_collate_fn, pad_token_id=regex_pad_id, device=device, allowed_max_length=1024)\n",
        "\n",
        "train_dataset_regex = InstructionDataset(train_set, tokenizer_v2)\n",
        "val_dataset_regex = InstructionDataset(val_set, tokenizer_v2)\n",
        "\n",
        "train_loader_regex = DataLoader(train_dataset_regex, batch_size=8, collate_fn=customized_collate_regex, shuffle=True)\n",
        "val_loader_regex = DataLoader(val_dataset_regex, batch_size=8, collate_fn=customized_collate_regex, shuffle=False)\n",
        "print(\"🟦 Regex loaders ready.\")\n",
        "\n",
        "# === GPT-2 tokenizer model ===\n",
        "gpt2_pad_id = 50256  # <|endoftext|> in GPT-2\n",
        "customized_collate_gpt2 = partial(custom_collate_fn, pad_token_id=gpt2_pad_id, device=device, allowed_max_length=1024)\n",
        "\n",
        "train_dataset_gpt2 = InstructionDataset(train_set, tokenizer_gpt2)\n",
        "val_dataset_gpt2 = InstructionDataset(val_set, tokenizer_gpt2)\n",
        "\n",
        "train_loader_gpt2 = DataLoader(train_dataset_gpt2, batch_size=8, collate_fn=customized_collate_gpt2, shuffle=True)\n",
        "val_loader_gpt2 = DataLoader(val_dataset_gpt2, batch_size=8, collate_fn=customized_collate_gpt2, shuffle=False)\n",
        "print(\"🟨 GPT-2 loaders ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3a2a968",
      "metadata": {},
      "source": [
        "### Step 2: Fine-Tuning the Regex Model\n",
        "\n",
        "- Saves the fine-tuned checkpoint as **`regex_finetuned.pth`**.  \n",
        "- Plots both **training** and **validation loss curves** across epochs.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "35f03270",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep 1 (Step 000000): Train loss 5.863, Val loss 5.848\n",
            "Ep 1 (Step 000050): Train loss 4.960, Val loss 5.017\n",
            "Ep 1 (Step 000100): Train loss 4.873, Val loss 4.871\n",
            "Ep 1 (Step 000150): Train loss 4.776, Val loss 4.811\n",
            "Ep 1 (Step 000200): Train loss 4.694, Val loss 4.775\n",
            "Ep 1 (Step 000250): Train loss 4.638, Val loss 4.727\n",
            "Ep 1 (Step 000300): Train loss 4.671, Val loss 4.697\n",
            "Ep 1 (Step 000350): Train loss 4.629, Val loss 4.673\n",
            "Ep 1 (Step 000400): Train loss 4.551, Val loss 4.633\n",
            "Ep 1 (Step 000450): Train loss 4.520, Val loss 4.611\n",
            "Ep 1 (Step 000500): Train loss 4.548, Val loss 4.591\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to the model to the model. ### Response : The model to the model. <|unk|> : The model. <|unk|> : The model' s performance. <|unk|> : The model' s performance of the model' s performance of the model' s performance of the model\n",
            "Ep 2 (Step 000550): Train loss 4.474, Val loss 4.575\n",
            "Ep 2 (Step 000600): Train loss 4.466, Val loss 4.549\n",
            "Ep 2 (Step 000650): Train loss 4.488, Val loss 4.524\n",
            "Ep 2 (Step 000700): Train loss 4.464, Val loss 4.517\n",
            "Ep 2 (Step 000750): Train loss 4.472, Val loss 4.494\n",
            "Ep 2 (Step 000800): Train loss 4.357, Val loss 4.482\n",
            "Ep 2 (Step 000850): Train loss 4.350, Val loss 4.461\n",
            "Ep 2 (Step 000900): Train loss 4.306, Val loss 4.449\n",
            "Ep 2 (Step 000950): Train loss 4.333, Val loss 4.410\n",
            "Ep 2 (Step 001000): Train loss 4.313, Val loss 4.411\n",
            "Ep 2 (Step 001050): Train loss 4.280, Val loss 4.387\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to the model to the model to the model to the model to the model to the model. ### Response : 1. <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|>\n",
            "Ep 3 (Step 001100): Train loss 4.230, Val loss 4.372\n",
            "Ep 3 (Step 001150): Train loss 4.216, Val loss 4.369\n",
            "Ep 3 (Step 001200): Train loss 4.149, Val loss 4.342\n",
            "Ep 3 (Step 001250): Train loss 4.210, Val loss 4.340\n",
            "Ep 3 (Step 001300): Train loss 4.193, Val loss 4.331\n",
            "Ep 3 (Step 001350): Train loss 4.166, Val loss 4.318\n",
            "Ep 3 (Step 001400): Train loss 4.120, Val loss 4.276\n",
            "Ep 3 (Step 001450): Train loss 4.088, Val loss 4.273\n",
            "Ep 3 (Step 001500): Train loss 4.052, Val loss 4.264\n",
            "Ep 3 (Step 001550): Train loss 4.126, Val loss 4.251\n",
            "Ep 3 (Step 001600): Train loss 4.070, Val loss 4.238\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to the model. ### Response : The passage discusses the model' s performance of the model' s performance of the model' s performance of the model. <|unk|> : The model' s performance of the model' s performance of the model' s performance.\n",
            "Ep 4 (Step 001650): Train loss 4.067, Val loss 4.209\n",
            "Ep 4 (Step 001700): Train loss 4.048, Val loss 4.209\n",
            "Ep 4 (Step 001750): Train loss 4.022, Val loss 4.192\n",
            "Ep 4 (Step 001800): Train loss 4.037, Val loss 4.178\n",
            "Ep 4 (Step 001850): Train loss 3.908, Val loss 4.174\n",
            "Ep 4 (Step 001900): Train loss 3.963, Val loss 4.151\n",
            "Ep 4 (Step 001950): Train loss 3.969, Val loss 4.144\n",
            "Ep 4 (Step 002000): Train loss 3.930, Val loss 4.108\n",
            "Ep 4 (Step 002050): Train loss 3.921, Val loss 4.105\n",
            "Ep 4 (Step 002100): Train loss 3.859, Val loss 4.095\n",
            "Ep 4 (Step 002150): Train loss 3.885, Val loss 4.068\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to the model. ### Response : The passage discusses the model is a model to the model. The model to the model, which is a model to the model. This summary captures the model is trained on the model' s performance of the model. This\n",
            "Ep 5 (Step 002200): Train loss 3.819, Val loss 4.058\n",
            "Ep 5 (Step 002250): Train loss 3.828, Val loss 4.057\n",
            "Ep 5 (Step 002300): Train loss 3.834, Val loss 4.016\n",
            "Ep 5 (Step 002350): Train loss 3.819, Val loss 4.019\n",
            "Ep 5 (Step 002400): Train loss 3.772, Val loss 3.995\n",
            "Ep 5 (Step 002450): Train loss 3.790, Val loss 3.980\n",
            "Ep 5 (Step 002500): Train loss 3.717, Val loss 3.979\n",
            "Ep 5 (Step 002550): Train loss 3.778, Val loss 3.946\n",
            "Ep 5 (Step 002600): Train loss 3.710, Val loss 3.964\n",
            "Ep 5 (Step 002650): Train loss 3.702, Val loss 3.935\n",
            "Ep 5 (Step 002700): Train loss 3.662, Val loss 3.930\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to the model performance of the model performance. ### Response : The passage discusses the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of\n",
            "Ep 6 (Step 002750): Train loss 3.673, Val loss 3.923\n",
            "Ep 6 (Step 002800): Train loss 3.672, Val loss 3.897\n",
            "Ep 6 (Step 002850): Train loss 3.636, Val loss 3.902\n",
            "Ep 6 (Step 002900): Train loss 3.616, Val loss 3.870\n",
            "Ep 6 (Step 002950): Train loss 3.588, Val loss 3.864\n",
            "Ep 6 (Step 003000): Train loss 3.618, Val loss 3.855\n",
            "Ep 6 (Step 003050): Train loss 3.604, Val loss 3.856\n",
            "Ep 6 (Step 003100): Train loss 3.570, Val loss 3.847\n",
            "Ep 6 (Step 003150): Train loss 3.528, Val loss 3.823\n",
            "Ep 6 (Step 003200): Train loss 3.503, Val loss 3.822\n",
            "Ep 6 (Step 003250): Train loss 3.541, Val loss 3.813\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to the model size of the model. ### Response : The passage discusses the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the\n",
            "Ep 7 (Step 003300): Train loss 3.525, Val loss 3.811\n",
            "Ep 7 (Step 003350): Train loss 3.539, Val loss 3.797\n",
            "Ep 7 (Step 003400): Train loss 3.518, Val loss 3.783\n",
            "Ep 7 (Step 003450): Train loss 3.481, Val loss 3.775\n",
            "Ep 7 (Step 003500): Train loss 3.475, Val loss 3.752\n",
            "Ep 7 (Step 003550): Train loss 3.426, Val loss 3.765\n",
            "Ep 7 (Step 003600): Train loss 3.412, Val loss 3.736\n",
            "Ep 7 (Step 003650): Train loss 3.453, Val loss 3.728\n",
            "Ep 7 (Step 003700): Train loss 3.386, Val loss 3.738\n",
            "Ep 7 (Step 003750): Train loss 3.426, Val loss 3.722\n",
            "Ep 7 (Step 003800): Train loss 3.357, Val loss 3.707\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the 7B model. ### Response : The performance of the 7B model,\n",
            "Ep 8 (Step 003850): Train loss 3.301, Val loss 3.672\n",
            "Ep 8 (Step 003900): Train loss 3.367, Val loss 3.704\n",
            "Ep 8 (Step 003950): Train loss 3.360, Val loss 3.700\n",
            "Ep 8 (Step 004000): Train loss 3.331, Val loss 3.660\n",
            "Ep 8 (Step 004050): Train loss 3.352, Val loss 3.673\n",
            "Ep 8 (Step 004100): Train loss 3.380, Val loss 3.647\n",
            "Ep 8 (Step 004150): Train loss 3.280, Val loss 3.621\n",
            "Ep 8 (Step 004200): Train loss 3.400, Val loss 3.631\n",
            "Ep 8 (Step 004250): Train loss 3.277, Val loss 3.636\n",
            "Ep 8 (Step 004300): Train loss 3.231, Val loss 3.609\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to the model size increases the model size increases the model size increases. ### Response : The study compares the performance of the performance of the performance of the performance of the performance of the performance of the performance of the model, which is a smaller model, with\n",
            "Ep 9 (Step 004350): Train loss 3.291, Val loss 3.610\n",
            "Ep 9 (Step 004400): Train loss 3.235, Val loss 3.606\n",
            "Ep 9 (Step 004450): Train loss 3.259, Val loss 3.614\n",
            "Ep 9 (Step 004500): Train loss 3.284, Val loss 3.595\n",
            "Ep 9 (Step 004550): Train loss 3.185, Val loss 3.572\n",
            "Ep 9 (Step 004600): Train loss 3.242, Val loss 3.576\n",
            "Ep 9 (Step 004650): Train loss 3.222, Val loss 3.572\n",
            "Ep 9 (Step 004700): Train loss 3.201, Val loss 3.557\n",
            "Ep 9 (Step 004750): Train loss 3.195, Val loss 3.569\n",
            "Ep 9 (Step 004800): Train loss 3.125, Val loss 3.526\n",
            "Ep 9 (Step 004850): Train loss 3.149, Val loss 3.514\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to the model size increases, the model is still significantly better than the model size. ### Response : The study compares the performance of the performance of the performance of the performance of the smaller models, which is trained on the 7B model, which is trained on\n",
            "Ep 10 (Step 004900): Train loss 3.091, Val loss 3.509\n",
            "Ep 10 (Step 004950): Train loss 3.110, Val loss 3.487\n",
            "Ep 10 (Step 005000): Train loss 3.104, Val loss 3.508\n",
            "Ep 10 (Step 005050): Train loss 2.990, Val loss 3.482\n",
            "Ep 10 (Step 005100): Train loss 3.101, Val loss 3.491\n",
            "Ep 10 (Step 005150): Train loss 3.022, Val loss 3.469\n",
            "Ep 10 (Step 005200): Train loss 3.039, Val loss 3.463\n",
            "Ep 10 (Step 005250): Train loss 3.056, Val loss 3.471\n",
            "Ep 10 (Step 005300): Train loss 2.977, Val loss 3.444\n",
            "Ep 10 (Step 005350): Train loss 2.980, Val loss 3.431\n",
            "Ep 10 (Step 005400): Train loss 3.042, Val loss 3.434\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to the model size increases, and the model size increases from the model size increases, and the model size increases, and the model size increases to the model size increases, and the model size increases to the model size increases. We observe that the model size\n",
            "Ep 11 (Step 005450): Train loss 3.022, Val loss 3.420\n",
            "Ep 11 (Step 005500): Train loss 2.986, Val loss 3.416\n",
            "Ep 11 (Step 005550): Train loss 2.883, Val loss 3.406\n",
            "Ep 11 (Step 005600): Train loss 2.941, Val loss 3.399\n",
            "Ep 11 (Step 005650): Train loss 2.954, Val loss 3.419\n",
            "Ep 11 (Step 005700): Train loss 2.969, Val loss 3.404\n",
            "Ep 11 (Step 005750): Train loss 2.960, Val loss 3.373\n",
            "Ep 11 (Step 005800): Train loss 2.890, Val loss 3.370\n",
            "Ep 11 (Step 005850): Train loss 2.889, Val loss 3.360\n",
            "Ep 11 (Step 005900): Train loss 2.891, Val loss 3.364\n",
            "Ep 11 (Step 005950): Train loss 2.875, Val loss 3.344\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to the model size increases, even when training can be more than the model size increases. ### Response : The study compares the performance of the performance of the performance of the scaling laws for the scaling laws and model size, with a smaller model size of parameters\n",
            "Ep 12 (Step 006000): Train loss 2.890, Val loss 3.343\n",
            "Ep 12 (Step 006050): Train loss 2.874, Val loss 3.335\n",
            "Ep 12 (Step 006100): Train loss 2.843, Val loss 3.325\n",
            "Ep 12 (Step 006150): Train loss 2.827, Val loss 3.307\n",
            "Ep 12 (Step 006200): Train loss 2.833, Val loss 3.311\n",
            "Ep 12 (Step 006250): Train loss 2.833, Val loss 3.303\n",
            "Ep 12 (Step 006300): Train loss 2.800, Val loss 3.315\n",
            "Ep 12 (Step 006350): Train loss 2.862, Val loss 3.298\n",
            "Ep 12 (Step 006400): Train loss 2.752, Val loss 3.292\n",
            "Ep 12 (Step 006450): Train loss 2.737, Val loss 3.286\n",
            "Ep 12 (Step 006500): Train loss 2.738, Val loss 3.275\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to the model size increases, and the model size increases from 8B to 1. 2. 5. 5. 5. ### Response : The study compares the performance of the performance of the smaller models, showing that larger models, showing that larger models with smaller\n",
            "Ep 13 (Step 006550): Train loss 2.724, Val loss 3.250\n",
            "Ep 13 (Step 006600): Train loss 2.720, Val loss 3.261\n",
            "Ep 13 (Step 006650): Train loss 2.726, Val loss 3.270\n",
            "Ep 13 (Step 006700): Train loss 2.720, Val loss 3.250\n",
            "Ep 13 (Step 006750): Train loss 2.739, Val loss 3.240\n",
            "Ep 13 (Step 006800): Train loss 2.697, Val loss 3.245\n",
            "Ep 13 (Step 006850): Train loss 2.670, Val loss 3.222\n",
            "Ep 13 (Step 006900): Train loss 2.693, Val loss 3.249\n",
            "Ep 13 (Step 006950): Train loss 2.627, Val loss 3.236\n",
            "Ep 13 (Step 007000): Train loss 2.620, Val loss 3.196\n",
            "Ep 13 (Step 007050): Train loss 2.599, Val loss 3.204\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to a smaller model, but also outperforms the larger model, despite the larger model, even with a smaller model, and the smaller model. ### Response : The study compares the scaling laws for a smaller model, showing that larger models, particularly in the smaller\n",
            "Ep 14 (Step 007100): Train loss 2.621, Val loss 3.182\n",
            "Ep 14 (Step 007150): Train loss 2.639, Val loss 3.181\n",
            "Ep 14 (Step 007200): Train loss 2.624, Val loss 3.191\n",
            "Ep 14 (Step 007250): Train loss 2.627, Val loss 3.189\n",
            "Ep 14 (Step 007300): Train loss 2.605, Val loss 3.157\n",
            "Ep 14 (Step 007350): Train loss 2.522, Val loss 3.162\n",
            "Ep 14 (Step 007400): Train loss 2.577, Val loss 3.156\n",
            "Ep 14 (Step 007450): Train loss 2.523, Val loss 3.128\n",
            "Ep 14 (Step 007500): Train loss 2.530, Val loss 3.137\n",
            "Ep 14 (Step 007550): Train loss 2.553, Val loss 3.144\n",
            "Ep 14 (Step 007600): Train loss 2.467, Val loss 3.129\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to train a model with a larger model, and model with a smaller model. ### Response : The study compares the scaling laws for scaling laws, showing that larger models, showing that larger models can be trained on a larger model size of parameters, with a\n",
            "Ep 15 (Step 007650): Train loss 2.540, Val loss 3.103\n",
            "Ep 15 (Step 007700): Train loss 2.473, Val loss 3.084\n",
            "Ep 15 (Step 007750): Train loss 2.498, Val loss 3.095\n",
            "Ep 15 (Step 007800): Train loss 2.494, Val loss 3.089\n",
            "Ep 15 (Step 007850): Train loss 2.509, Val loss 3.087\n",
            "Ep 15 (Step 007900): Train loss 2.491, Val loss 3.099\n",
            "Ep 15 (Step 007950): Train loss 2.450, Val loss 3.080\n",
            "Ep 15 (Step 008000): Train loss 2.470, Val loss 3.079\n",
            "Ep 15 (Step 008050): Train loss 2.451, Val loss 3.078\n",
            "Ep 15 (Step 008100): Train loss 2.440, Val loss 3.076\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to the model size increases. ### Response : The study compares the performance improvements in the performance of the 8B model, showing that the performance of the 8B model, showing that the larger models, showing that larger models with a larger parameter size of parameters, showing\n",
            "Ep 16 (Step 008150): Train loss 2.441, Val loss 3.040\n",
            "Ep 16 (Step 008200): Train loss 2.396, Val loss 3.047\n",
            "Ep 16 (Step 008250): Train loss 2.388, Val loss 3.054\n",
            "Ep 16 (Step 008300): Train loss 2.343, Val loss 3.030\n",
            "Ep 16 (Step 008350): Train loss 2.429, Val loss 3.042\n",
            "Ep 16 (Step 008400): Train loss 2.380, Val loss 3.033\n",
            "Ep 16 (Step 008450): Train loss 2.351, Val loss 3.029\n",
            "Ep 16 (Step 008500): Train loss 2.375, Val loss 3.022\n",
            "Ep 16 (Step 008550): Train loss 2.328, Val loss 3.023\n",
            "Ep 16 (Step 008600): Train loss 2.318, Val loss 2.996\n",
            "Ep 16 (Step 008650): Train loss 2.316, Val loss 2.997\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to train a model with a larger model, and model with a smaller model. ### Response : The study compares the scaling laws for a larger model, showing that larger models trained on a smaller size of parameters, and outperforms larger parameter model sizes, with a\n",
            "Ep 17 (Step 008700): Train loss 2.388, Val loss 2.989\n",
            "Ep 17 (Step 008750): Train loss 2.291, Val loss 2.986\n",
            "Ep 17 (Step 008800): Train loss 2.251, Val loss 2.985\n",
            "Ep 17 (Step 008850): Train loss 2.274, Val loss 2.987\n",
            "Ep 17 (Step 008900): Train loss 2.306, Val loss 2.985\n",
            "Ep 17 (Step 008950): Train loss 2.235, Val loss 2.962\n",
            "Ep 17 (Step 009000): Train loss 2.264, Val loss 2.943\n",
            "Ep 17 (Step 009050): Train loss 2.251, Val loss 2.958\n",
            "Ep 17 (Step 009100): Train loss 2.222, Val loss 2.958\n",
            "Ep 17 (Step 009150): Train loss 2.218, Val loss 2.942\n",
            "Ep 17 (Step 009200): Train loss 2.218, Val loss 2.932\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to 540B can be a similar trend. ### Response : The study compares the performance of LLaMA models with a 7B and 8B model, showing that outperforms Qwen1. 8B and outperforms Qwen1. 8B and 8B and 70B on the 7B model, showing that outperforms larger models\n",
            "Ep 18 (Step 009250): Train loss 2.222, Val loss 2.912\n",
            "Ep 18 (Step 009300): Train loss 2.199, Val loss 2.915\n",
            "Ep 18 (Step 009350): Train loss 2.209, Val loss 2.921\n",
            "Ep 18 (Step 009400): Train loss 2.154, Val loss 2.897\n",
            "Ep 18 (Step 009450): Train loss 2.167, Val loss 2.931\n",
            "Ep 18 (Step 009500): Train loss 2.166, Val loss 2.906\n",
            "Ep 18 (Step 009550): Train loss 2.151, Val loss 2.898\n",
            "Ep 18 (Step 009600): Train loss 2.195, Val loss 2.887\n",
            "Ep 18 (Step 009650): Train loss 2.107, Val loss 2.884\n",
            "Ep 18 (Step 009700): Train loss 2.118, Val loss 2.874\n",
            "Ep 18 (Step 009750): Train loss 2.135, Val loss 2.867\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to train a model with a similar parameter model. ### Response : The study compares the performance of the 8B model on the 8B model, showing that larger models with a 7B parameter model, showing that larger parameter model size, showing that larger parameter size increases,\n",
            "Ep 19 (Step 009800): Train loss 2.052, Val loss 2.875\n",
            "Ep 19 (Step 009850): Train loss 2.161, Val loss 2.886\n",
            "Ep 19 (Step 009900): Train loss 2.107, Val loss 2.867\n",
            "Ep 19 (Step 009950): Train loss 2.083, Val loss 2.851\n",
            "Ep 19 (Step 010000): Train loss 2.119, Val loss 2.859\n",
            "Ep 19 (Step 010050): Train loss 2.092, Val loss 2.850\n",
            "Ep 19 (Step 010100): Train loss 2.044, Val loss 2.841\n",
            "Ep 19 (Step 010150): Train loss 2.035, Val loss 2.833\n",
            "Ep 19 (Step 010200): Train loss 2.053, Val loss 2.831\n",
            "Ep 19 (Step 010250): Train loss 2.016, Val loss 2.824\n",
            "Ep 19 (Step 010300): Train loss 2.016, Val loss 2.808\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to train a model with a similar parameter model, and that of the larger model, as the larger model, and model is trained on a larger parameter model. ### Response : The study compares the performance of a smaller language model with a larger model with a\n",
            "Ep 20 (Step 010350): Train loss 2.000, Val loss 2.811\n",
            "Ep 20 (Step 010400): Train loss 2.016, Val loss 2.806\n",
            "Ep 20 (Step 010450): Train loss 2.011, Val loss 2.799\n",
            "Ep 20 (Step 010500): Train loss 2.023, Val loss 2.797\n",
            "Ep 20 (Step 010550): Train loss 1.938, Val loss 2.782\n",
            "Ep 20 (Step 010600): Train loss 1.957, Val loss 2.778\n",
            "Ep 20 (Step 010650): Train loss 1.988, Val loss 2.788\n",
            "Ep 20 (Step 010700): Train loss 1.970, Val loss 2.792\n",
            "Ep 20 (Step 010750): Train loss 1.965, Val loss 2.776\n",
            "Ep 20 (Step 010800): Train loss 1.914, Val loss 2.785\n",
            "Ep 20 (Step 010850): Train loss 1.946, Val loss 2.757\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to train a model with a ### Response : The study compares the performance of a linear scaling curve with larger models( 8B, 8B, and 70B parameters) with larger models like Llama-2 and 34B and 70B, showing that larger models with larger sizes( 8B,\n",
            "Ep 21 (Step 010900): Train loss 1.891, Val loss 2.746\n",
            "Ep 21 (Step 010950): Train loss 1.920, Val loss 2.749\n",
            "Ep 21 (Step 011000): Train loss 1.850, Val loss 2.742\n",
            "Ep 21 (Step 011050): Train loss 1.880, Val loss 2.751\n",
            "Ep 21 (Step 011100): Train loss 1.881, Val loss 2.736\n",
            "Ep 21 (Step 011150): Train loss 1.909, Val loss 2.754\n",
            "Ep 21 (Step 011200): Train loss 1.879, Val loss 2.747\n",
            "Ep 21 (Step 011250): Train loss 1.856, Val loss 2.732\n",
            "Ep 21 (Step 011300): Train loss 1.908, Val loss 2.717\n",
            "Ep 21 (Step 011350): Train loss 1.860, Val loss 2.717\n",
            "Ep 21 (Step 011400): Train loss 1.810, Val loss 2.700\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to train a model with a similar parameter model. ### Response : The study compares the performance of the 8B model, showing that larger models with larger parameter scales, showing that larger models trained on a larger model size of 8 billion parameters, while significantly larger parameter\n",
            "Ep 22 (Step 011450): Train loss 1.797, Val loss 2.697\n",
            "Ep 22 (Step 011500): Train loss 1.799, Val loss 2.689\n",
            "Ep 22 (Step 011550): Train loss 1.812, Val loss 2.699\n",
            "Ep 22 (Step 011600): Train loss 1.752, Val loss 2.686\n",
            "Ep 22 (Step 011650): Train loss 1.809, Val loss 2.680\n",
            "Ep 22 (Step 011700): Train loss 1.799, Val loss 2.682\n",
            "Ep 22 (Step 011750): Train loss 1.804, Val loss 2.679\n",
            "Ep 22 (Step 011800): Train loss 1.793, Val loss 2.675\n",
            "Ep 22 (Step 011850): Train loss 1.806, Val loss 2.676\n",
            "Ep 22 (Step 011900): Train loss 1.782, Val loss 2.645\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to train a model with a 1. 5 billion parameter model, as described in Figure 1. ### Response : The study compares the performance of different models with different parameter sizes( 8B and 1. 7B parameters) and outperforms Qwen1. 5 billion parameters, showing\n",
            "Ep 23 (Step 011950): Train loss 1.756, Val loss 2.644\n",
            "Ep 23 (Step 012000): Train loss 1.761, Val loss 2.655\n",
            "Ep 23 (Step 012050): Train loss 1.720, Val loss 2.637\n",
            "Ep 23 (Step 012100): Train loss 1.738, Val loss 2.632\n",
            "Ep 23 (Step 012150): Train loss 1.738, Val loss 2.628\n",
            "Ep 23 (Step 012200): Train loss 1.707, Val loss 2.637\n",
            "Ep 23 (Step 012250): Train loss 1.690, Val loss 2.612\n",
            "Ep 23 (Step 012300): Train loss 1.659, Val loss 2.623\n",
            "Ep 23 (Step 012350): Train loss 1.675, Val loss 2.614\n",
            "Ep 23 (Step 012400): Train loss 1.652, Val loss 2.610\n",
            "Ep 23 (Step 012450): Train loss 1.662, Val loss 2.600\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to train a model with a ### Response : The study compares the performance of a linear scaling laws to scaling laws for a fixed scaling laws for a fixed size of model, showing that larger models like Llama-2 and 1 billion parameters. The study compares to larger models\n",
            "Ep 24 (Step 012500): Train loss 1.633, Val loss 2.596\n",
            "Ep 24 (Step 012550): Train loss 1.672, Val loss 2.598\n",
            "Ep 24 (Step 012600): Train loss 1.656, Val loss 2.569\n",
            "Ep 24 (Step 012650): Train loss 1.676, Val loss 2.585\n",
            "Ep 24 (Step 012700): Train loss 1.631, Val loss 2.570\n",
            "Ep 24 (Step 012750): Train loss 1.633, Val loss 2.581\n",
            "Ep 24 (Step 012800): Train loss 1.602, Val loss 2.580\n",
            "Ep 24 (Step 012850): Train loss 1.616, Val loss 2.575\n",
            "Ep 24 (Step 012900): Train loss 1.648, Val loss 2.588\n",
            "Ep 24 (Step 012950): Train loss 1.608, Val loss 2.568\n",
            "Ep 24 (Step 013000): Train loss 1.557, Val loss 2.565\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to train a model with a similar number of parameters. ### Response : The study compares the performance of different models( 8B, 7B, and 34B, and 34B), showing that larger models trained on a larger 8B model, and outperforms larger parameter model,\n",
            "Ep 25 (Step 013050): Train loss 1.593, Val loss 2.555\n",
            "Ep 25 (Step 013100): Train loss 1.577, Val loss 2.560\n",
            "Ep 25 (Step 013150): Train loss 1.589, Val loss 2.548\n",
            "Ep 25 (Step 013200): Train loss 1.541, Val loss 2.524\n",
            "Ep 25 (Step 013250): Train loss 1.567, Val loss 2.539\n",
            "Ep 25 (Step 013300): Train loss 1.568, Val loss 2.537\n",
            "Ep 25 (Step 013350): Train loss 1.570, Val loss 2.534\n",
            "Ep 25 (Step 013400): Train loss 1.527, Val loss 2.529\n",
            "Ep 25 (Step 013450): Train loss 1.519, Val loss 2.517\n",
            "Ep 25 (Step 013500): Train loss 1.503, Val loss 2.510\n",
            "Ep 25 (Step 013550): Train loss 1.513, Val loss 2.503\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to train a model with a 1. 5 billion parameter model, and 1. 5 billion parameters. ### Response : The study compares two models with a similar parameter size of 8 billion parameters and 1. 8B and 9B models with 1. 3B parameters, showing\n",
            "Ep 26 (Step 013600): Train loss 1.477, Val loss 2.507\n",
            "Ep 26 (Step 013650): Train loss 1.529, Val loss 2.498\n",
            "Ep 26 (Step 013700): Train loss 1.476, Val loss 2.493\n",
            "Ep 26 (Step 013750): Train loss 1.494, Val loss 2.497\n",
            "Ep 26 (Step 013800): Train loss 1.491, Val loss 2.483\n",
            "Ep 26 (Step 013850): Train loss 1.449, Val loss 2.493\n",
            "Ep 26 (Step 013900): Train loss 1.485, Val loss 2.493\n",
            "Ep 26 (Step 013950): Train loss 1.440, Val loss 2.480\n",
            "Ep 26 (Step 014000): Train loss 1.469, Val loss 2.491\n",
            "Ep 26 (Step 014050): Train loss 1.456, Val loss 2.487\n",
            "Ep 26 (Step 014100): Train loss 1.436, Val loss 2.485\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to train a model with a similar parameter model. ### Response : The study compares the performance of the 8B model, showing that larger models trained on a larger parameter model, showing that larger models trained on more tokens, trained on more tokens, with larger models\n",
            "Ep 27 (Step 014150): Train loss 1.457, Val loss 2.465\n",
            "Ep 27 (Step 014200): Train loss 1.440, Val loss 2.464\n",
            "Ep 27 (Step 014250): Train loss 1.411, Val loss 2.468\n",
            "Ep 27 (Step 014300): Train loss 1.403, Val loss 2.456\n",
            "Ep 27 (Step 014350): Train loss 1.376, Val loss 2.439\n",
            "Ep 27 (Step 014400): Train loss 1.431, Val loss 2.443\n",
            "Ep 27 (Step 014450): Train loss 1.415, Val loss 2.459\n",
            "Ep 27 (Step 014500): Train loss 1.369, Val loss 2.443\n",
            "Ep 27 (Step 014550): Train loss 1.378, Val loss 2.451\n",
            "Ep 27 (Step 014600): Train loss 1.383, Val loss 2.436\n",
            "Ep 27 (Step 014650): Train loss 1.391, Val loss 2.434\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to train a model with a 1. 0. 5 billion parameter model, and 1. 0. ### Response : The study compares the optimal model size for a fixed scaling laws and training loss in performance, showing that larger models trained on a 1 billion parameter\n",
            "Ep 28 (Step 014700): Train loss 1.317, Val loss 2.431\n",
            "Ep 28 (Step 014750): Train loss 1.391, Val loss 2.428\n",
            "Ep 28 (Step 014800): Train loss 1.374, Val loss 2.408\n",
            "Ep 28 (Step 014850): Train loss 1.316, Val loss 2.409\n",
            "Ep 28 (Step 014900): Train loss 1.354, Val loss 2.423\n",
            "Ep 28 (Step 014950): Train loss 1.349, Val loss 2.405\n",
            "Ep 28 (Step 015000): Train loss 1.295, Val loss 2.405\n",
            "Ep 28 (Step 015050): Train loss 1.322, Val loss 2.422\n",
            "Ep 28 (Step 015100): Train loss 1.300, Val loss 2.403\n",
            "Ep 28 (Step 015150): Train loss 1.313, Val loss 2.400\n",
            "Ep 28 (Step 015200): Train loss 1.263, Val loss 2.394\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to train a model with a similar size. ### Response : The study compares the performance of different models with varying sizes( 8B parameters) and model sizes, showing that larger models trained on downstream tasks, with larger models trained on more data size and model size\n",
            "Ep 29 (Step 015250): Train loss 1.311, Val loss 2.388\n",
            "Ep 29 (Step 015300): Train loss 1.228, Val loss 2.387\n",
            "Ep 29 (Step 015350): Train loss 1.268, Val loss 2.391\n",
            "Ep 29 (Step 015400): Train loss 1.272, Val loss 2.377\n",
            "Ep 29 (Step 015450): Train loss 1.224, Val loss 2.378\n",
            "Ep 29 (Step 015500): Train loss 1.282, Val loss 2.386\n",
            "Ep 29 (Step 015550): Train loss 1.259, Val loss 2.375\n",
            "Ep 29 (Step 015600): Train loss 1.253, Val loss 2.367\n",
            "Ep 29 (Step 015650): Train loss 1.224, Val loss 2.364\n",
            "Ep 29 (Step 015700): Train loss 1.256, Val loss 2.365\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to train a model, the optimal model, and the optimal model size is more efficient. ### Response : The study compares two models of varying parameter sizes( 1. 8B parameters) and fixed size for the optimal model sizes( 1. 8B parameters),\n",
            "Ep 30 (Step 015750): Train loss 1.233, Val loss 2.357\n",
            "Ep 30 (Step 015800): Train loss 1.218, Val loss 2.357\n",
            "Ep 30 (Step 015850): Train loss 1.227, Val loss 2.360\n",
            "Ep 30 (Step 015900): Train loss 1.222, Val loss 2.353\n",
            "Ep 30 (Step 015950): Train loss 1.227, Val loss 2.362\n",
            "Ep 30 (Step 016000): Train loss 1.213, Val loss 2.344\n",
            "Ep 30 (Step 016050): Train loss 1.173, Val loss 2.340\n",
            "Ep 30 (Step 016100): Train loss 1.238, Val loss 2.352\n",
            "Ep 30 (Step 016150): Train loss 1.193, Val loss 2.340\n",
            "Ep 30 (Step 016200): Train loss 1.173, Val loss 2.319\n",
            "Ep 30 (Step 016250): Train loss 1.179, Val loss 2.331\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to train a model, the optimal training loss and downstream tasks, as a ### Response : The study compares different models of varying sizes( 8B and 540B) and 540B parameters( 8B, and 540B) with a similar parameter size of 1. 8B model,\n",
            "Ep 31 (Step 016300): Train loss 1.156, Val loss 2.306\n",
            "Ep 31 (Step 016350): Train loss 1.153, Val loss 2.322\n",
            "Ep 31 (Step 016400): Train loss 1.136, Val loss 2.314\n",
            "Ep 31 (Step 016450): Train loss 1.129, Val loss 2.314\n",
            "Ep 31 (Step 016500): Train loss 1.128, Val loss 2.330\n",
            "Ep 31 (Step 016550): Train loss 1.124, Val loss 2.326\n",
            "Ep 31 (Step 016600): Train loss 1.154, Val loss 2.313\n",
            "Ep 31 (Step 016650): Train loss 1.134, Val loss 2.292\n",
            "Ep 31 (Step 016700): Train loss 1.136, Val loss 2.302\n",
            "Ep 31 (Step 016750): Train loss 1.138, Val loss 2.304\n",
            "Ep 31 (Step 016800): Train loss 1.123, Val loss 2.281\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The study compares different parameter sizes( 1. 7B parameters) and model sizes( 1. 8B parameters) on a fixed batch size and fixed batch sizes, showing that larger models can outperform larger sizes( 1. 8B parameters). The\n",
            "Ep 32 (Step 016850): Train loss 1.067, Val loss 2.297\n",
            "Ep 32 (Step 016900): Train loss 1.094, Val loss 2.282\n",
            "Ep 32 (Step 016950): Train loss 1.072, Val loss 2.283\n",
            "Ep 32 (Step 017000): Train loss 1.094, Val loss 2.279\n",
            "Ep 32 (Step 017050): Train loss 1.089, Val loss 2.272\n",
            "Ep 32 (Step 017100): Train loss 1.040, Val loss 2.273\n",
            "Ep 32 (Step 017150): Train loss 1.055, Val loss 2.285\n",
            "Ep 32 (Step 017200): Train loss 1.064, Val loss 2.289\n",
            "Ep 32 (Step 017250): Train loss 1.052, Val loss 2.273\n",
            "Ep 32 (Step 017300): Train loss 1.087, Val loss 2.268\n",
            "Ep 32 (Step 017350): Train loss 1.068, Val loss 2.272\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to train a model with more parameters. ### Response : The study compares the scaling curves for the optimal model size to linear coefficients of the optimal model, showing that larger models trained on 8 billion parameters trained on 8 billion tokens. The study compares the optimal model\n",
            "Ep 33 (Step 017400): Train loss 1.016, Val loss 2.240\n",
            "Ep 33 (Step 017450): Train loss 1.051, Val loss 2.266\n",
            "Ep 33 (Step 017500): Train loss 1.047, Val loss 2.255\n",
            "Ep 33 (Step 017550): Train loss 1.012, Val loss 2.268\n",
            "Ep 33 (Step 017600): Train loss 1.015, Val loss 2.257\n",
            "Ep 33 (Step 017650): Train loss 1.028, Val loss 2.249\n",
            "Ep 33 (Step 017700): Train loss 1.053, Val loss 2.254\n",
            "Ep 33 (Step 017750): Train loss 1.012, Val loss 2.242\n",
            "Ep 33 (Step 017800): Train loss 1.024, Val loss 2.240\n",
            "Ep 33 (Step 017850): Train loss 0.978, Val loss 2.233\n",
            "Ep 33 (Step 017900): Train loss 1.019, Val loss 2.234\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The study compares the optimal model size and fixed size for a fixed model, showing that scaling performance improves with larger models like Chinchilla and 540B outperforms larger models like Chinchilla and Chinchilla. 1B shows similar performance with larger parameter sizes, showing that larger\n",
            "Ep 34 (Step 017950): Train loss 0.957, Val loss 2.235\n",
            "Ep 34 (Step 018000): Train loss 0.993, Val loss 2.230\n",
            "Ep 34 (Step 018050): Train loss 1.013, Val loss 2.244\n",
            "Ep 34 (Step 018100): Train loss 0.962, Val loss 2.232\n",
            "Ep 34 (Step 018150): Train loss 0.964, Val loss 2.223\n",
            "Ep 34 (Step 018200): Train loss 0.944, Val loss 2.218\n",
            "Ep 34 (Step 018250): Train loss 0.930, Val loss 2.218\n",
            "Ep 34 (Step 018300): Train loss 0.947, Val loss 2.238\n",
            "Ep 34 (Step 018350): Train loss 0.938, Val loss 2.220\n",
            "Ep 34 (Step 018400): Train loss 0.923, Val loss 2.207\n",
            "Ep 34 (Step 018450): Train loss 0.957, Val loss 2.219\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The study compares the performance of a model with a 1. 3B model with 9 billion parameters and 9. 3B parameter model, showing that larger models trained on more downstream tasks, particularly with more parameters. The study shows that while scaling from\n",
            "Ep 35 (Step 018500): Train loss 0.910, Val loss 2.206\n",
            "Ep 35 (Step 018550): Train loss 0.933, Val loss 2.221\n",
            "Ep 35 (Step 018600): Train loss 0.947, Val loss 2.210\n",
            "Ep 35 (Step 018650): Train loss 0.912, Val loss 2.203\n",
            "Ep 35 (Step 018700): Train loss 0.913, Val loss 2.211\n",
            "Ep 35 (Step 018750): Train loss 0.904, Val loss 2.209\n",
            "Ep 35 (Step 018800): Train loss 0.907, Val loss 2.193\n",
            "Ep 35 (Step 018850): Train loss 0.930, Val loss 2.215\n",
            "Ep 35 (Step 018900): Train loss 0.904, Val loss 2.190\n",
            "Ep 35 (Step 018950): Train loss 0.921, Val loss 2.201\n",
            "Ep 35 (Step 019000): Train loss 0.890, Val loss 2.175\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The study compares different parameter sizes( 1. 5B and 540B) against varying parameter models, showing that larger models trained on more data. It is noted that while larger models trained on more tokens( 1. 8B and 540B) is more\n",
            "Ep 36 (Step 019050): Train loss 0.890, Val loss 2.187\n",
            "Ep 36 (Step 019100): Train loss 0.899, Val loss 2.189\n",
            "Ep 36 (Step 019150): Train loss 0.872, Val loss 2.197\n",
            "Ep 36 (Step 019200): Train loss 0.890, Val loss 2.181\n",
            "Ep 36 (Step 019250): Train loss 0.869, Val loss 2.188\n",
            "Ep 36 (Step 019300): Train loss 0.834, Val loss 2.178\n",
            "Ep 36 (Step 019350): Train loss 0.838, Val loss 2.179\n",
            "Ep 36 (Step 019400): Train loss 0.823, Val loss 2.182\n",
            "Ep 36 (Step 019450): Train loss 0.852, Val loss 2.168\n",
            "Ep 36 (Step 019500): Train loss 0.853, Val loss 2.167\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The study compares the performance of different models of varying sizes( 1. 5B and 7B parameters) and fixed a 1. 1B model sizes( 1. 8B parameters), showing that larger models trained on more parameters, with a 1 billion\n",
            "Ep 37 (Step 019550): Train loss 0.851, Val loss 2.171\n",
            "Ep 37 (Step 019600): Train loss 0.839, Val loss 2.167\n",
            "Ep 37 (Step 019650): Train loss 0.830, Val loss 2.162\n",
            "Ep 37 (Step 019700): Train loss 0.837, Val loss 2.160\n",
            "Ep 37 (Step 019750): Train loss 0.825, Val loss 2.160\n",
            "Ep 37 (Step 019800): Train loss 0.823, Val loss 2.142\n",
            "Ep 37 (Step 019850): Train loss 0.803, Val loss 2.155\n",
            "Ep 37 (Step 019900): Train loss 0.805, Val loss 2.147\n",
            "Ep 37 (Step 019950): Train loss 0.808, Val loss 2.148\n",
            "Ep 37 (Step 020000): Train loss 0.781, Val loss 2.155\n",
            "Ep 37 (Step 020050): Train loss 0.814, Val loss 2.147\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The study compares the performance of different models of varying sizes( ranging from 1. 5B to 1022 FLOPs) and model sizes( ranging from 1 billion parameters) to 1022 FLOPs and training FLOPs. The study compares the optimal model size for both\n",
            "Ep 38 (Step 020100): Train loss 0.780, Val loss 2.154\n",
            "Ep 38 (Step 020150): Train loss 0.777, Val loss 2.159\n",
            "Ep 38 (Step 020200): Train loss 0.800, Val loss 2.154\n",
            "Ep 38 (Step 020250): Train loss 0.769, Val loss 2.140\n",
            "Ep 38 (Step 020300): Train loss 0.795, Val loss 2.161\n",
            "Ep 38 (Step 020350): Train loss 0.767, Val loss 2.151\n",
            "Ep 38 (Step 020400): Train loss 0.766, Val loss 2.156\n",
            "Ep 38 (Step 020450): Train loss 0.769, Val loss 2.152\n",
            "Ep 38 (Step 020500): Train loss 0.762, Val loss 2.137\n",
            "Ep 38 (Step 020550): Train loss 0.758, Val loss 2.129\n",
            "Ep 38 (Step 020600): Train loss 0.732, Val loss 2.134\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The study compares the optimal model size and model sizes( 1. 5B to 1. 8B parameters) and fixed computational budget, showing that larger models trained on downstream tasks, particularly focusing on downstream tasks. The study also briefly scaling laws that\n",
            "Ep 39 (Step 020650): Train loss 0.727, Val loss 2.124\n",
            "Ep 39 (Step 020700): Train loss 0.744, Val loss 2.130\n",
            "Ep 39 (Step 020750): Train loss 0.735, Val loss 2.130\n",
            "Ep 39 (Step 020800): Train loss 0.735, Val loss 2.130\n",
            "Ep 39 (Step 020850): Train loss 0.721, Val loss 2.129\n",
            "Ep 39 (Step 020900): Train loss 0.720, Val loss 2.120\n",
            "Ep 39 (Step 020950): Train loss 0.733, Val loss 2.117\n",
            "Ep 39 (Step 021000): Train loss 0.721, Val loss 2.133\n",
            "Ep 39 (Step 021050): Train loss 0.714, Val loss 2.127\n",
            "Ep 39 (Step 021100): Train loss 0.708, Val loss 2.111\n",
            "Ep 39 (Step 021150): Train loss 0.712, Val loss 2.109\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The passage discusses scaling laws for a model called Model FLOPs and fixed performance, showing that scaling laws can lead to scaling with larger models and more parameters, as the optimal model size increases. It is noted that while scaling from 1 billion parameters\n",
            "Ep 40 (Step 021200): Train loss 0.714, Val loss 2.116\n",
            "Ep 40 (Step 021250): Train loss 0.679, Val loss 2.113\n",
            "Ep 40 (Step 021300): Train loss 0.683, Val loss 2.126\n",
            "Ep 40 (Step 021350): Train loss 0.678, Val loss 2.109\n",
            "Ep 40 (Step 021400): Train loss 0.685, Val loss 2.112\n",
            "Ep 40 (Step 021450): Train loss 0.693, Val loss 2.112\n",
            "Ep 40 (Step 021500): Train loss 0.689, Val loss 2.111\n",
            "Ep 40 (Step 021550): Train loss 0.670, Val loss 2.111\n",
            "Ep 40 (Step 021600): Train loss 0.691, Val loss 2.102\n",
            "Ep 40 (Step 021650): Train loss 0.659, Val loss 2.098\n",
            "Ep 40 (Step 021700): Train loss 0.675, Val loss 2.098\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The study compares the performance of a model with 9. 3B model, showing that scaling performance improves significantly with larger models and more parameters, indicating that the larger models trained with more parameters and more parameters trained on more data. The study also\n",
            "Ep 41 (Step 021750): Train loss 0.652, Val loss 2.099\n",
            "Ep 41 (Step 021800): Train loss 0.654, Val loss 2.105\n",
            "Ep 41 (Step 021850): Train loss 0.666, Val loss 2.103\n",
            "Ep 41 (Step 021900): Train loss 0.642, Val loss 2.105\n",
            "Ep 41 (Step 021950): Train loss 0.640, Val loss 2.109\n",
            "Ep 41 (Step 022000): Train loss 0.663, Val loss 2.109\n",
            "Ep 41 (Step 022050): Train loss 0.647, Val loss 2.109\n",
            "Ep 41 (Step 022100): Train loss 0.652, Val loss 2.101\n",
            "Ep 41 (Step 022150): Train loss 0.639, Val loss 2.107\n",
            "Ep 41 (Step 022200): Train loss 0.631, Val loss 2.107\n",
            "Ep 41 (Step 022250): Train loss 0.625, Val loss 2.095\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The study compares the optimal model size and model size for a fixed model, showing that scaling performance improves with larger models like 62B and 540B outperforms larger models like 8B and 540B on 9. 9. The 62B model size is comparable to the\n",
            "Ep 42 (Step 022300): Train loss 0.641, Val loss 2.089\n",
            "Ep 42 (Step 022350): Train loss 0.610, Val loss 2.099\n",
            "Ep 42 (Step 022400): Train loss 0.616, Val loss 2.099\n",
            "Ep 42 (Step 022450): Train loss 0.623, Val loss 2.103\n",
            "Ep 42 (Step 022500): Train loss 0.619, Val loss 2.099\n",
            "Ep 42 (Step 022550): Train loss 0.627, Val loss 2.109\n",
            "Ep 42 (Step 022600): Train loss 0.628, Val loss 2.096\n",
            "Ep 42 (Step 022650): Train loss 0.604, Val loss 2.093\n",
            "Ep 42 (Step 022700): Train loss 0.607, Val loss 2.083\n",
            "Ep 42 (Step 022750): Train loss 0.603, Val loss 2.090\n",
            "Ep 42 (Step 022800): Train loss 0.606, Val loss 2.096\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The study compares the optimal model size and model size for a fixed inference and 9. 1. 3B parameter size for 9 billion, showing that the 1. 8B model, 1. 7B model, and 6 billion parameters, while larger models\n",
            "Ep 43 (Step 022850): Train loss 0.569, Val loss 2.093\n",
            "Ep 43 (Step 022900): Train loss 0.583, Val loss 2.090\n",
            "Ep 43 (Step 022950): Train loss 0.564, Val loss 2.086\n",
            "Ep 43 (Step 023000): Train loss 0.585, Val loss 2.082\n",
            "Ep 43 (Step 023050): Train loss 0.609, Val loss 2.090\n",
            "Ep 43 (Step 023100): Train loss 0.553, Val loss 2.082\n",
            "Ep 43 (Step 023150): Train loss 0.576, Val loss 2.068\n",
            "Ep 43 (Step 023200): Train loss 0.577, Val loss 2.070\n",
            "Ep 43 (Step 023250): Train loss 0.579, Val loss 2.094\n",
            "Ep 43 (Step 023300): Train loss 0.563, Val loss 2.081\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The passage discusses the optimal model size and fixed size for a fixed model, showing that scaling performance is comparable to larger models with similar size. It is noted that while scaling from 3. 8B to 62B to 62B, 1 billion parameters trained\n",
            "Ep 44 (Step 023350): Train loss 0.576, Val loss 2.098\n",
            "Ep 44 (Step 023400): Train loss 0.568, Val loss 2.090\n",
            "Ep 44 (Step 023450): Train loss 0.541, Val loss 2.087\n",
            "Ep 44 (Step 023500): Train loss 0.531, Val loss 2.082\n",
            "Ep 44 (Step 023550): Train loss 0.537, Val loss 2.086\n",
            "Ep 44 (Step 023600): Train loss 0.555, Val loss 2.078\n",
            "Ep 44 (Step 023650): Train loss 0.529, Val loss 2.071\n",
            "Ep 44 (Step 023700): Train loss 0.551, Val loss 2.080\n",
            "Ep 44 (Step 023750): Train loss 0.537, Val loss 2.083\n",
            "Ep 44 (Step 023800): Train loss 0.532, Val loss 2.078\n",
            "Ep 44 (Step 023850): Train loss 0.528, Val loss 2.065\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The study compares the performance of a model with 9. 3B parameter model, showing that scaling performance improves significantly to better performance with larger models and training parameters, and the 1. 3B parameters trained on 9. The 1 billion parameter model,\n",
            "Ep 45 (Step 023900): Train loss 0.542, Val loss 2.075\n",
            "Ep 45 (Step 023950): Train loss 0.518, Val loss 2.072\n",
            "Ep 45 (Step 024000): Train loss 0.535, Val loss 2.077\n",
            "Ep 45 (Step 024050): Train loss 0.538, Val loss 2.078\n",
            "Ep 45 (Step 024100): Train loss 0.521, Val loss 2.079\n",
            "Ep 45 (Step 024150): Train loss 0.513, Val loss 2.076\n",
            "Ep 45 (Step 024200): Train loss 0.506, Val loss 2.062\n",
            "Ep 45 (Step 024250): Train loss 0.503, Val loss 2.068\n",
            "Ep 45 (Step 024300): Train loss 0.496, Val loss 2.075\n",
            "Ep 45 (Step 024350): Train loss 0.516, Val loss 2.061\n",
            "Ep 45 (Step 024400): Train loss 0.513, Val loss 2.082\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The study reports results on downstream metrics for downstream tasks, showing that while scaling curves are not sufficient to train smaller models( 1. 5B and 9 billion parameters) show that while larger models have already trained on more data, they train on\n",
            "Ep 46 (Step 024450): Train loss 0.488, Val loss 2.073\n",
            "Ep 46 (Step 024500): Train loss 0.467, Val loss 2.087\n",
            "Ep 46 (Step 024550): Train loss 0.501, Val loss 2.083\n",
            "Ep 46 (Step 024600): Train loss 0.480, Val loss 2.067\n",
            "Ep 46 (Step 024650): Train loss 0.507, Val loss 2.075\n",
            "Ep 46 (Step 024700): Train loss 0.485, Val loss 2.066\n",
            "Ep 46 (Step 024750): Train loss 0.495, Val loss 2.072\n",
            "Ep 46 (Step 024800): Train loss 0.497, Val loss 2.069\n",
            "Ep 46 (Step 024850): Train loss 0.477, Val loss 2.064\n",
            "Ep 46 (Step 024900): Train loss 0.473, Val loss 2.067\n",
            "Ep 46 (Step 024950): Train loss 0.483, Val loss 2.066\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The study compares the performance of a model, showing that scaling curves for 9. 9. 1 × 1022 FLOPs model, and 1. 1B model, showing that the 1. 5 billion parameters generally outperforms 1. 9. The optimal model\n",
            "Ep 47 (Step 025000): Train loss 0.463, Val loss 2.063\n",
            "Ep 47 (Step 025050): Train loss 0.449, Val loss 2.075\n",
            "Ep 47 (Step 025100): Train loss 0.456, Val loss 2.071\n",
            "Ep 47 (Step 025150): Train loss 0.465, Val loss 2.083\n",
            "Ep 47 (Step 025200): Train loss 0.473, Val loss 2.084\n",
            "Ep 47 (Step 025250): Train loss 0.442, Val loss 2.066\n",
            "Ep 47 (Step 025300): Train loss 0.447, Val loss 2.062\n",
            "Ep 47 (Step 025350): Train loss 0.458, Val loss 2.065\n",
            "Ep 47 (Step 025400): Train loss 0.456, Val loss 2.073\n",
            "Ep 47 (Step 025450): Train loss 0.449, Val loss 2.071\n",
            "Ep 47 (Step 025500): Train loss 0.438, Val loss 2.066\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The study compares the optimal model size and model size for a fixed model, showing that scaling laws trained on downstream tasks, and training loss should be trained on downstream tasks, but the 1. 5B model shows that while larger models trained on\n",
            "Ep 48 (Step 025550): Train loss 0.421, Val loss 2.063\n",
            "Ep 48 (Step 025600): Train loss 0.439, Val loss 2.079\n",
            "Ep 48 (Step 025650): Train loss 0.434, Val loss 2.064\n",
            "Ep 48 (Step 025700): Train loss 0.424, Val loss 2.068\n",
            "Ep 48 (Step 025750): Train loss 0.433, Val loss 2.070\n",
            "Ep 48 (Step 025800): Train loss 0.425, Val loss 2.083\n",
            "Ep 48 (Step 025850): Train loss 0.419, Val loss 2.069\n",
            "Ep 48 (Step 025900): Train loss 0.431, Val loss 2.070\n",
            "Ep 48 (Step 025950): Train loss 0.414, Val loss 2.070\n",
            "Ep 48 (Step 026000): Train loss 0.418, Val loss 2.081\n",
            "Ep 48 (Step 026050): Train loss 0.407, Val loss 2.067\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The study compares the performance of a model with 9. 5B model, showing that scaling curves in downstream tasks with a 1. 5B model, showing that the 1 model outperforms 9. 9. 9. 9. 1 billion parameters, and\n",
            "Ep 49 (Step 026100): Train loss 0.409, Val loss 2.067\n",
            "Ep 49 (Step 026150): Train loss 0.395, Val loss 2.073\n",
            "Ep 49 (Step 026200): Train loss 0.400, Val loss 2.057\n",
            "Ep 49 (Step 026250): Train loss 0.400, Val loss 2.064\n",
            "Ep 49 (Step 026300): Train loss 0.409, Val loss 2.078\n",
            "Ep 49 (Step 026350): Train loss 0.402, Val loss 2.068\n",
            "Ep 49 (Step 026400): Train loss 0.410, Val loss 2.069\n",
            "Ep 49 (Step 026450): Train loss 0.397, Val loss 2.072\n",
            "Ep 49 (Step 026500): Train loss 0.410, Val loss 2.074\n",
            "Ep 49 (Step 026550): Train loss 0.398, Val loss 2.068\n",
            "Ep 49 (Step 026600): Train loss 0.407, Val loss 2.082\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The study summarizes the optimal model size and model size for a fixed model, and training loss becomes critical for achieving better performance, indicating that the 1. 5 model trained with 9 billion parameters. The training loss is conducted for 9 billion parameter\n",
            "Ep 50 (Step 026650): Train loss 0.387, Val loss 2.080\n",
            "Ep 50 (Step 026700): Train loss 0.379, Val loss 2.074\n",
            "Ep 50 (Step 026750): Train loss 0.378, Val loss 2.074\n",
            "Ep 50 (Step 026800): Train loss 0.375, Val loss 2.074\n",
            "Ep 50 (Step 026850): Train loss 0.387, Val loss 2.068\n",
            "Ep 50 (Step 026900): Train loss 0.385, Val loss 2.070\n",
            "Ep 50 (Step 026950): Train loss 0.372, Val loss 2.068\n",
            "Ep 50 (Step 027000): Train loss 0.366, Val loss 2.060\n",
            "Ep 50 (Step 027050): Train loss 0.374, Val loss 2.068\n",
            "Ep 50 (Step 027100): Train loss 0.378, Val loss 2.056\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The study compares the performance of different models using a scaling curves in Table 1. 5 to 1022 and 1. The study reports the optimal model size for both the optimal model size and training loss, and the lowest loss is consistent, and\n",
            "Ep 51 (Step 027150): Train loss 0.369, Val loss 2.054\n",
            "Ep 51 (Step 027200): Train loss 0.366, Val loss 2.080\n",
            "Ep 51 (Step 027250): Train loss 0.360, Val loss 2.070\n",
            "Ep 51 (Step 027300): Train loss 0.350, Val loss 2.073\n",
            "Ep 51 (Step 027350): Train loss 0.344, Val loss 2.072\n",
            "Ep 51 (Step 027400): Train loss 0.343, Val loss 2.075\n",
            "Ep 51 (Step 027450): Train loss 0.365, Val loss 2.085\n",
            "Ep 51 (Step 027500): Train loss 0.339, Val loss 2.080\n",
            "Ep 51 (Step 027550): Train loss 0.343, Val loss 2.088\n",
            "Ep 51 (Step 027600): Train loss 0.344, Val loss 2.067\n",
            "Ep 51 (Step 027650): Train loss 0.351, Val loss 2.080\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The study compares the performance of different models using a scaling curves in Table 1. 5B model sizes( 1. 3B parameters) and training loss, showing that scaling from 1. 2 billion parameters trained on downstream tasks, indicating that while scaling\n",
            "Ep 52 (Step 027700): Train loss 0.354, Val loss 2.075\n",
            "Ep 52 (Step 027750): Train loss 0.337, Val loss 2.085\n",
            "Ep 52 (Step 027800): Train loss 0.342, Val loss 2.084\n",
            "Ep 52 (Step 027850): Train loss 0.332, Val loss 2.094\n",
            "Ep 52 (Step 027900): Train loss 0.335, Val loss 2.077\n",
            "Ep 52 (Step 027950): Train loss 0.336, Val loss 2.086\n",
            "Ep 52 (Step 028000): Train loss 0.321, Val loss 2.084\n",
            "Ep 52 (Step 028050): Train loss 0.299, Val loss 2.072\n",
            "Ep 52 (Step 028100): Train loss 0.339, Val loss 2.076\n",
            "Ep 52 (Step 028150): Train loss 0.315, Val loss 2.063\n",
            "Ep 52 (Step 028200): Train loss 0.317, Val loss 2.089\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The passage discusses ablation results for different models of similar sizes and fixed performance varies based on downstream metrics, showing that the 1. 3B model, 1. 3B parameters trained for 9. 8B and 1 65B parameters are comparable to those of the\n",
            "Ep 53 (Step 028250): Train loss 0.312, Val loss 2.087\n",
            "Ep 53 (Step 028300): Train loss 0.313, Val loss 2.079\n",
            "Ep 53 (Step 028350): Train loss 0.315, Val loss 2.077\n",
            "Ep 53 (Step 028400): Train loss 0.317, Val loss 2.079\n",
            "Ep 53 (Step 028450): Train loss 0.311, Val loss 2.079\n",
            "Ep 53 (Step 028500): Train loss 0.301, Val loss 2.075\n",
            "Ep 53 (Step 028550): Train loss 0.316, Val loss 2.089\n",
            "Ep 53 (Step 028600): Train loss 0.317, Val loss 2.084\n",
            "Ep 53 (Step 028650): Train loss 0.299, Val loss 2.068\n",
            "Ep 53 (Step 028700): Train loss 0.305, Val loss 2.072\n",
            "Ep 53 (Step 028750): Train loss 0.311, Val loss 2.087\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The passage discusses the optimal model size and model size for downstream tasks, particularly focusing on downstream tasks, particularly in Table 1. The training loss involves training these models with varying parameter sizes( 1. 8B and 3. 8B parameters) and\n",
            "Ep 54 (Step 028800): Train loss 0.279, Val loss 2.079\n",
            "Ep 54 (Step 028850): Train loss 0.296, Val loss 2.096\n",
            "Ep 54 (Step 028900): Train loss 0.300, Val loss 2.085\n",
            "Ep 54 (Step 028950): Train loss 0.305, Val loss 2.096\n",
            "Ep 54 (Step 029000): Train loss 0.291, Val loss 2.091\n",
            "Ep 54 (Step 029050): Train loss 0.290, Val loss 2.078\n",
            "Ep 54 (Step 029100): Train loss 0.286, Val loss 2.085\n",
            "Ep 54 (Step 029150): Train loss 0.280, Val loss 2.081\n",
            "Ep 54 (Step 029200): Train loss 0.277, Val loss 2.087\n",
            "Ep 54 (Step 029250): Train loss 0.268, Val loss 2.089\n",
            "Ep 54 (Step 029300): Train loss 0.272, Val loss 2.078\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The study compares the performance of a linear correlation between model size and model sizes( 1. 5B and 9. 8B), showing that while training loss becomes less optimal model size, and that while scaling up to 1 billion parameters, and\n",
            "Ep 55 (Step 029350): Train loss 0.282, Val loss 2.089\n",
            "Ep 55 (Step 029400): Train loss 0.280, Val loss 2.100\n",
            "Ep 55 (Step 029450): Train loss 0.275, Val loss 2.095\n",
            "Ep 55 (Step 029500): Train loss 0.281, Val loss 2.096\n",
            "Ep 55 (Step 029550): Train loss 0.266, Val loss 2.101\n",
            "Ep 55 (Step 029600): Train loss 0.270, Val loss 2.099\n",
            "Ep 55 (Step 029650): Train loss 0.273, Val loss 2.090\n",
            "Ep 55 (Step 029700): Train loss 0.285, Val loss 2.101\n",
            "Ep 55 (Step 029750): Train loss 0.263, Val loss 2.101\n",
            "Ep 55 (Step 029800): Train loss 0.264, Val loss 2.090\n",
            "Ep 55 (Step 029850): Train loss 0.266, Val loss 2.085\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The study compares the performance of a model with 9. 5B model, showing that scaling laws not always predict optimal model size to predict how performance loss and downstream metrics, they are based on downstream metrics. However, they note that while scaling\n",
            "Ep 56 (Step 029900): Train loss 0.268, Val loss 2.099\n",
            "Ep 56 (Step 029950): Train loss 0.271, Val loss 2.109\n",
            "Ep 56 (Step 030000): Train loss 0.239, Val loss 2.103\n",
            "Ep 56 (Step 030050): Train loss 0.246, Val loss 2.090\n",
            "Ep 56 (Step 030100): Train loss 0.247, Val loss 2.100\n",
            "Ep 56 (Step 030150): Train loss 0.255, Val loss 2.104\n",
            "Ep 56 (Step 030200): Train loss 0.246, Val loss 2.110\n",
            "Ep 56 (Step 030250): Train loss 0.256, Val loss 2.108\n",
            "Ep 56 (Step 030300): Train loss 0.241, Val loss 2.100\n",
            "Ep 56 (Step 030350): Train loss 0.252, Val loss 2.102\n",
            "Ep 56 (Step 030400): Train loss 0.248, Val loss 2.112\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The study compares the optimal model size and model size for a particular model, training loss and model size fixed computational resources. It finds that while training loss outperforms larger models like Mixtral and trained on specific downstream tasks, the 1. 3B model\n",
            "Ep 57 (Step 030450): Train loss 0.238, Val loss 2.123\n",
            "Ep 57 (Step 030500): Train loss 0.242, Val loss 2.112\n",
            "Ep 57 (Step 030550): Train loss 0.238, Val loss 2.109\n",
            "Ep 57 (Step 030600): Train loss 0.235, Val loss 2.125\n",
            "Ep 57 (Step 030650): Train loss 0.234, Val loss 2.109\n",
            "Ep 57 (Step 030700): Train loss 0.235, Val loss 2.121\n",
            "Ep 57 (Step 030750): Train loss 0.228, Val loss 2.120\n",
            "Ep 57 (Step 030800): Train loss 0.223, Val loss 2.111\n",
            "Ep 57 (Step 030850): Train loss 0.225, Val loss 2.100\n",
            "Ep 57 (Step 030900): Train loss 0.228, Val loss 2.103\n",
            "Ep 57 (Step 030950): Train loss 0.230, Val loss 2.121\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The study compares the optimal model size and model size for downstream tasks, including scaling laws trained on downstream tasks with similar size and training parameters. It finds that while scaling laws are typically similar to the optimal model size, slightly expanded to better\n",
            "Ep 58 (Step 031000): Train loss 0.225, Val loss 2.109\n",
            "Ep 58 (Step 031050): Train loss 0.225, Val loss 2.132\n",
            "Ep 58 (Step 031100): Train loss 0.219, Val loss 2.120\n",
            "Ep 58 (Step 031150): Train loss 0.222, Val loss 2.121\n",
            "Ep 58 (Step 031200): Train loss 0.221, Val loss 2.115\n",
            "Ep 58 (Step 031250): Train loss 0.219, Val loss 2.126\n",
            "Ep 58 (Step 031300): Train loss 0.210, Val loss 2.113\n",
            "Ep 58 (Step 031350): Train loss 0.205, Val loss 2.119\n",
            "Ep 58 (Step 031400): Train loss 0.221, Val loss 2.116\n",
            "Ep 58 (Step 031450): Train loss 0.214, Val loss 2.123\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The study compares the optimal model size and model size for a particular training loss and model to determine optimal model size for downstream metrics, as evidenced by 9. 5B model, 1. 7B parameters on 9. The lowest loss is observed for\n",
            "Ep 59 (Step 031500): Train loss 0.228, Val loss 2.129\n",
            "Ep 59 (Step 031550): Train loss 0.204, Val loss 2.124\n",
            "Ep 59 (Step 031600): Train loss 0.209, Val loss 2.120\n",
            "Ep 59 (Step 031650): Train loss 0.202, Val loss 2.123\n",
            "Ep 59 (Step 031700): Train loss 0.218, Val loss 2.130\n",
            "Ep 59 (Step 031750): Train loss 0.213, Val loss 2.140\n",
            "Ep 59 (Step 031800): Train loss 0.210, Val loss 2.132\n",
            "Ep 59 (Step 031850): Train loss 0.202, Val loss 2.137\n",
            "Ep 59 (Step 031900): Train loss 0.207, Val loss 2.125\n",
            "Ep 59 (Step 031950): Train loss 0.195, Val loss 2.116\n",
            "Ep 59 (Step 032000): Train loss 0.202, Val loss 2.125\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The study compares the optimal model size and scaling laws for downstream metrics, including scaling laws with optimal model size and downstream metrics like learning rate and training loss while maintaining optimal model size becomes crucial. However, they observed that while scaling generally perform\n",
            "Ep 60 (Step 032050): Train loss 0.194, Val loss 2.116\n",
            "Ep 60 (Step 032100): Train loss 0.199, Val loss 2.134\n",
            "Ep 60 (Step 032150): Train loss 0.185, Val loss 2.140\n",
            "Ep 60 (Step 032200): Train loss 0.186, Val loss 2.136\n",
            "Ep 60 (Step 032250): Train loss 0.186, Val loss 2.125\n",
            "Ep 60 (Step 032300): Train loss 0.198, Val loss 2.143\n",
            "Ep 60 (Step 032350): Train loss 0.179, Val loss 2.132\n",
            "Ep 60 (Step 032400): Train loss 0.197, Val loss 2.132\n",
            "Ep 60 (Step 032450): Train loss 0.186, Val loss 2.139\n",
            "Ep 60 (Step 032500): Train loss 0.186, Val loss 2.135\n",
            "Ep 60 (Step 032550): Train loss 0.195, Val loss 2.138\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction : Summarize the following passage in <|unk|> sentences. ### Input : We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9. 5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9. 5B model, which shows the lowest loss( in Table 1) and is closest to the optimal model, slightly underperforms the 16. 1B model on downstream tasks. This suggests that while scaling laws can be used to ### Response : The passage discusses ablation results for downstream metrics for a given model, showing that training loss and downstream metrics, particularly focusing on downstream metrics. The study compares the scaling model and the optimal model size while maintaining similar performance across different downstream metrics.\n",
            "🟦 saved → models/regex_finetuned.pth\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "gpt2 = False\n",
        "model_regex = model_regex.to(device)\n",
        "model_regex.train()\n",
        "\n",
        "optimizer_regex = torch.optim.AdamW(model_regex.parameters(), lr=5e-5, weight_decay=0.1)\n",
        "\n",
        "torch.manual_seed(123)\n",
        "train_losses_rx, val_losses_rx, tokens_rx = train_model_simple(\n",
        "    model_regex, train_loader_regex, val_loader_regex, optimizer_regex, device,\n",
        "    num_epochs=60, eval_freq=50, eval_iter=20,\n",
        "    start_context=format_input(val_set[0]), tokenizer=tokenizer_v2)\n",
        "\n",
        "torch.save(model_regex.state_dict(), \"models/regex_finetuned.pth\")\n",
        "print(\"🟦 saved → models/regex_finetuned.pth\")\n",
        "\n",
        "epochs_tensor = torch.linspace(0, 60, len(train_losses_rx))\n",
        "plot_losses(epochs_tensor, tokens_rx, train_losses_rx, val_losses_rx)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2be15b38",
      "metadata": {},
      "source": [
        "### Step 3: Fine-Tuning the GPT-2 Model\n",
        "\n",
        "- Saves the fine-tuned checkpoint as **`gpt2_finetuned.pth`**.  \n",
        "- Plots both **training** and **validation loss curves** across epochs.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "10cfb3f0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep 1 (Step 000000): Train loss 5.339, Val loss 5.312\n",
            "Ep 1 (Step 000050): Train loss 4.353, Val loss 4.392\n",
            "Ep 1 (Step 000100): Train loss 4.293, Val loss 4.267\n",
            "Ep 1 (Step 000150): Train loss 4.149, Val loss 4.209\n",
            "Ep 1 (Step 000200): Train loss 4.150, Val loss 4.187\n",
            "Ep 1 (Step 000250): Train loss 3.987, Val loss 4.134\n",
            "Ep 1 (Step 000300): Train loss 4.191, Val loss 4.116\n",
            "Ep 1 (Step 000350): Train loss 4.024, Val loss 4.087\n",
            "Ep 1 (Step 000400): Train loss 3.953, Val loss 4.038\n",
            "Ep 1 (Step 000450): Train loss 3.910, Val loss 4.049\n",
            "Ep 1 (Step 000500): Train loss 3.956, Val loss 4.014\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to the model to the model, we use of the model to the model to the model to the model, we use of the model to the model to the model to the model to the model to the model to the model to the model to the model\n",
            "Ep 2 (Step 000550): Train loss 3.884, Val loss 3.998\n",
            "Ep 2 (Step 000600): Train loss 3.893, Val loss 3.978\n",
            "Ep 2 (Step 000650): Train loss 3.944, Val loss 3.967\n",
            "Ep 2 (Step 000700): Train loss 3.896, Val loss 3.946\n",
            "Ep 2 (Step 000750): Train loss 3.894, Val loss 3.926\n",
            "Ep 2 (Step 000800): Train loss 3.819, Val loss 3.923\n",
            "Ep 2 (Step 000850): Train loss 3.804, Val loss 3.911\n",
            "Ep 2 (Step 000900): Train loss 3.736, Val loss 3.887\n",
            "Ep 2 (Step 000950): Train loss 3.789, Val loss 3.857\n",
            "Ep 2 (Step 001000): Train loss 3.766, Val loss 3.860\n",
            "Ep 2 (Step 001050): Train loss 3.750, Val loss 3.838\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to the model performance of the model performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of\n",
            "Ep 3 (Step 001100): Train loss 3.643, Val loss 3.826\n",
            "Ep 3 (Step 001150): Train loss 3.652, Val loss 3.815\n",
            "Ep 3 (Step 001200): Train loss 3.611, Val loss 3.795\n",
            "Ep 3 (Step 001250): Train loss 3.616, Val loss 3.787\n",
            "Ep 3 (Step 001300): Train loss 3.631, Val loss 3.770\n",
            "Ep 3 (Step 001350): Train loss 3.611, Val loss 3.777\n",
            "Ep 3 (Step 001400): Train loss 3.515, Val loss 3.743\n",
            "Ep 3 (Step 001450): Train loss 3.539, Val loss 3.722\n",
            "Ep 3 (Step 001500): Train loss 3.515, Val loss 3.717\n",
            "Ep 3 (Step 001550): Train loss 3.557, Val loss 3.707\n",
            "Ep 3 (Step 001600): Train loss 3.499, Val loss 3.692\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to the model performance on the model performance of the performance on the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of\n",
            "Ep 4 (Step 001650): Train loss 3.559, Val loss 3.695\n",
            "Ep 4 (Step 001700): Train loss 3.468, Val loss 3.674\n",
            "Ep 4 (Step 001750): Train loss 3.473, Val loss 3.670\n",
            "Ep 4 (Step 001800): Train loss 3.501, Val loss 3.661\n",
            "Ep 4 (Step 001850): Train loss 3.399, Val loss 3.647\n",
            "Ep 4 (Step 001900): Train loss 3.426, Val loss 3.638\n",
            "Ep 4 (Step 001950): Train loss 3.396, Val loss 3.613\n",
            "Ep 4 (Step 002000): Train loss 3.423, Val loss 3.607\n",
            "Ep 4 (Step 002050): Train loss 3.403, Val loss 3.605\n",
            "Ep 4 (Step 002100): Train loss 3.314, Val loss 3.595\n",
            "Ep 4 (Step 002150): Train loss 3.360, Val loss 3.570\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance\n",
            "Ep 5 (Step 002200): Train loss 3.273, Val loss 3.551\n",
            "Ep 5 (Step 002250): Train loss 3.301, Val loss 3.561\n",
            "Ep 5 (Step 002300): Train loss 3.317, Val loss 3.528\n",
            "Ep 5 (Step 002350): Train loss 3.324, Val loss 3.534\n",
            "Ep 5 (Step 002400): Train loss 3.268, Val loss 3.529\n",
            "Ep 5 (Step 002450): Train loss 3.298, Val loss 3.507\n",
            "Ep 5 (Step 002500): Train loss 3.217, Val loss 3.506\n",
            "Ep 5 (Step 002550): Train loss 3.265, Val loss 3.492\n",
            "Ep 5 (Step 002600): Train loss 3.159, Val loss 3.481\n",
            "Ep 5 (Step 002650): Train loss 3.219, Val loss 3.477\n",
            "Ep 5 (Step 002700): Train loss 3.139, Val loss 3.471\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to the training data is not only on the training data.  ### Response: The study compares the training data is trained on the training data, which is trained on the training data. The training data is trained on the training data, the training\n",
            "Ep 6 (Step 002750): Train loss 3.208, Val loss 3.473\n",
            "Ep 6 (Step 002800): Train loss 3.185, Val loss 3.434\n",
            "Ep 6 (Step 002850): Train loss 3.188, Val loss 3.460\n",
            "Ep 6 (Step 002900): Train loss 3.162, Val loss 3.436\n",
            "Ep 6 (Step 002950): Train loss 3.130, Val loss 3.433\n",
            "Ep 6 (Step 003000): Train loss 3.146, Val loss 3.416\n",
            "Ep 6 (Step 003050): Train loss 3.105, Val loss 3.407\n",
            "Ep 6 (Step 003100): Train loss 3.089, Val loss 3.403\n",
            "Ep 6 (Step 003150): Train loss 3.059, Val loss 3.374\n",
            "Ep 6 (Step 003200): Train loss 3.031, Val loss 3.393\n",
            "Ep 6 (Step 003250): Train loss 3.092, Val loss 3.369\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to the performance of the performance of the performance of the performance of the performance of the performance of the performance.  ### Response: The study compares the performance of the performance of the performance of the performance of the performance of the performance of the performance\n",
            "Ep 7 (Step 003300): Train loss 3.076, Val loss 3.380\n",
            "Ep 7 (Step 003350): Train loss 3.084, Val loss 3.370\n",
            "Ep 7 (Step 003400): Train loss 3.066, Val loss 3.363\n",
            "Ep 7 (Step 003450): Train loss 3.023, Val loss 3.358\n",
            "Ep 7 (Step 003500): Train loss 3.043, Val loss 3.346\n",
            "Ep 7 (Step 003550): Train loss 2.949, Val loss 3.332\n",
            "Ep 7 (Step 003600): Train loss 2.968, Val loss 3.325\n",
            "Ep 7 (Step 003650): Train loss 2.995, Val loss 3.321\n",
            "Ep 7 (Step 003700): Train loss 2.913, Val loss 3.314\n",
            "Ep 7 (Step 003750): Train loss 2.929, Val loss 3.298\n",
            "Ep 7 (Step 003800): Train loss 2.911, Val loss 3.288\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance of the performance.  ### Response: The study\n",
            "Ep 8 (Step 003850): Train loss 2.858, Val loss 3.266\n",
            "Ep 8 (Step 003900): Train loss 2.874, Val loss 3.272\n",
            "Ep 8 (Step 003950): Train loss 2.917, Val loss 3.294\n",
            "Ep 8 (Step 004000): Train loss 2.909, Val loss 3.263\n",
            "Ep 8 (Step 004050): Train loss 2.883, Val loss 3.243\n",
            "Ep 8 (Step 004100): Train loss 2.947, Val loss 3.244\n",
            "Ep 8 (Step 004150): Train loss 2.839, Val loss 3.236\n",
            "Ep 8 (Step 004200): Train loss 2.953, Val loss 3.227\n",
            "Ep 8 (Step 004250): Train loss 2.853, Val loss 3.233\n",
            "Ep 8 (Step 004300): Train loss 2.799, Val loss 3.225\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to the model size of the model size and performance on the model size.  ### Response: The study compares the performance of the performance of the performance of the performance of the model size, and the model size. The model is trained on the\n",
            "Ep 9 (Step 004350): Train loss 2.874, Val loss 3.213\n",
            "Ep 9 (Step 004400): Train loss 2.806, Val loss 3.213\n",
            "Ep 9 (Step 004450): Train loss 2.822, Val loss 3.212\n",
            "Ep 9 (Step 004500): Train loss 2.876, Val loss 3.201\n",
            "Ep 9 (Step 004550): Train loss 2.771, Val loss 3.197\n",
            "Ep 9 (Step 004600): Train loss 2.809, Val loss 3.180\n",
            "Ep 9 (Step 004650): Train loss 2.826, Val loss 3.185\n",
            "Ep 9 (Step 004700): Train loss 2.783, Val loss 3.174\n",
            "Ep 9 (Step 004750): Train loss 2.785, Val loss 3.171\n",
            "Ep 9 (Step 004800): Train loss 2.721, Val loss 3.141\n",
            "Ep 9 (Step 004850): Train loss 2.742, Val loss 3.131\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to the performance of the model size increases the smaller models.  ### Response: The study compares scaling models, which is trained on a larger model, with larger models like GPT-3.5 and GPT-3.5, the\n",
            "Ep 10 (Step 004900): Train loss 2.698, Val loss 3.129\n",
            "Ep 10 (Step 004950): Train loss 2.716, Val loss 3.130\n",
            "Ep 10 (Step 005000): Train loss 2.680, Val loss 3.130\n",
            "Ep 10 (Step 005050): Train loss 2.616, Val loss 3.137\n",
            "Ep 10 (Step 005100): Train loss 2.668, Val loss 3.125\n",
            "Ep 10 (Step 005150): Train loss 2.614, Val loss 3.117\n",
            "Ep 10 (Step 005200): Train loss 2.645, Val loss 3.104\n",
            "Ep 10 (Step 005250): Train loss 2.641, Val loss 3.104\n",
            "Ep 10 (Step 005300): Train loss 2.592, Val loss 3.092\n",
            "Ep 10 (Step 005350): Train loss 2.615, Val loss 3.085\n",
            "Ep 10 (Step 005400): Train loss 2.663, Val loss 3.080\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to the performance of the model size increases the performance of the model.  ### Response: The study compares scaling model, with larger models, and performance on a smaller model, with larger models trained on a smaller model, and 1.5B\n",
            "Ep 11 (Step 005450): Train loss 2.636, Val loss 3.078\n",
            "Ep 11 (Step 005500): Train loss 2.610, Val loss 3.067\n",
            "Ep 11 (Step 005550): Train loss 2.528, Val loss 3.096\n",
            "Ep 11 (Step 005600): Train loss 2.552, Val loss 3.049\n",
            "Ep 11 (Step 005650): Train loss 2.568, Val loss 3.050\n",
            "Ep 11 (Step 005700): Train loss 2.583, Val loss 3.044\n",
            "Ep 11 (Step 005750): Train loss 2.599, Val loss 3.031\n",
            "Ep 11 (Step 005800): Train loss 2.520, Val loss 3.030\n",
            "Ep 11 (Step 005850): Train loss 2.518, Val loss 3.026\n",
            "Ep 11 (Step 005900): Train loss 2.524, Val loss 3.030\n",
            "Ep 11 (Step 005950): Train loss 2.515, Val loss 3.010\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to the performance of the model size is not only 1.  ### Response: The study compares the performance of training data contamination and performance on a larger model, with larger models, with larger models like GPT-3B and GPT-\n",
            "Ep 12 (Step 006000): Train loss 2.517, Val loss 3.015\n",
            "Ep 12 (Step 006050): Train loss 2.534, Val loss 3.007\n",
            "Ep 12 (Step 006100): Train loss 2.508, Val loss 3.004\n",
            "Ep 12 (Step 006150): Train loss 2.472, Val loss 2.985\n",
            "Ep 12 (Step 006200): Train loss 2.516, Val loss 2.997\n",
            "Ep 12 (Step 006250): Train loss 2.487, Val loss 2.984\n",
            "Ep 12 (Step 006300): Train loss 2.417, Val loss 2.975\n",
            "Ep 12 (Step 006350): Train loss 2.537, Val loss 2.985\n",
            "Ep 12 (Step 006400): Train loss 2.416, Val loss 2.968\n",
            "Ep 12 (Step 006450): Train loss 2.377, Val loss 2.963\n",
            "Ep 12 (Step 006500): Train loss 2.371, Val loss 2.950\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to the training data is more efficient.  ### Response: The study compares scaling laws for training data contamination, which is a smaller model with a smaller model trained on a smaller model, with a smaller model trained on a smaller model. Specifically,\n",
            "Ep 13 (Step 006550): Train loss 2.412, Val loss 2.961\n",
            "Ep 13 (Step 006600): Train loss 2.380, Val loss 2.943\n",
            "Ep 13 (Step 006650): Train loss 2.346, Val loss 2.940\n",
            "Ep 13 (Step 006700): Train loss 2.384, Val loss 2.947\n",
            "Ep 13 (Step 006750): Train loss 2.401, Val loss 2.941\n",
            "Ep 13 (Step 006800): Train loss 2.337, Val loss 2.922\n",
            "Ep 13 (Step 006850): Train loss 2.340, Val loss 2.919\n",
            "Ep 13 (Step 006900): Train loss 2.338, Val loss 2.919\n",
            "Ep 13 (Step 006950): Train loss 2.300, Val loss 2.913\n",
            "Ep 13 (Step 007000): Train loss 2.284, Val loss 2.884\n",
            "Ep 13 (Step 007050): Train loss 2.278, Val loss 2.903\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to the performance of the model size increases.  ### Response: The study compares the scaling laws of scaling model sizes, showing that larger models on a smaller model, showing that larger models trained on a smaller model trained on a larger batch size of\n",
            "Ep 14 (Step 007100): Train loss 2.309, Val loss 2.880\n",
            "Ep 14 (Step 007150): Train loss 2.351, Val loss 2.902\n",
            "Ep 14 (Step 007200): Train loss 2.292, Val loss 2.882\n",
            "Ep 14 (Step 007250): Train loss 2.312, Val loss 2.884\n",
            "Ep 14 (Step 007300): Train loss 2.301, Val loss 2.868\n",
            "Ep 14 (Step 007350): Train loss 2.226, Val loss 2.861\n",
            "Ep 14 (Step 007400): Train loss 2.258, Val loss 2.859\n",
            "Ep 14 (Step 007450): Train loss 2.237, Val loss 2.848\n",
            "Ep 14 (Step 007500): Train loss 2.236, Val loss 2.848\n",
            "Ep 14 (Step 007550): Train loss 2.259, Val loss 2.843\n",
            "Ep 14 (Step 007600): Train loss 2.188, Val loss 2.840\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to the model size is more parameters.  ### Response: The study compares scaling performance of scaling performance, showing that larger models like the larger models trained on a smaller model trained on 1.5 trillion tokens, and 1. The 1.5\n",
            "Ep 15 (Step 007650): Train loss 2.221, Val loss 2.830\n",
            "Ep 15 (Step 007700): Train loss 2.202, Val loss 2.827\n",
            "Ep 15 (Step 007750): Train loss 2.196, Val loss 2.810\n",
            "Ep 15 (Step 007800): Train loss 2.187, Val loss 2.815\n",
            "Ep 15 (Step 007850): Train loss 2.243, Val loss 2.822\n",
            "Ep 15 (Step 007900): Train loss 2.214, Val loss 2.827\n",
            "Ep 15 (Step 007950): Train loss 2.162, Val loss 2.800\n",
            "Ep 15 (Step 008000): Train loss 2.195, Val loss 2.806\n",
            "Ep 15 (Step 008050): Train loss 2.165, Val loss 2.808\n",
            "Ep 15 (Step 008100): Train loss 2.146, Val loss 2.814\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to be more efficient.  ### Response: The study compares scaling performance improvements and performance improvements over the larger models on a larger model, with a smaller models trained on a smaller model, with a smaller model, and the larger model, and the\n",
            "Ep 16 (Step 008150): Train loss 2.160, Val loss 2.787\n",
            "Ep 16 (Step 008200): Train loss 2.104, Val loss 2.768\n",
            "Ep 16 (Step 008250): Train loss 2.111, Val loss 2.787\n",
            "Ep 16 (Step 008300): Train loss 2.079, Val loss 2.770\n",
            "Ep 16 (Step 008350): Train loss 2.172, Val loss 2.772\n",
            "Ep 16 (Step 008400): Train loss 2.103, Val loss 2.768\n",
            "Ep 16 (Step 008450): Train loss 2.078, Val loss 2.758\n",
            "Ep 16 (Step 008500): Train loss 2.094, Val loss 2.746\n",
            "Ep 16 (Step 008550): Train loss 2.045, Val loss 2.754\n",
            "Ep 16 (Step 008600): Train loss 2.057, Val loss 2.742\n",
            "Ep 16 (Step 008650): Train loss 2.039, Val loss 2.741\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to be more efficient.  ### Response: The study compares scaling laws to train a smaller models on a smaller model on a smaller model, showing that larger models like GPT-3.5 and Chinchilla. The study compares the scaling\n",
            "Ep 17 (Step 008700): Train loss 2.101, Val loss 2.725\n",
            "Ep 17 (Step 008750): Train loss 2.030, Val loss 2.730\n",
            "Ep 17 (Step 008800): Train loss 1.982, Val loss 2.741\n",
            "Ep 17 (Step 008850): Train loss 2.034, Val loss 2.725\n",
            "Ep 17 (Step 008900): Train loss 2.057, Val loss 2.717\n",
            "Ep 17 (Step 008950): Train loss 1.982, Val loss 2.719\n",
            "Ep 17 (Step 009000): Train loss 2.022, Val loss 2.706\n",
            "Ep 17 (Step 009050): Train loss 2.003, Val loss 2.711\n",
            "Ep 17 (Step 009100): Train loss 1.951, Val loss 2.710\n",
            "Ep 17 (Step 009150): Train loss 1.990, Val loss 2.697\n",
            "Ep 17 (Step 009200): Train loss 1.976, Val loss 2.680\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to be more training compute budget.  ### Response: The study compares scaling models on a 1.5B model on a 1.5B model with 1.8B parameters, showing that larger models trained on a 1.8B model\n",
            "Ep 18 (Step 009250): Train loss 1.994, Val loss 2.678\n",
            "Ep 18 (Step 009300): Train loss 1.984, Val loss 2.689\n",
            "Ep 18 (Step 009350): Train loss 1.970, Val loss 2.682\n",
            "Ep 18 (Step 009400): Train loss 1.937, Val loss 2.680\n",
            "Ep 18 (Step 009450): Train loss 1.910, Val loss 2.675\n",
            "Ep 18 (Step 009500): Train loss 1.916, Val loss 2.654\n",
            "Ep 18 (Step 009550): Train loss 1.905, Val loss 2.657\n",
            "Ep 18 (Step 009600): Train loss 1.954, Val loss 2.656\n",
            "Ep 18 (Step 009650): Train loss 1.886, Val loss 2.649\n",
            "Ep 18 (Step 009700): Train loss 1.894, Val loss 2.634\n",
            "Ep 18 (Step 009750): Train loss 1.913, Val loss 2.647\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to be more training compute budget.  ### Response: The study compares scaling performance of scaling laws for models with 1.5B and 1.5B models on 1.8B and 1.0B parameters, showing that larger models like\n",
            "Ep 19 (Step 009800): Train loss 1.817, Val loss 2.647\n",
            "Ep 19 (Step 009850): Train loss 1.905, Val loss 2.640\n",
            "Ep 19 (Step 009900): Train loss 1.880, Val loss 2.643\n",
            "Ep 19 (Step 009950): Train loss 1.852, Val loss 2.629\n",
            "Ep 19 (Step 010000): Train loss 1.892, Val loss 2.628\n",
            "Ep 19 (Step 010050): Train loss 1.871, Val loss 2.623\n",
            "Ep 19 (Step 010100): Train loss 1.845, Val loss 2.632\n",
            "Ep 19 (Step 010150): Train loss 1.826, Val loss 2.620\n",
            "Ep 19 (Step 010200): Train loss 1.829, Val loss 2.607\n",
            "Ep 19 (Step 010250): Train loss 1.795, Val loss 2.604\n",
            "Ep 19 (Step 010300): Train loss 1.796, Val loss 2.592\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to be more efficient.  ### Response: The study compares scaling laws for a 1.5B model, showing that larger models like GPT-3 and Chinchilla.5, showing that larger models like GPT-3.5\n",
            "Ep 20 (Step 010350): Train loss 1.780, Val loss 2.594\n",
            "Ep 20 (Step 010400): Train loss 1.794, Val loss 2.602\n",
            "Ep 20 (Step 010450): Train loss 1.805, Val loss 2.588\n",
            "Ep 20 (Step 010500): Train loss 1.816, Val loss 2.593\n",
            "Ep 20 (Step 010550): Train loss 1.701, Val loss 2.575\n",
            "Ep 20 (Step 010600): Train loss 1.757, Val loss 2.561\n",
            "Ep 20 (Step 010650): Train loss 1.785, Val loss 2.584\n",
            "Ep 20 (Step 010700): Train loss 1.763, Val loss 2.575\n",
            "Ep 20 (Step 010750): Train loss 1.771, Val loss 2.578\n",
            "Ep 20 (Step 010800): Train loss 1.703, Val loss 2.570\n",
            "Ep 20 (Step 010850): Train loss 1.744, Val loss 2.572\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to train a model on a smaller model.  ### Response: The study compares the scaling laws of scaling model sizes of the larger models with 1 billion parameters, showing that larger models trained on a 1 billion parameter model, and 1 billion parameters,\n",
            "Ep 21 (Step 010900): Train loss 1.708, Val loss 2.555\n",
            "Ep 21 (Step 010950): Train loss 1.713, Val loss 2.548\n",
            "Ep 21 (Step 011000): Train loss 1.685, Val loss 2.557\n",
            "Ep 21 (Step 011050): Train loss 1.680, Val loss 2.550\n",
            "Ep 21 (Step 011100): Train loss 1.702, Val loss 2.564\n",
            "Ep 21 (Step 011150): Train loss 1.724, Val loss 2.555\n",
            "Ep 21 (Step 011200): Train loss 1.701, Val loss 2.552\n",
            "Ep 21 (Step 011250): Train loss 1.681, Val loss 2.546\n",
            "Ep 21 (Step 011300): Train loss 1.722, Val loss 2.528\n",
            "Ep 21 (Step 011350): Train loss 1.643, Val loss 2.511\n",
            "Ep 21 (Step 011400): Train loss 1.627, Val loss 2.520\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to be more efficient.  ### Response: The study compares scaling laws with a 1.5B model with 1.5 billion parameters and 1.0B parameters, showing that larger models trained on a smaller model, with a 1.8\n",
            "Ep 22 (Step 011450): Train loss 1.633, Val loss 2.525\n",
            "Ep 22 (Step 011500): Train loss 1.648, Val loss 2.517\n",
            "Ep 22 (Step 011550): Train loss 1.630, Val loss 2.509\n",
            "Ep 22 (Step 011600): Train loss 1.594, Val loss 2.501\n",
            "Ep 22 (Step 011650): Train loss 1.646, Val loss 2.508\n",
            "Ep 22 (Step 011700): Train loss 1.631, Val loss 2.512\n",
            "Ep 22 (Step 011750): Train loss 1.647, Val loss 2.504\n",
            "Ep 22 (Step 011800): Train loss 1.619, Val loss 2.495\n",
            "Ep 22 (Step 011850): Train loss 1.629, Val loss 2.499\n",
            "Ep 22 (Step 011900): Train loss 1.626, Val loss 2.486\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to train a model with a smaller model, training loss is more training loss and to train a smaller model.  ### Response: The study compares the scaling laws for 1.5 models with varying parameter models, showing that larger models like GPT\n",
            "Ep 23 (Step 011950): Train loss 1.584, Val loss 2.477\n",
            "Ep 23 (Step 012000): Train loss 1.604, Val loss 2.466\n",
            "Ep 23 (Step 012050): Train loss 1.571, Val loss 2.474\n",
            "Ep 23 (Step 012100): Train loss 1.572, Val loss 2.472\n",
            "Ep 23 (Step 012150): Train loss 1.569, Val loss 2.469\n",
            "Ep 23 (Step 012200): Train loss 1.537, Val loss 2.466\n",
            "Ep 23 (Step 012250): Train loss 1.549, Val loss 2.464\n",
            "Ep 23 (Step 012300): Train loss 1.513, Val loss 2.456\n",
            "Ep 23 (Step 012350): Train loss 1.520, Val loss 2.461\n",
            "Ep 23 (Step 012400): Train loss 1.525, Val loss 2.459\n",
            "Ep 23 (Step 012450): Train loss 1.526, Val loss 2.456\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to train a model with a smaller model (i.e., training loss and the 34B model).  ### Response: The study compares the scaling curves for a scaling laws for the optimal model and training loss for a 1.5B model\n",
            "Ep 24 (Step 012500): Train loss 1.489, Val loss 2.442\n",
            "Ep 24 (Step 012550): Train loss 1.518, Val loss 2.450\n",
            "Ep 24 (Step 012600): Train loss 1.526, Val loss 2.436\n",
            "Ep 24 (Step 012650): Train loss 1.514, Val loss 2.441\n",
            "Ep 24 (Step 012700): Train loss 1.479, Val loss 2.427\n",
            "Ep 24 (Step 012750): Train loss 1.499, Val loss 2.431\n",
            "Ep 24 (Step 012800): Train loss 1.466, Val loss 2.425\n",
            "Ep 24 (Step 012850): Train loss 1.486, Val loss 2.441\n",
            "Ep 24 (Step 012900): Train loss 1.475, Val loss 2.420\n",
            "Ep 24 (Step 012950): Train loss 1.455, Val loss 2.437\n",
            "Ep 24 (Step 013000): Train loss 1.405, Val loss 2.417\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to train a model with a smaller model (i.e., training loss).  ### Response: The study compares the performance of different model sizes of the 34B model on a 1.8B model, showing that larger models, and 1\n",
            "Ep 25 (Step 013050): Train loss 1.455, Val loss 2.408\n",
            "Ep 25 (Step 013100): Train loss 1.441, Val loss 2.408\n",
            "Ep 25 (Step 013150): Train loss 1.433, Val loss 2.417\n",
            "Ep 25 (Step 013200): Train loss 1.412, Val loss 2.395\n",
            "Ep 25 (Step 013250): Train loss 1.436, Val loss 2.404\n",
            "Ep 25 (Step 013300): Train loss 1.420, Val loss 2.400\n",
            "Ep 25 (Step 013350): Train loss 1.424, Val loss 2.391\n",
            "Ep 25 (Step 013400): Train loss 1.401, Val loss 2.390\n",
            "Ep 25 (Step 013450): Train loss 1.395, Val loss 2.383\n",
            "Ep 25 (Step 013500): Train loss 1.394, Val loss 2.389\n",
            "Ep 25 (Step 013550): Train loss 1.387, Val loss 2.367\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to train a model on downstream tasks (Section 2.1.1.1.1.1), and (2), and2). Notably, the scaling laws are not yet to be due to the optimal model size, the training cost is not yet\n",
            "Ep 26 (Step 013600): Train loss 1.333, Val loss 2.372\n",
            "Ep 26 (Step 013650): Train loss 1.410, Val loss 2.380\n",
            "Ep 26 (Step 013700): Train loss 1.334, Val loss 2.357\n",
            "Ep 26 (Step 013750): Train loss 1.369, Val loss 2.372\n",
            "Ep 26 (Step 013800): Train loss 1.375, Val loss 2.366\n",
            "Ep 26 (Step 013850): Train loss 1.325, Val loss 2.360\n",
            "Ep 26 (Step 013900): Train loss 1.351, Val loss 2.363\n",
            "Ep 26 (Step 013950): Train loss 1.311, Val loss 2.350\n",
            "Ep 26 (Step 014000): Train loss 1.331, Val loss 2.359\n",
            "Ep 26 (Step 014050): Train loss 1.338, Val loss 2.343\n",
            "Ep 26 (Step 014100): Train loss 1.277, Val loss 2.342\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to be more efficient.  ### Response: The study compares the scaling curves for a 1-shot and 1.5B model, showing that larger models trained for 1.5B and 1.5B parameters, showing that larger models like\n",
            "Ep 27 (Step 014150): Train loss 1.341, Val loss 2.335\n",
            "Ep 27 (Step 014200): Train loss 1.321, Val loss 2.347\n",
            "Ep 27 (Step 014250): Train loss 1.285, Val loss 2.345\n",
            "Ep 27 (Step 014300): Train loss 1.287, Val loss 2.335\n",
            "Ep 27 (Step 014350): Train loss 1.279, Val loss 2.329\n",
            "Ep 27 (Step 014400): Train loss 1.318, Val loss 2.334\n",
            "Ep 27 (Step 014450): Train loss 1.299, Val loss 2.332\n",
            "Ep 27 (Step 014500): Train loss 1.256, Val loss 2.337\n",
            "Ep 27 (Step 014550): Train loss 1.274, Val loss 2.342\n",
            "Ep 27 (Step 014600): Train loss 1.283, Val loss 2.328\n",
            "Ep 27 (Step 014650): Train loss 1.270, Val loss 2.319\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to train a model with a similar number of parameters (i.e., training loss and training loss).  ### Response: The study compares the scaling curves for a 1.5B model, showing that larger models trained on 1 trillion tokens,\n",
            "Ep 28 (Step 014700): Train loss 1.215, Val loss 2.324\n",
            "Ep 28 (Step 014750): Train loss 1.279, Val loss 2.323\n",
            "Ep 28 (Step 014800): Train loss 1.274, Val loss 2.314\n",
            "Ep 28 (Step 014850): Train loss 1.236, Val loss 2.306\n",
            "Ep 28 (Step 014900): Train loss 1.228, Val loss 2.305\n",
            "Ep 28 (Step 014950): Train loss 1.229, Val loss 2.288\n",
            "Ep 28 (Step 015000): Train loss 1.215, Val loss 2.300\n",
            "Ep 28 (Step 015050): Train loss 1.211, Val loss 2.305\n",
            "Ep 28 (Step 015100): Train loss 1.223, Val loss 2.315\n",
            "Ep 28 (Step 015150): Train loss 1.208, Val loss 2.297\n",
            "Ep 28 (Step 015200): Train loss 1.170, Val loss 2.299\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to train a model with a similar number of parameters (in Table 2.1.1.1) is more efficient to  ### Response: The study compares the scaling curves for different models of varying parameter sizes (number of parameters) and training\n",
            "Ep 29 (Step 015250): Train loss 1.192, Val loss 2.284\n",
            "Ep 29 (Step 015300): Train loss 1.145, Val loss 2.279\n",
            "Ep 29 (Step 015350): Train loss 1.169, Val loss 2.288\n",
            "Ep 29 (Step 015400): Train loss 1.170, Val loss 2.275\n",
            "Ep 29 (Step 015450): Train loss 1.140, Val loss 2.280\n",
            "Ep 29 (Step 015500): Train loss 1.157, Val loss 2.275\n",
            "Ep 29 (Step 015550): Train loss 1.166, Val loss 2.278\n",
            "Ep 29 (Step 015600): Train loss 1.153, Val loss 2.261\n",
            "Ep 29 (Step 015650): Train loss 1.134, Val loss 2.253\n",
            "Ep 29 (Step 015700): Train loss 1.157, Val loss 2.264\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to be more efficient.  ### Response: The study compares scaling curves for a 1.5B model with 1.3B and 1.5B parameters, showing that larger models trained on a 1.8B model, showing that larger\n",
            "Ep 30 (Step 015750): Train loss 1.149, Val loss 2.270\n",
            "Ep 30 (Step 015800): Train loss 1.141, Val loss 2.264\n",
            "Ep 30 (Step 015850): Train loss 1.158, Val loss 2.270\n",
            "Ep 30 (Step 015900): Train loss 1.139, Val loss 2.260\n",
            "Ep 30 (Step 015950): Train loss 1.113, Val loss 2.250\n",
            "Ep 30 (Step 016000): Train loss 1.133, Val loss 2.267\n",
            "Ep 30 (Step 016050): Train loss 1.071, Val loss 2.247\n",
            "Ep 30 (Step 016100): Train loss 1.135, Val loss 2.247\n",
            "Ep 30 (Step 016150): Train loss 1.099, Val loss 2.237\n",
            "Ep 30 (Step 016200): Train loss 1.099, Val loss 2.241\n",
            "Ep 30 (Step 016250): Train loss 1.091, Val loss 2.243\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to train a model with a similar size (in Table 2).  ### Response: The study compares the performance of the optimal model and training loss and training losses and FLOPs on downstream tasks, showing that larger models trained on downstream tasks,\n",
            "Ep 31 (Step 016300): Train loss 1.082, Val loss 2.236\n",
            "Ep 31 (Step 016350): Train loss 1.075, Val loss 2.234\n",
            "Ep 31 (Step 016400): Train loss 1.064, Val loss 2.238\n",
            "Ep 31 (Step 016450): Train loss 1.045, Val loss 2.223\n",
            "Ep 31 (Step 016500): Train loss 1.050, Val loss 2.226\n",
            "Ep 31 (Step 016550): Train loss 1.052, Val loss 2.231\n",
            "Ep 31 (Step 016600): Train loss 1.071, Val loss 2.224\n",
            "Ep 31 (Step 016650): Train loss 1.058, Val loss 2.215\n",
            "Ep 31 (Step 016700): Train loss 1.037, Val loss 2.210\n",
            "Ep 31 (Step 016750): Train loss 1.051, Val loss 2.196\n",
            "Ep 31 (Step 016800): Train loss 1.055, Val loss 2.213\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to train a model on downstream tasks (in factuality, code, code, code, and mathematical reasoning, code).  ### Response: The study compares the performance of the PaLM model and 1.5 Pro model and 1.8B\n",
            "Ep 32 (Step 016850): Train loss 0.988, Val loss 2.198\n",
            "Ep 32 (Step 016900): Train loss 1.027, Val loss 2.204\n",
            "Ep 32 (Step 016950): Train loss 0.996, Val loss 2.204\n",
            "Ep 32 (Step 017000): Train loss 1.005, Val loss 2.193\n",
            "Ep 32 (Step 017050): Train loss 1.015, Val loss 2.191\n",
            "Ep 32 (Step 017100): Train loss 0.963, Val loss 2.183\n",
            "Ep 32 (Step 017150): Train loss 0.984, Val loss 2.194\n",
            "Ep 32 (Step 017200): Train loss 0.994, Val loss 2.199\n",
            "Ep 32 (Step 017250): Train loss 0.966, Val loss 2.182\n",
            "Ep 32 (Step 017300): Train loss 1.005, Val loss 2.183\n",
            "Ep 32 (Step 017350): Train loss 0.994, Val loss 2.194\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to train a model with a similar number of parameters (in Table 1.5B, the PaLM 540B model is trained on downstream tasks, and is close to the best model, and it is likely to  ### Response: The study\n",
            "Ep 33 (Step 017400): Train loss 0.956, Val loss 2.170\n",
            "Ep 33 (Step 017450): Train loss 0.965, Val loss 2.185\n",
            "Ep 33 (Step 017500): Train loss 0.959, Val loss 2.177\n",
            "Ep 33 (Step 017550): Train loss 0.946, Val loss 2.174\n",
            "Ep 33 (Step 017600): Train loss 0.946, Val loss 2.179\n",
            "Ep 33 (Step 017650): Train loss 0.973, Val loss 2.176\n",
            "Ep 33 (Step 017700): Train loss 0.986, Val loss 2.171\n",
            "Ep 33 (Step 017750): Train loss 0.955, Val loss 2.177\n",
            "Ep 33 (Step 017800): Train loss 0.966, Val loss 2.163\n",
            "Ep 33 (Step 017850): Train loss 0.925, Val loss 2.161\n",
            "Ep 33 (Step 017900): Train loss 0.964, Val loss 2.173\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to train a model on downstream tasks (Section 6.1.1.1).  ### Response: The study compares the scaling curves for 1.5 and 1.5 Pro model sizes of 1.8B and 1.5B,\n",
            "Ep 34 (Step 017950): Train loss 0.907, Val loss 2.165\n",
            "Ep 34 (Step 018000): Train loss 0.934, Val loss 2.169\n",
            "Ep 34 (Step 018050): Train loss 0.963, Val loss 2.165\n",
            "Ep 34 (Step 018100): Train loss 0.918, Val loss 2.172\n",
            "Ep 34 (Step 018150): Train loss 0.905, Val loss 2.157\n",
            "Ep 34 (Step 018200): Train loss 0.901, Val loss 2.156\n",
            "Ep 34 (Step 018250): Train loss 0.882, Val loss 2.148\n",
            "Ep 34 (Step 018300): Train loss 0.886, Val loss 2.158\n",
            "Ep 34 (Step 018350): Train loss 0.871, Val loss 2.140\n",
            "Ep 34 (Step 018400): Train loss 0.884, Val loss 2.153\n",
            "Ep 34 (Step 018450): Train loss 0.875, Val loss 2.136\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to train a model on downstream tasks (Section 2.1.1.1.2.1.1.2.1), it is a smaller model with 1.8B and (Section 3.2), and a similar number of parameters for\n",
            "Ep 35 (Step 018500): Train loss 0.849, Val loss 2.133\n",
            "Ep 35 (Step 018550): Train loss 0.876, Val loss 2.143\n",
            "Ep 35 (Step 018600): Train loss 0.881, Val loss 2.141\n",
            "Ep 35 (Step 018650): Train loss 0.856, Val loss 2.138\n",
            "Ep 35 (Step 018700): Train loss 0.857, Val loss 2.150\n",
            "Ep 35 (Step 018750): Train loss 0.847, Val loss 2.141\n",
            "Ep 35 (Step 018800): Train loss 0.865, Val loss 2.144\n",
            "Ep 35 (Step 018850): Train loss 0.868, Val loss 2.136\n",
            "Ep 35 (Step 018900): Train loss 0.862, Val loss 2.131\n",
            "Ep 35 (Step 018950): Train loss 0.859, Val loss 2.120\n",
            "Ep 35 (Step 019000): Train loss 0.834, Val loss 2.115\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to train a model on downstream tasks (in factuality, code), and downstream tasks are close to the best model’s performance.  ### Response: The study compares the performance of the PaLM model using a scaling curve in the 1\n",
            "Ep 36 (Step 019050): Train loss 0.832, Val loss 2.112\n",
            "Ep 36 (Step 019100): Train loss 0.841, Val loss 2.126\n",
            "Ep 36 (Step 019150): Train loss 0.813, Val loss 2.121\n",
            "Ep 36 (Step 019200): Train loss 0.839, Val loss 2.124\n",
            "Ep 36 (Step 019250): Train loss 0.816, Val loss 2.120\n",
            "Ep 36 (Step 019300): Train loss 0.796, Val loss 2.120\n",
            "Ep 36 (Step 019350): Train loss 0.798, Val loss 2.109\n",
            "Ep 36 (Step 019400): Train loss 0.779, Val loss 2.117\n",
            "Ep 36 (Step 019450): Train loss 0.824, Val loss 2.110\n",
            "Ep 36 (Step 019500): Train loss 0.808, Val loss 2.105\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to train a model to train a model on downstream tasks.  ### Response: The study compares the scaling performance of models of varying parameter sizes (with 1.5B and 1.8B parameters) and using the same training loss as the\n",
            "Ep 37 (Step 019550): Train loss 0.794, Val loss 2.100\n",
            "Ep 37 (Step 019600): Train loss 0.789, Val loss 2.106\n",
            "Ep 37 (Step 019650): Train loss 0.783, Val loss 2.096\n",
            "Ep 37 (Step 019700): Train loss 0.789, Val loss 2.092\n",
            "Ep 37 (Step 019750): Train loss 0.779, Val loss 2.093\n",
            "Ep 37 (Step 019800): Train loss 0.789, Val loss 2.104\n",
            "Ep 37 (Step 019850): Train loss 0.768, Val loss 2.088\n",
            "Ep 37 (Step 019900): Train loss 0.765, Val loss 2.100\n",
            "Ep 37 (Step 019950): Train loss 0.771, Val loss 2.093\n",
            "Ep 37 (Step 020000): Train loss 0.735, Val loss 2.083\n",
            "Ep 37 (Step 020050): Train loss 0.774, Val loss 2.081\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to train a model on downstream tasks, and to train a model on downstream tasks.  ### Response: The study compares the scaling curves for 1.5 and 1.5 models using a 1.8B model trained loss and training loss,\n",
            "Ep 38 (Step 020100): Train loss 0.739, Val loss 2.086\n",
            "Ep 38 (Step 020150): Train loss 0.711, Val loss 2.078\n",
            "Ep 38 (Step 020200): Train loss 0.759, Val loss 2.094\n",
            "Ep 38 (Step 020250): Train loss 0.735, Val loss 2.083\n",
            "Ep 38 (Step 020300): Train loss 0.734, Val loss 2.085\n",
            "Ep 38 (Step 020350): Train loss 0.725, Val loss 2.087\n",
            "Ep 38 (Step 020400): Train loss 0.708, Val loss 2.082\n",
            "Ep 38 (Step 020450): Train loss 0.732, Val loss 2.085\n",
            "Ep 38 (Step 020500): Train loss 0.717, Val loss 2.077\n",
            "Ep 38 (Step 020550): Train loss 0.733, Val loss 2.070\n",
            "Ep 38 (Step 020600): Train loss 0.701, Val loss 2.072\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to train a model on downstream tasks (in Table 1) and is more efficient (i.e., we observe that it achieves optimal model size to train similar performance on downstream tasks.  ### Response: The study compares the performance of Llama\n",
            "Ep 39 (Step 020650): Train loss 0.695, Val loss 2.062\n",
            "Ep 39 (Step 020700): Train loss 0.695, Val loss 2.055\n",
            "Ep 39 (Step 020750): Train loss 0.696, Val loss 2.059\n",
            "Ep 39 (Step 020800): Train loss 0.696, Val loss 2.072\n",
            "Ep 39 (Step 020850): Train loss 0.682, Val loss 2.058\n",
            "Ep 39 (Step 020900): Train loss 0.679, Val loss 2.057\n",
            "Ep 39 (Step 020950): Train loss 0.695, Val loss 2.056\n",
            "Ep 39 (Step 021000): Train loss 0.690, Val loss 2.070\n",
            "Ep 39 (Step 021050): Train loss 0.675, Val loss 2.064\n",
            "Ep 39 (Step 021100): Train loss 0.677, Val loss 2.051\n",
            "Ep 39 (Step 021150): Train loss 0.670, Val loss 2.058\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to train a log-loss-off, i.e., from model to train smaller models, and on downstream tasks.  ### Response: The study compares the scaling curves for the performance of different models of varying parameter sizes (8B and\n",
            "Ep 40 (Step 021200): Train loss 0.679, Val loss 2.047\n",
            "Ep 40 (Step 021250): Train loss 0.653, Val loss 2.056\n",
            "Ep 40 (Step 021300): Train loss 0.648, Val loss 2.056\n",
            "Ep 40 (Step 021350): Train loss 0.646, Val loss 2.053\n",
            "Ep 40 (Step 021400): Train loss 0.654, Val loss 2.055\n",
            "Ep 40 (Step 021450): Train loss 0.670, Val loss 2.055\n",
            "Ep 40 (Step 021500): Train loss 0.660, Val loss 2.048\n",
            "Ep 40 (Step 021550): Train loss 0.629, Val loss 2.044\n",
            "Ep 40 (Step 021600): Train loss 0.669, Val loss 2.039\n",
            "Ep 40 (Step 021650): Train loss 0.629, Val loss 2.038\n",
            "Ep 40 (Step 021700): Train loss 0.647, Val loss 2.042\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to train a model on downstream tasks (in particular, it achieves optimal performance on downstream tasks (Section 5.1).  ### Response: The study compares scaling curves for a log-loss-loss and a model with 1.5 and 1\n",
            "Ep 41 (Step 021750): Train loss 0.635, Val loss 2.052\n",
            "Ep 41 (Step 021800): Train loss 0.622, Val loss 2.052\n",
            "Ep 41 (Step 021850): Train loss 0.638, Val loss 2.044\n",
            "Ep 41 (Step 021900): Train loss 0.615, Val loss 2.043\n",
            "Ep 41 (Step 021950): Train loss 0.613, Val loss 2.046\n",
            "Ep 41 (Step 022000): Train loss 0.629, Val loss 2.045\n",
            "Ep 41 (Step 022050): Train loss 0.608, Val loss 2.034\n",
            "Ep 41 (Step 022100): Train loss 0.607, Val loss 2.019\n",
            "Ep 41 (Step 022150): Train loss 0.613, Val loss 2.035\n",
            "Ep 41 (Step 022200): Train loss 0.594, Val loss 2.021\n",
            "Ep 41 (Step 022250): Train loss 0.602, Val loss 2.031\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to train a model on downstream tasks, as described in Section 3.1.  ### Response: The study compares the performance of different model sizes and training losses and using a 1.5 Pro model trained on 250B and 1.8B\n",
            "Ep 42 (Step 022300): Train loss 0.608, Val loss 2.029\n",
            "Ep 42 (Step 022350): Train loss 0.577, Val loss 2.025\n",
            "Ep 42 (Step 022400): Train loss 0.584, Val loss 2.025\n",
            "Ep 42 (Step 022450): Train loss 0.592, Val loss 2.038\n",
            "Ep 42 (Step 022500): Train loss 0.598, Val loss 2.043\n",
            "Ep 42 (Step 022550): Train loss 0.599, Val loss 2.037\n",
            "Ep 42 (Step 022600): Train loss 0.591, Val loss 2.030\n",
            "Ep 42 (Step 022650): Train loss 0.587, Val loss 2.032\n",
            "Ep 42 (Step 022700): Train loss 0.585, Val loss 2.024\n",
            "Ep 42 (Step 022750): Train loss 0.567, Val loss 2.019\n",
            "Ep 42 (Step 022800): Train loss 0.563, Val loss 2.028\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to train a model on downstream tasks (Section 2.1.1.1).  ### Response: The study found that larger models trained on downstream tasks (like model sizes) and downstream tasks, showing that larger models trained on downstream tasks,\n",
            "Ep 43 (Step 022850): Train loss 0.548, Val loss 2.024\n",
            "Ep 43 (Step 022900): Train loss 0.550, Val loss 2.025\n",
            "Ep 43 (Step 022950): Train loss 0.549, Val loss 2.027\n",
            "Ep 43 (Step 023000): Train loss 0.557, Val loss 2.021\n",
            "Ep 43 (Step 023050): Train loss 0.578, Val loss 2.019\n",
            "Ep 43 (Step 023100): Train loss 0.535, Val loss 2.015\n",
            "Ep 43 (Step 023150): Train loss 0.566, Val loss 2.010\n",
            "Ep 43 (Step 023200): Train loss 0.557, Val loss 2.010\n",
            "Ep 43 (Step 023250): Train loss 0.564, Val loss 2.018\n",
            "Ep 43 (Step 023300): Train loss 0.535, Val loss 2.011\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to train a model on downstream tasks (Section 3.1.1).  ### Response: The study compares the scaling curves for a model and model on downstream tasks, showing that model performance improves with downstream tasks. It finds that optimal performance on\n",
            "Ep 44 (Step 023350): Train loss 0.545, Val loss 2.014\n",
            "Ep 44 (Step 023400): Train loss 0.536, Val loss 2.011\n",
            "Ep 44 (Step 023450): Train loss 0.530, Val loss 2.021\n",
            "Ep 44 (Step 023500): Train loss 0.517, Val loss 2.018\n",
            "Ep 44 (Step 023550): Train loss 0.511, Val loss 2.013\n",
            "Ep 44 (Step 023600): Train loss 0.529, Val loss 2.016\n",
            "Ep 44 (Step 023650): Train loss 0.511, Val loss 2.006\n",
            "Ep 44 (Step 023700): Train loss 0.537, Val loss 2.009\n",
            "Ep 44 (Step 023750): Train loss 0.524, Val loss 2.023\n",
            "Ep 44 (Step 023800): Train loss 0.497, Val loss 1.994\n",
            "Ep 44 (Step 023850): Train loss 0.503, Val loss 1.999\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to train a log-loss-loss-free model on downstream tasks.  ### Response: The study compares the scaling curves for a log-loss and model, showing that the optimal model was trained using the same tokenizer and training loss,\n",
            "Ep 45 (Step 023900): Train loss 0.517, Val loss 2.005\n",
            "Ep 45 (Step 023950): Train loss 0.502, Val loss 2.004\n",
            "Ep 45 (Step 024000): Train loss 0.509, Val loss 1.988\n",
            "Ep 45 (Step 024050): Train loss 0.509, Val loss 1.997\n",
            "Ep 45 (Step 024100): Train loss 0.493, Val loss 1.994\n",
            "Ep 45 (Step 024150): Train loss 0.493, Val loss 2.002\n",
            "Ep 45 (Step 024200): Train loss 0.480, Val loss 1.994\n",
            "Ep 45 (Step 024250): Train loss 0.498, Val loss 2.004\n",
            "Ep 45 (Step 024300): Train loss 0.484, Val loss 2.000\n",
            "Ep 45 (Step 024350): Train loss 0.490, Val loss 1.994\n",
            "Ep 45 (Step 024400): Train loss 0.489, Val loss 1.998\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to train a model on downstream tasks, and to determine the best model on downstream tasks (in Table 1) that as a log-loss-in-the-loss and is used for downstream tasks.  ### Response: The study compares the\n",
            "Ep 46 (Step 024450): Train loss 0.472, Val loss 2.002\n",
            "Ep 46 (Step 024500): Train loss 0.452, Val loss 2.000\n",
            "Ep 46 (Step 024550): Train loss 0.474, Val loss 1.997\n",
            "Ep 46 (Step 024600): Train loss 0.465, Val loss 1.993\n",
            "Ep 46 (Step 024650): Train loss 0.489, Val loss 2.002\n",
            "Ep 46 (Step 024700): Train loss 0.459, Val loss 1.986\n",
            "Ep 46 (Step 024750): Train loss 0.475, Val loss 1.991\n",
            "Ep 46 (Step 024800): Train loss 0.469, Val loss 1.989\n",
            "Ep 46 (Step 024850): Train loss 0.471, Val loss 2.008\n",
            "Ep 46 (Step 024900): Train loss 0.463, Val loss 1.989\n",
            "Ep 46 (Step 024950): Train loss 0.476, Val loss 2.004\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to train a model on downstream tasks (Section 3.1.1.1).  ### Response: The study shows that trained for a 1.5B and 540B and 6.8B model, showing that larger models (like trained\n",
            "Ep 47 (Step 025000): Train loss 0.456, Val loss 1.993\n",
            "Ep 47 (Step 025050): Train loss 0.434, Val loss 1.999\n",
            "Ep 47 (Step 025100): Train loss 0.435, Val loss 1.991\n",
            "Ep 47 (Step 025150): Train loss 0.442, Val loss 1.996\n",
            "Ep 47 (Step 025200): Train loss 0.450, Val loss 2.006\n",
            "Ep 47 (Step 025250): Train loss 0.437, Val loss 1.997\n",
            "Ep 47 (Step 025300): Train loss 0.450, Val loss 2.000\n",
            "Ep 47 (Step 025350): Train loss 0.444, Val loss 1.986\n",
            "Ep 47 (Step 025400): Train loss 0.437, Val loss 1.989\n",
            "Ep 47 (Step 025450): Train loss 0.430, Val loss 1.985\n",
            "Ep 47 (Step 025500): Train loss 0.427, Val loss 1.986\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to train a log-loss-free model.  ### Response: The study compares the performance of different model sizes (with 1 × 1022B and 1.5B parameters) and trained models, showing that larger models, and training losses\n",
            "Ep 48 (Step 025550): Train loss 0.420, Val loss 1.998\n",
            "Ep 48 (Step 025600): Train loss 0.423, Val loss 1.988\n",
            "Ep 48 (Step 025650): Train loss 0.427, Val loss 1.998\n",
            "Ep 48 (Step 025700): Train loss 0.413, Val loss 2.005\n",
            "Ep 48 (Step 025750): Train loss 0.421, Val loss 1.991\n",
            "Ep 48 (Step 025800): Train loss 0.408, Val loss 1.988\n",
            "Ep 48 (Step 025850): Train loss 0.405, Val loss 1.987\n",
            "Ep 48 (Step 025900): Train loss 0.410, Val loss 1.990\n",
            "Ep 48 (Step 025950): Train loss 0.415, Val loss 1.991\n",
            "Ep 48 (Step 026000): Train loss 0.401, Val loss 1.989\n",
            "Ep 48 (Step 026050): Train loss 0.401, Val loss 1.987\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to train a model on downstream tasks (Section 2.1.1.1), and  ### Response: The study compares the scaling curves for training models using 1.5 and 1.5B parameters trained on 250B and 1.5\n",
            "Ep 49 (Step 026100): Train loss 0.396, Val loss 1.985\n",
            "Ep 49 (Step 026150): Train loss 0.391, Val loss 1.984\n",
            "Ep 49 (Step 026200): Train loss 0.394, Val loss 1.979\n",
            "Ep 49 (Step 026250): Train loss 0.396, Val loss 2.000\n",
            "Ep 49 (Step 026300): Train loss 0.403, Val loss 1.989\n",
            "Ep 49 (Step 026350): Train loss 0.396, Val loss 1.987\n",
            "Ep 49 (Step 026400): Train loss 0.408, Val loss 1.978\n",
            "Ep 49 (Step 026450): Train loss 0.388, Val loss 1.987\n",
            "Ep 49 (Step 026500): Train loss 0.404, Val loss 1.995\n",
            "Ep 49 (Step 026550): Train loss 0.388, Val loss 1.984\n",
            "Ep 49 (Step 026600): Train loss 0.383, Val loss 1.983\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to  ### Response: The study shows that larger models trained on downstream tasks exhibit optimal model sizes (ranging from training parameters) and training loss, finding that larger models like the lowest loss, training loss and optimal performance is more pronounced for downstream tasks\n",
            "Ep 50 (Step 026650): Train loss 0.372, Val loss 1.985\n",
            "Ep 50 (Step 026700): Train loss 0.374, Val loss 1.987\n",
            "Ep 50 (Step 026750): Train loss 0.376, Val loss 1.980\n",
            "Ep 50 (Step 026800): Train loss 0.360, Val loss 1.983\n",
            "Ep 50 (Step 026850): Train loss 0.374, Val loss 1.975\n",
            "Ep 50 (Step 026900): Train loss 0.391, Val loss 2.001\n",
            "Ep 50 (Step 026950): Train loss 0.374, Val loss 1.979\n",
            "Ep 50 (Step 027000): Train loss 0.360, Val loss 1.975\n",
            "Ep 50 (Step 027050): Train loss 0.359, Val loss 1.986\n",
            "Ep 50 (Step 027100): Train loss 0.370, Val loss 1.972\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to train a model on downstream tasks, i.e., from Model FLOP and  ### Response: The study shows that model performance improves downstream tasks, particularly focusing on downstream tasks like MMLU and BBH, showing that model size increases\n",
            "Ep 51 (Step 027150): Train loss 0.368, Val loss 1.984\n",
            "Ep 51 (Step 027200): Train loss 0.358, Val loss 1.984\n",
            "Ep 51 (Step 027250): Train loss 0.355, Val loss 1.984\n",
            "Ep 51 (Step 027300): Train loss 0.350, Val loss 1.988\n",
            "Ep 51 (Step 027350): Train loss 0.351, Val loss 2.002\n",
            "Ep 51 (Step 027400): Train loss 0.341, Val loss 1.984\n",
            "Ep 51 (Step 027450): Train loss 0.361, Val loss 1.992\n",
            "Ep 51 (Step 027500): Train loss 0.330, Val loss 1.981\n",
            "Ep 51 (Step 027550): Train loss 0.335, Val loss 1.984\n",
            "Ep 51 (Step 027600): Train loss 0.338, Val loss 1.978\n",
            "Ep 51 (Step 027650): Train loss 0.343, Val loss 1.978\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to train a model on downstream tasks, as a function of downstream tasks (in particular, training FLOPs). For example, the 9B model is trained for a given scale, the 9B model, and is trained for code (in total).\n",
            "Ep 52 (Step 027700): Train loss 0.349, Val loss 1.988\n",
            "Ep 52 (Step 027750): Train loss 0.335, Val loss 1.986\n",
            "Ep 52 (Step 027800): Train loss 0.331, Val loss 1.978\n",
            "Ep 52 (Step 027850): Train loss 0.327, Val loss 1.987\n",
            "Ep 52 (Step 027900): Train loss 0.334, Val loss 1.986\n",
            "Ep 52 (Step 027950): Train loss 0.338, Val loss 1.995\n",
            "Ep 52 (Step 028000): Train loss 0.313, Val loss 1.987\n",
            "Ep 52 (Step 028050): Train loss 0.314, Val loss 2.001\n",
            "Ep 52 (Step 028100): Train loss 0.323, Val loss 1.976\n",
            "Ep 52 (Step 028150): Train loss 0.322, Val loss 1.978\n",
            "Ep 52 (Step 028200): Train loss 0.307, Val loss 1.970\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to train a log-loss-free model on downstream tasks.  ### Response: The study compares the performance of a larger model and downstream benchmark models using the same training loss and test sets, showing that optimal performance is not observed on downstream tasks\n",
            "Ep 53 (Step 028250): Train loss 0.314, Val loss 2.001\n",
            "Ep 53 (Step 028300): Train loss 0.313, Val loss 1.986\n",
            "Ep 53 (Step 028350): Train loss 0.310, Val loss 1.976\n",
            "Ep 53 (Step 028400): Train loss 0.326, Val loss 2.001\n",
            "Ep 53 (Step 028450): Train loss 0.311, Val loss 1.988\n",
            "Ep 53 (Step 028500): Train loss 0.304, Val loss 1.982\n",
            "Ep 53 (Step 028550): Train loss 0.312, Val loss 1.987\n",
            "Ep 53 (Step 028600): Train loss 0.317, Val loss 1.975\n",
            "Ep 53 (Step 028650): Train loss 0.300, Val loss 1.977\n",
            "Ep 53 (Step 028700): Train loss 0.305, Val loss 1.985\n",
            "Ep 53 (Step 028750): Train loss 0.307, Val loss 1.995\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to train a log-loss-free model on downstream tasks.  ### Response: The study compares the performance of two models—data and train similar sizes (with models) and train them against a 1.5B parameter model, trained on\n",
            "Ep 54 (Step 028800): Train loss 0.285, Val loss 1.990\n",
            "Ep 54 (Step 028850): Train loss 0.290, Val loss 1.990\n",
            "Ep 54 (Step 028900): Train loss 0.292, Val loss 1.982\n",
            "Ep 54 (Step 028950): Train loss 0.303, Val loss 1.984\n",
            "Ep 54 (Step 029000): Train loss 0.289, Val loss 1.988\n",
            "Ep 54 (Step 029050): Train loss 0.298, Val loss 1.982\n",
            "Ep 54 (Step 029100): Train loss 0.287, Val loss 1.984\n",
            "Ep 54 (Step 029150): Train loss 0.297, Val loss 1.995\n",
            "Ep 54 (Step 029200): Train loss 0.284, Val loss 1.988\n",
            "Ep 54 (Step 029250): Train loss 0.268, Val loss 1.983\n",
            "Ep 54 (Step 029300): Train loss 0.279, Val loss 1.972\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to train a log-loss-loss-free model on downstream tasks.  ### Response: The study compares the performance of various models of varying sizes of varying sizes (ranging from 125 million parameters) and training parameters, showing that optimal model performance\n",
            "Ep 55 (Step 029350): Train loss 0.282, Val loss 1.984\n",
            "Ep 55 (Step 029400): Train loss 0.281, Val loss 1.980\n",
            "Ep 55 (Step 029450): Train loss 0.279, Val loss 1.978\n",
            "Ep 55 (Step 029500): Train loss 0.274, Val loss 1.980\n",
            "Ep 55 (Step 029550): Train loss 0.271, Val loss 1.987\n",
            "Ep 55 (Step 029600): Train loss 0.279, Val loss 1.993\n",
            "Ep 55 (Step 029650): Train loss 0.280, Val loss 1.983\n",
            "Ep 55 (Step 029700): Train loss 0.282, Val loss 1.995\n",
            "Ep 55 (Step 029750): Train loss 0.270, Val loss 1.987\n",
            "Ep 55 (Step 029800): Train loss 0.265, Val loss 1.991\n",
            "Ep 55 (Step 029850): Train loss 0.276, Val loss 1.991\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to  ### Response: The study shows that scaling predictions for model size (number of parameters) and model sizes (number of parameters) and training loss, finding that the optimal model parameters (number of parameters) perform better than 1 × 1022\n",
            "🟨 saved → models/gpt2_finetuned.pth\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gpt2 = True\n",
        "model_gpt2 = model_gpt2.to(device)\n",
        "model_gpt2.train()\n",
        "\n",
        "optimizer_gpt2 = torch.optim.AdamW(model_gpt2.parameters(), lr=5e-5, weight_decay=0.1)\n",
        "\n",
        "torch.manual_seed(123)\n",
        "train_losses_g2, val_losses_g2, tokens_g2 = train_model_simple(\n",
        "    model_gpt2, train_loader_gpt2, val_loader_gpt2, optimizer_gpt2, device,\n",
        "    num_epochs=55, eval_freq=50, eval_iter=20,\n",
        "    start_context=format_input(val_set[0]), tokenizer=tokenizer_gpt2)\n",
        "\n",
        "torch.save(model_gpt2.state_dict(), \"models/gpt2_finetuned.pth\")\n",
        "print(\"🟨 saved → models/gpt2_finetuned.pth\")\n",
        "\n",
        "epochs_tensor = torch.linspace(0, 55, len(train_losses_g2))\n",
        "plot_losses(epochs_tensor, tokens_g2, train_losses_g2, val_losses_g2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5b19268",
      "metadata": {},
      "source": [
        "### Step 4: Load the fine-tuned Regex and GPT-2 models from saved checkpoints\n",
        "\n",
        "This step restores the previously trained models (`regex_finetuned.pth` and `gpt2_finetuned.pth`) so they can be used later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "d9b1e073",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🟦 Regex finetuned model loaded.\n",
            "🟨 GPT-2 finetuned model loaded.\n"
          ]
        }
      ],
      "source": [
        "GPT_CONFIG = {\n",
        "    \"vocab_size\": 1000,  # placeholder, will be updated\n",
        "    \"context_length\": 1024,\n",
        "    \"emb_dim\": 768,\n",
        "    \"n_heads\": 12,\n",
        "    \"n_layers\": 12,\n",
        "    \"drop_rate\": 0.3,\n",
        "    \"qkv_bias\": False}\n",
        "\n",
        "# === Load fine-tuned Regex model ===\n",
        "gpt2 = False\n",
        "tokenizer_v2 = SimpleTokenizerV2(vocab)\n",
        "vocab_size = len(tokenizer_v2.tokens2ids)\n",
        "GPT_CONFIG[\"vocab_size\"] = vocab_size\n",
        "\n",
        "model_regex = GPTModel(GPT_CONFIG).to(device)\n",
        "model_regex.load_state_dict(torch.load(\"models/regex_finetuned.pth\", map_location=device))\n",
        "model_regex.eval()\n",
        "print(\"🟦 Regex finetuned model loaded.\")\n",
        "\n",
        "# === Load fine-tuned GPT-2 model ===\n",
        "gpt2 = True\n",
        "tokenizer_gpt2 = tiktoken.get_encoding(\"gpt2\")\n",
        "vocab_size = tokenizer_gpt2.n_vocab\n",
        "GPT_CONFIG[\"vocab_size\"] = vocab_size\n",
        "\n",
        "model_gpt2 = GPTModel(GPT_CONFIG).to(device)\n",
        "model_gpt2.load_state_dict(torch.load(\"models/gpt2_finetuned.pth\", map_location=device))\n",
        "model_gpt2.eval()\n",
        "print(\"🟨 GPT-2 finetuned model loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b88c00ef",
      "metadata": {},
      "source": [
        "### Step 5: Generating Model Responses on the Validation Set\n",
        "\n",
        "- The function `generate_text()` uses the helper `generate_text_simple()` to autoregressively predict the next tokens up to `max_new_tokens = 256`.\n",
        "- Each prompt follows the **Alpaca-style format**, combining:\n",
        "  - `### Instruction:` (task description)\n",
        "  - `### Input:` (optional contextual passage)\n",
        "  - `### Response:` (expected model output)\n",
        "- Each generated response is stored under the key `\"model_response\"` for later evaluation.\n",
        "- The resulting subset is saved as **`gpt2_val_responses.json`**.\n",
        "\n",
        "This step effectively simulates how the fine-tuned model would respond to unseen instructions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "c65e15a4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_text(model, tokenizer, prompt, max_new_tokens=256):\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "    encoded = text_to_token_ids(prompt, tokenizer).to(device)\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate_text_simple(model=model, idx=encoded, max_new_tokens=max_new_tokens, context_size=context_size)\n",
        "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    return decoded_text[len(prompt):].replace(\"### Response:\", \"\").strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e358ef4",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating models responses: 100%|██████████| 500/500 [16:54<00:00,  2.03s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🟦 Saved Regex model responses → data/regex_val_responses.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating models responses: 100%|██████████| 500/500 [24:43<00:00,  2.97s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🟨 Saved GPT-2 model responses → data/gpt2_val_responses.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# debug_subset = val_set[:15] # use only first 15 entries for quick testing.\n",
        "\n",
        "# === Generate Regex model responses ===\n",
        "gpt2 = False\n",
        "for i, entry in tqdm(enumerate(val_set), total=len(val_set), desc=\"Generating models responses\"):\n",
        "    alpaca_prompt = (\n",
        "        f\"Below is an instruction that describes a task. \"\n",
        "        f\"Write a response that appropriately completes the request.\"\n",
        "        f\"\\n\\n### Instruction:\\n{entry['Instruction']}\")\n",
        "    if entry['Input']:\n",
        "        alpaca_prompt += f\"\\n\\n### Input:\\n{entry['Input']}\"\n",
        "    alpaca_prompt += \"\\n\\n### Response:\\n\"\n",
        "\n",
        "    entry[\"model_response\"] = generate_text(model_regex, tokenizer_v2, alpaca_prompt)\n",
        "\n",
        "with open(\"responces/regex_val_responses.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(val_set, f, indent=2, ensure_ascii=False)\n",
        "print(\"🟦 Saved Regex model responses → responces/regex_val_responses.json\")\n",
        "\n",
        "\n",
        "# === Generate GPT-2 model responses ===\n",
        "gpt2 = True\n",
        "for i, entry in tqdm(enumerate(val_set), total=len(val_set), desc=\"Generating models responses\"):\n",
        "    alpaca_prompt = (\n",
        "        f\"Below is an instruction that describes a task. \"\n",
        "        f\"Write a response that appropriately completes the request.\"\n",
        "        f\"\\n\\n### Instruction:\\n{entry['Instruction']}\")\n",
        "    if entry['Input']:\n",
        "        alpaca_prompt += f\"\\n\\n### Input:\\n{entry['Input']}\"\n",
        "    alpaca_prompt += \"\\n\\n### Response:\\n\"\n",
        "\n",
        "    entry[\"model_response\"] = generate_text(model_gpt2, tokenizer_gpt2, alpaca_prompt)\n",
        "\n",
        "with open(\"responces/gpt2_val_responses.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(val_set, f, indent=2, ensure_ascii=False)\n",
        "print(\"🟨 Saved GPT-2 model responses → responces/gpt2_val_responses.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05992a25",
      "metadata": {},
      "source": [
        "### Step 6: Loading Saved Model Response Validation Files\n",
        "\n",
        "Here we load the JSON files containing the generated validation responses from both fine-tuned models:\n",
        "\n",
        "- **Regex Model:** `Regex_val_responses.json`\n",
        "- **GPT-2 Model:** `gpt2_val_responses.json`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a280ba1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🟦 Loaded 500 Regex val responses.\n",
            "🟨 Loaded 500 GPT-2 val responses.\n"
          ]
        }
      ],
      "source": [
        "# Load Regex model responses\n",
        "with open(\"responces/regex_val_responses.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    regex_val_responses = json.load(f)\n",
        "print(f\"🟦 Loaded {len(regex_val_responses)} Regex val responses.\")\n",
        "\n",
        "# Load GPT-2 model responses\n",
        "with open(\"responces/gpt2_val_responses.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    gpt2_val_responses = json.load(f)\n",
        "print(f\"🟨 Loaded {len(gpt2_val_responses)} GPT-2 val responses.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70747227",
      "metadata": {},
      "source": [
        "## Part 4 — Evaluation on the Validation Set"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f23ccfa",
      "metadata": {},
      "source": [
        "We evaluate the fine-tuned models using:\n",
        "1.  **Automatic scoring (Quantitatively):** Using **BLEU-4**, **ROUGE-1/2/L-F1**, **METEOR**, **Token-F1**, and **BERTScore** metrics.\n",
        "2.  **LLM-based evaluation (Qualitatively):** Using an LLM model (**`Phi-3-mini-4k-instruct`**) as a judge to grade responses from 0–100.\n",
        "\n",
        "This helps quantify how well the model follows instructions compared to ground-truth responses."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7ece86a",
      "metadata": {},
      "source": [
        "### Step 1: Automatic scoring (Quantitatively).\n",
        "- Using **BLEU-4**, **ROUGE-1/2/L-F1**, **METEOR**, **Token-F1**, and **BERTScore** metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "8d09202f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List, Dict, Tuple, Optional\n",
        "import re, random\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import sacrebleu\n",
        "from rouge_score import rouge_scorer\n",
        "import nltk\n",
        "from nltk.translate.meteor_score import meteor_score as nltk_meteor_score\n",
        "from bert_score import score as bertscore_score\n",
        "\n",
        "# -------- Utilities --------\n",
        "def _flat(s: Optional[str]) -> str:\n",
        "    return (s or \"\").strip()\n",
        "\n",
        "def _normalize(s: str) -> str:\n",
        "    return re.sub(r\"\\s+\", \" \", s.lower().strip())\n",
        "\n",
        "def _fmt(x, nd=3):\n",
        "    try: return f\"{float(x):.{nd}f}\"\n",
        "    except Exception: return \"n/a\"\n",
        "\n",
        "# -------- Metrics ----------\n",
        "def compute_bleu(hyps: List[str], refs: List[str]) -> Dict:\n",
        "    bleu = sacrebleu.corpus_bleu(hyps, [refs])\n",
        "    return {\n",
        "        \"BLEU-4\": float(bleu.score),\n",
        "        \"P1\": float(bleu.precisions[0]),\n",
        "        \"P2\": float(bleu.precisions[1]),\n",
        "        \"P3\": float(bleu.precisions[2]),\n",
        "        \"P4\": float(bleu.precisions[3]),\n",
        "        \"BP\": float(bleu.bp),\n",
        "        \"sys_len\": int(bleu.sys_len),\n",
        "        \"ref_len\": int(bleu.ref_len),\n",
        "    }\n",
        "\n",
        "def compute_rouge(hyps: List[str], refs: List[str]) -> Dict:\n",
        "    scorer = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeL\"], use_stemmer=True)\n",
        "    r1 = r2 = rl = 0.0\n",
        "    for h, r in zip(hyps, refs):\n",
        "        s = scorer.score(r, h)\n",
        "        r1 += s[\"rouge1\"].fmeasure; r2 += s[\"rouge2\"].fmeasure; rl += s[\"rougeL\"].fmeasure\n",
        "    n = max(1, len(hyps))\n",
        "    return {\"ROUGE-1-F1\": r1/n, \"ROUGE-2-F1\": r2/n, \"ROUGE-L-F1\": rl/n}\n",
        "\n",
        "def compute_meteor(hyps: List[str], refs: List[str]) -> Dict:\n",
        "    for pkg in [\"corpora/wordnet\", \"corpora/omw-1.4\"]:\n",
        "        try: nltk.data.find(pkg)\n",
        "        except LookupError: nltk.download(pkg.split(\"/\")[1], quiet=True)\n",
        "    scores = [nltk_meteor_score([_flat(r).split()], _flat(h).split())\n",
        "              for h, r in zip(hyps, refs) if _flat(h) and _flat(r)]\n",
        "    return {\"METEOR\": float(sum(scores)/len(scores)) if scores else 0.0, \"used\": len(scores)}\n",
        "\n",
        "def compute_token_f1(hyps: List[str], refs: List[str]) -> Dict:\n",
        "    tp=fp=fn=0\n",
        "    for h, r in zip(hyps, refs):\n",
        "        ch, cr = Counter(_flat(h).split()), Counter(_flat(r).split())\n",
        "        common = ch & cr\n",
        "        tp += sum(common.values()); fp += sum((ch - cr).values()); fn += sum((cr - ch).values())\n",
        "    prec = tp/(tp+fp) if tp+fp else 0.0\n",
        "    rec  = tp/(tp+fn) if tp+fn else 0.0\n",
        "    f1   = 2*prec*rec/(prec+rec) if prec+rec else 0.0\n",
        "    return {\"precision\": prec, \"recall\": rec, \"F1\": f1}\n",
        "\n",
        "def compute_bertscore(hyps: List[str], refs: List[str]) -> Dict:\n",
        "    P,R,F1 = bertscore_score(hyps, refs, lang=\"en\",\n",
        "                             model_type=\"roberta-large\",\n",
        "                             rescale_with_baseline=True)\n",
        "    return {\"P\": float(P.mean()), \"R\": float(R.mean()), \"F1\": float(F1.mean()),\n",
        "            \"model_type\": \"roberta-large\", \"baseline_rescale\": True}\n",
        "\n",
        "def compute_all_metrics(hyps: List[str], refs: List[str]) -> Dict:\n",
        "    return {\n",
        "        \"BLEU\": compute_bleu(hyps, refs),\n",
        "        \"ROUGE\": compute_rouge(hyps, refs),\n",
        "        \"METEOR\": compute_meteor(hyps, refs),\n",
        "        \"TokenF1\": compute_token_f1(hyps, refs),\n",
        "        \"BERTScore\": compute_bertscore(hyps, refs),\n",
        "        \"count\": len(hyps)\n",
        "    }\n",
        "\n",
        "# -------- Helpers --------\n",
        "def _pick(d: dict, keys: List[str]) -> Optional[str]:\n",
        "    for k in keys:\n",
        "        if k in d and d[k] is not None:\n",
        "            return str(d[k])\n",
        "    return None\n",
        "\n",
        "def _extract_pairs(dataset: List[dict],\n",
        "                   pred_keys=(\"model_response\",\"prediction\",\"output\",\"hypothesis\",\"hyp\"),\n",
        "                   ref_keys=(\"Response\",\"response\",\"reference\",\"ref\",\"target\",\"gold\"),\n",
        "                   normalize_text: bool=False) -> Tuple[List[str], List[str]]:\n",
        "    hyps, refs = [], []\n",
        "    for ex in dataset:\n",
        "        h = _pick(ex, pred_keys) or \"\"\n",
        "        r = _pick(ex, ref_keys) or \"\"\n",
        "        if normalize_text: h, r = _normalize(h), _normalize(r)\n",
        "        hyps.append(h); refs.append(r)\n",
        "    return hyps, refs\n",
        "\n",
        "# -------- Output --------\n",
        "def _print_metrics_table(model_name: str, M: Dict):\n",
        "    print(f\"\\n=== {model_name} Quantitative Metrics ===\")\n",
        "    b = M[\"BLEU\"]\n",
        "    print(f\"BLEU-4:    {_fmt(b['BLEU-4'],2)}  (BP={_fmt(b['BP'],2)}, P1={_fmt(b['P1'],2)}, P2={_fmt(b['P2'],2)}, P3={_fmt(b['P3'],2)}, P4={_fmt(b['P4'],2)})\")\n",
        "    r = M[\"ROUGE\"]\n",
        "    print(f\"ROUGE-1/2/L-F1: {_fmt(r['ROUGE-1-F1'])} / {_fmt(r['ROUGE-2-F1'])} / {_fmt(r['ROUGE-L-F1'])}\")\n",
        "    m = M[\"METEOR\"]\n",
        "    print(f\"METEOR:    {_fmt(m['METEOR'])} (used {m['used']})\")\n",
        "    tf1 = M[\"TokenF1\"]; print(f\"Token-F1:  {_fmt(tf1['F1'])} (P={_fmt(tf1['precision'])}, R={_fmt(tf1['recall'])})\")\n",
        "    bs = M[\"BERTScore\"]\n",
        "    print(f\"BERTScore: {_fmt(bs['F1'])} (P={_fmt(bs['P'])}, R={_fmt(bs['R'])}, model={bs['model_type']}, baseline={bs['baseline_rescale']})\")\n",
        "    print(f\"Count:     {M['count']}\")\n",
        "\n",
        "# -------- Main --------\n",
        "def evaluate_model_outputs(model_name: str, dataset: List[dict], *, normalize_text=False, seed=1234) -> Dict:\n",
        "    random.seed(seed); np.random.seed(seed)\n",
        "    try:\n",
        "        import torch; torch.manual_seed(seed)\n",
        "    except Exception:\n",
        "        pass\n",
        "    hyps, refs = _extract_pairs(dataset, normalize_text=normalize_text)\n",
        "    M = compute_all_metrics(hyps, refs)\n",
        "    _print_metrics_table(model_name, M)\n",
        "    return M"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "067a73d5",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Regex Quantitative Metrics ===\n",
            "BLEU-4:    4.99  (BP=1.00, P1=24.68, P2=7.71, P3=2.84, P4=1.14)\n",
            "ROUGE-1/2/L-F1: 0.383 / 0.114 / 0.198\n",
            "METEOR:    0.237 (used 500)\n",
            "Token-F1:  0.299 (P=0.250, R=0.373)\n",
            "BERTScore: -0.104 (P=-0.239, R=0.039, model=roberta-large, baseline=True)\n",
            "Count:     500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== GPT-2 Quantitative Metrics ===\n",
            "BLEU-4:    9.18  (BP=1.00, P1=35.78, P2=12.66, P3=5.61, P4=2.79)\n",
            "ROUGE-1/2/L-F1: 0.400 / 0.120 / 0.212\n",
            "METEOR:    0.235 (used 500)\n",
            "Token-F1:  0.324 (P=0.306, R=0.345)\n",
            "BERTScore: 0.061 (P=0.009, R=0.115, model=roberta-large, baseline=True)\n",
            "Count:     500\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'BLEU': {'BLEU-4': 9.180958802774926,\n",
              "  'P1': 35.77764899924219,\n",
              "  'P2': 12.66377110106121,\n",
              "  'P3': 5.610578869248415,\n",
              "  'P4': 2.794921610265215,\n",
              "  'BP': 1.0,\n",
              "  'sys_len': 112165,\n",
              "  'ref_len': 92124},\n",
              " 'ROUGE': {'ROUGE-1-F1': 0.3995022426567119,\n",
              "  'ROUGE-2-F1': 0.11980240790786621,\n",
              "  'ROUGE-L-F1': 0.21195809219572329},\n",
              " 'METEOR': {'METEOR': 0.23484834823196452, 'used': 500},\n",
              " 'TokenF1': {'precision': 0.30569031381096345,\n",
              "  'recall': 0.34501593683458753,\n",
              "  'F1': 0.3241647975445638},\n",
              " 'BERTScore': {'P': 0.009061903692781925,\n",
              "  'R': 0.11460093408823013,\n",
              "  'F1': 0.060826919972896576,\n",
              "  'model_type': 'roberta-large',\n",
              "  'baseline_rescale': True},\n",
              " 'count': 500}"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate_model_outputs(\"Regex\", regex_val_responses)\n",
        "evaluate_model_outputs(\"GPT-2\", gpt2_val_responses)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45c9f306",
      "metadata": {},
      "source": [
        "### Step 2: LLM-based evaluation (Qualitatively):\n",
        "- Using an LLM model (**`Qwen2.5-7B-Instruct`**) as a judge to grade responses from 0–100."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "0fc7dbb2",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "06086d3d73e14d2ea65af761ff9dda46",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from typing import List, Optional\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "JUDGE_MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\" # or \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "\n",
        "# ---- Load judge (simple device/dtype selection) ----\n",
        "torch_dtype = (torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
        "               else (torch.float16 if torch.cuda.is_available() else None))\n",
        "tokenizer_judge = AutoTokenizer.from_pretrained(JUDGE_MODEL_NAME)\n",
        "model_judge = AutoModelForCausalLM.from_pretrained(JUDGE_MODEL_NAME, torch_dtype=torch_dtype, device_map=\"auto\")\n",
        "\n",
        "judge_pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_judge,\n",
        "    tokenizer=tokenizer_judge,\n",
        "    batch_size=4,\n",
        "    do_sample=False,\n",
        "    temperature=1e-5,\n",
        "    max_new_tokens=64,\n",
        "    pad_token_id=tokenizer_judge.eos_token_id,\n",
        ")\n",
        "\n",
        "# ---- Prompt + score parsing ----\n",
        "SYSTEM_RUBRIC = (\n",
        "    \"You are an expert grader. You will rate a candidate answer against a reference.\\n\"\n",
        "    \"Scoring: 0 to 100 (100 = perfect). Consider correctness, faithfulness to the reference, \"\n",
        "    \"coverage/completeness, and clarity. IMPORTANT: Respond with ONLY the final integer score, \"\n",
        "    \"no words, no punctuation, no explanations.\"\n",
        ")\n",
        "\n",
        "def make_prompt(instruction: str, inp: Optional[str], reference: str, hypothesis: str) -> str:\n",
        "    return (\n",
        "        f\"{SYSTEM_RUBRIC}\\n\\n\"\n",
        "        f\"Instruction:\\n{(instruction or '').strip()}\\n\\n\"\n",
        "        f\"Input:\\n{(inp or '').strip()}\\n\\n\"\n",
        "        f\"Reference answer:\\n{(reference or '').strip()}\\n\\n\"\n",
        "        f\"Model's answer:\\n{(hypothesis or '').strip()}\\n\\n\"\n",
        "        \"Final score (0-100):\"\n",
        "    )\n",
        "\n",
        "_num_pat = re.compile(r\"\\b(\\d{1,3})\\b\")\n",
        "def parse_score(text: str) -> Optional[int]:\n",
        "    m = _num_pat.findall(text or \"\")\n",
        "    if not m: return None\n",
        "    try:\n",
        "        return max(0, min(100, int(m[-1])))\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# ---- Batch query ----\n",
        "def query_judge_batch(prompts: List[str]) -> List[Optional[int]]:\n",
        "    outs = judge_pipe(prompts)\n",
        "    scores = []\n",
        "    for o in outs:\n",
        "        if isinstance(o, list): o = o[0]\n",
        "        scores.append(parse_score(o.get(\"generated_text\", \"\")))\n",
        "    return scores\n",
        "\n",
        "# ---- Evaluate one model ----\n",
        "def evaluate_with_judge(model_name: str, dataset: List[dict], *, batch_size: int = 4, limit: Optional[int] = None, seed: int = 1234) -> dict:\n",
        "    random.seed(seed)\n",
        "    n = len(dataset) if limit is None else min(len(dataset), int(limit))\n",
        "    prompts = [\n",
        "        make_prompt(ex.get(\"Instruction\",\"\"), ex.get(\"Input\",\"\"), ex.get(\"Response\",\"\"), ex.get(\"model_response\",\"\"))\n",
        "        for ex in dataset[:n]\n",
        "    ]\n",
        "\n",
        "    scores: List[int] = []\n",
        "    for i in tqdm(range(0, len(prompts), batch_size), desc=f\"Judge Scoring ({model_name})\"):\n",
        "        batch_scores = query_judge_batch(prompts[i:i+batch_size])\n",
        "        scores.extend(int(s) for s in batch_scores if isinstance(s, int))\n",
        "\n",
        "    if scores:\n",
        "        avg, mn, mx = sum(scores)/len(scores), min(scores), max(scores)\n",
        "        print(f\"\\n[Judge] {model_name}: avg={avg:.1f}% | min={mn} | max={mx} | n={len(scores)}\\n\")\n",
        "        return {\"model\": model_name, \"avg\": avg, \"min\": mn, \"max\": mx, \"n\": len(scores), \"scores\": scores}\n",
        "    else:\n",
        "        print(f\"\\n[Judge] {model_name}: no valid scores parsed.\\n\")\n",
        "        return {\"model\": model_name, \"avg\": None, \"min\": None, \"max\": None, \"n\": 0, \"scores\": []}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "a5a1a114",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Judge Scoring (Regex): 100%|██████████| 125/125 [07:37<00:00,  3.66s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Judge] Regex: avg=43.3% | min=0 | max=100 | n=500\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Judge Scoring (GPT-2): 100%|██████████| 125/125 [07:31<00:00,  3.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Judge] GPT-2: avg=49.9% | min=0 | max=100 | n=500\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'model': 'GPT-2',\n",
              " 'avg': 49.932,\n",
              " 'min': 0,\n",
              " 'max': 100,\n",
              " 'n': 500,\n",
              " 'scores': [72,\n",
              "  65,\n",
              "  65,\n",
              "  25,\n",
              "  35,\n",
              "  85,\n",
              "  55,\n",
              "  100,\n",
              "  65,\n",
              "  72,\n",
              "  4,\n",
              "  4,\n",
              "  65,\n",
              "  65,\n",
              "  100,\n",
              "  65,\n",
              "  100,\n",
              "  55,\n",
              "  58,\n",
              "  25,\n",
              "  0,\n",
              "  72,\n",
              "  25,\n",
              "  63,\n",
              "  4,\n",
              "  65,\n",
              "  58,\n",
              "  3,\n",
              "  42,\n",
              "  58,\n",
              "  65,\n",
              "  25,\n",
              "  55,\n",
              "  65,\n",
              "  65,\n",
              "  25,\n",
              "  2,\n",
              "  100,\n",
              "  3,\n",
              "  65,\n",
              "  85,\n",
              "  100,\n",
              "  65,\n",
              "  55,\n",
              "  4,\n",
              "  58,\n",
              "  4,\n",
              "  72,\n",
              "  65,\n",
              "  55,\n",
              "  15,\n",
              "  58,\n",
              "  100,\n",
              "  58,\n",
              "  72,\n",
              "  5,\n",
              "  72,\n",
              "  42,\n",
              "  25,\n",
              "  15,\n",
              "  3,\n",
              "  100,\n",
              "  25,\n",
              "  25,\n",
              "  100,\n",
              "  35,\n",
              "  75,\n",
              "  65,\n",
              "  85,\n",
              "  50,\n",
              "  100,\n",
              "  42,\n",
              "  65,\n",
              "  42,\n",
              "  50,\n",
              "  25,\n",
              "  25,\n",
              "  95,\n",
              "  35,\n",
              "  3,\n",
              "  72,\n",
              "  65,\n",
              "  52,\n",
              "  3,\n",
              "  65,\n",
              "  25,\n",
              "  75,\n",
              "  0,\n",
              "  3,\n",
              "  62,\n",
              "  65,\n",
              "  35,\n",
              "  65,\n",
              "  65,\n",
              "  52,\n",
              "  15,\n",
              "  52,\n",
              "  65,\n",
              "  35,\n",
              "  1,\n",
              "  58,\n",
              "  62,\n",
              "  1,\n",
              "  55,\n",
              "  65,\n",
              "  32,\n",
              "  65,\n",
              "  55,\n",
              "  65,\n",
              "  78,\n",
              "  52,\n",
              "  67,\n",
              "  85,\n",
              "  1,\n",
              "  15,\n",
              "  2,\n",
              "  55,\n",
              "  85,\n",
              "  5,\n",
              "  75,\n",
              "  55,\n",
              "  42,\n",
              "  0,\n",
              "  58,\n",
              "  65,\n",
              "  58,\n",
              "  65,\n",
              "  65,\n",
              "  55,\n",
              "  65,\n",
              "  65,\n",
              "  42,\n",
              "  72,\n",
              "  75,\n",
              "  2,\n",
              "  55,\n",
              "  65,\n",
              "  55,\n",
              "  100,\n",
              "  67,\n",
              "  65,\n",
              "  65,\n",
              "  55,\n",
              "  65,\n",
              "  25,\n",
              "  20,\n",
              "  1,\n",
              "  75,\n",
              "  25,\n",
              "  50,\n",
              "  100,\n",
              "  58,\n",
              "  75,\n",
              "  72,\n",
              "  65,\n",
              "  58,\n",
              "  0,\n",
              "  55,\n",
              "  58,\n",
              "  55,\n",
              "  100,\n",
              "  72,\n",
              "  65,\n",
              "  25,\n",
              "  65,\n",
              "  58,\n",
              "  25,\n",
              "  25,\n",
              "  25,\n",
              "  65,\n",
              "  100,\n",
              "  5,\n",
              "  65,\n",
              "  65,\n",
              "  65,\n",
              "  45,\n",
              "  32,\n",
              "  65,\n",
              "  15,\n",
              "  100,\n",
              "  32,\n",
              "  62,\n",
              "  65,\n",
              "  5,\n",
              "  25,\n",
              "  58,\n",
              "  65,\n",
              "  100,\n",
              "  58,\n",
              "  75,\n",
              "  100,\n",
              "  55,\n",
              "  78,\n",
              "  65,\n",
              "  65,\n",
              "  25,\n",
              "  55,\n",
              "  65,\n",
              "  55,\n",
              "  85,\n",
              "  72,\n",
              "  62,\n",
              "  13,\n",
              "  100,\n",
              "  100,\n",
              "  72,\n",
              "  58,\n",
              "  25,\n",
              "  4,\n",
              "  75,\n",
              "  2,\n",
              "  1,\n",
              "  35,\n",
              "  55,\n",
              "  35,\n",
              "  58,\n",
              "  25,\n",
              "  75,\n",
              "  65,\n",
              "  35,\n",
              "  72,\n",
              "  5,\n",
              "  65,\n",
              "  65,\n",
              "  65,\n",
              "  37,\n",
              "  55,\n",
              "  55,\n",
              "  45,\n",
              "  2,\n",
              "  65,\n",
              "  25,\n",
              "  65,\n",
              "  72,\n",
              "  55,\n",
              "  65,\n",
              "  65,\n",
              "  25,\n",
              "  23,\n",
              "  1,\n",
              "  65,\n",
              "  55,\n",
              "  72,\n",
              "  100,\n",
              "  67,\n",
              "  85,\n",
              "  15,\n",
              "  65,\n",
              "  55,\n",
              "  100,\n",
              "  72,\n",
              "  25,\n",
              "  25,\n",
              "  15,\n",
              "  45,\n",
              "  65,\n",
              "  55,\n",
              "  25,\n",
              "  58,\n",
              "  25,\n",
              "  100,\n",
              "  62,\n",
              "  75,\n",
              "  65,\n",
              "  4,\n",
              "  100,\n",
              "  35,\n",
              "  58,\n",
              "  52,\n",
              "  65,\n",
              "  45,\n",
              "  55,\n",
              "  45,\n",
              "  6,\n",
              "  65,\n",
              "  35,\n",
              "  25,\n",
              "  23,\n",
              "  35,\n",
              "  42,\n",
              "  58,\n",
              "  65,\n",
              "  58,\n",
              "  65,\n",
              "  58,\n",
              "  72,\n",
              "  55,\n",
              "  25,\n",
              "  3,\n",
              "  4,\n",
              "  72,\n",
              "  65,\n",
              "  100,\n",
              "  25,\n",
              "  65,\n",
              "  62,\n",
              "  35,\n",
              "  75,\n",
              "  65,\n",
              "  45,\n",
              "  55,\n",
              "  55,\n",
              "  45,\n",
              "  20,\n",
              "  5,\n",
              "  2,\n",
              "  25,\n",
              "  55,\n",
              "  100,\n",
              "  25,\n",
              "  5,\n",
              "  100,\n",
              "  15,\n",
              "  25,\n",
              "  58,\n",
              "  35,\n",
              "  20,\n",
              "  55,\n",
              "  100,\n",
              "  37,\n",
              "  80,\n",
              "  65,\n",
              "  25,\n",
              "  0,\n",
              "  25,\n",
              "  55,\n",
              "  72,\n",
              "  35,\n",
              "  65,\n",
              "  25,\n",
              "  58,\n",
              "  55,\n",
              "  65,\n",
              "  4,\n",
              "  65,\n",
              "  65,\n",
              "  100,\n",
              "  65,\n",
              "  25,\n",
              "  65,\n",
              "  75,\n",
              "  35,\n",
              "  65,\n",
              "  65,\n",
              "  25,\n",
              "  65,\n",
              "  100,\n",
              "  55,\n",
              "  72,\n",
              "  0,\n",
              "  58,\n",
              "  55,\n",
              "  65,\n",
              "  58,\n",
              "  65,\n",
              "  25,\n",
              "  25,\n",
              "  5,\n",
              "  52,\n",
              "  55,\n",
              "  100,\n",
              "  55,\n",
              "  55,\n",
              "  2,\n",
              "  65,\n",
              "  65,\n",
              "  2,\n",
              "  65,\n",
              "  42,\n",
              "  62,\n",
              "  65,\n",
              "  35,\n",
              "  55,\n",
              "  58,\n",
              "  62,\n",
              "  75,\n",
              "  72,\n",
              "  65,\n",
              "  3,\n",
              "  25,\n",
              "  75,\n",
              "  58,\n",
              "  55,\n",
              "  25,\n",
              "  42,\n",
              "  75,\n",
              "  25,\n",
              "  65,\n",
              "  100,\n",
              "  65,\n",
              "  55,\n",
              "  25,\n",
              "  25,\n",
              "  55,\n",
              "  65,\n",
              "  35,\n",
              "  15,\n",
              "  65,\n",
              "  58,\n",
              "  25,\n",
              "  100,\n",
              "  75,\n",
              "  20,\n",
              "  1,\n",
              "  86,\n",
              "  100,\n",
              "  45,\n",
              "  25,\n",
              "  58,\n",
              "  55,\n",
              "  65,\n",
              "  2,\n",
              "  55,\n",
              "  20,\n",
              "  65,\n",
              "  0,\n",
              "  65,\n",
              "  65,\n",
              "  25,\n",
              "  100,\n",
              "  42,\n",
              "  25,\n",
              "  52,\n",
              "  100,\n",
              "  45,\n",
              "  65,\n",
              "  42,\n",
              "  25,\n",
              "  65,\n",
              "  75,\n",
              "  5,\n",
              "  55,\n",
              "  55,\n",
              "  58,\n",
              "  65,\n",
              "  25,\n",
              "  65,\n",
              "  55,\n",
              "  67,\n",
              "  25,\n",
              "  25,\n",
              "  11,\n",
              "  25,\n",
              "  65,\n",
              "  3,\n",
              "  5,\n",
              "  65,\n",
              "  2,\n",
              "  58,\n",
              "  65,\n",
              "  3,\n",
              "  1,\n",
              "  65,\n",
              "  25,\n",
              "  65,\n",
              "  25,\n",
              "  35,\n",
              "  5,\n",
              "  25,\n",
              "  65,\n",
              "  72,\n",
              "  65,\n",
              "  45,\n",
              "  20,\n",
              "  65,\n",
              "  35,\n",
              "  3,\n",
              "  100,\n",
              "  65,\n",
              "  58,\n",
              "  5,\n",
              "  0,\n",
              "  35,\n",
              "  100,\n",
              "  25,\n",
              "  62,\n",
              "  60,\n",
              "  1,\n",
              "  65,\n",
              "  23,\n",
              "  25,\n",
              "  72,\n",
              "  100,\n",
              "  50,\n",
              "  55,\n",
              "  50,\n",
              "  100,\n",
              "  42,\n",
              "  65,\n",
              "  25,\n",
              "  42,\n",
              "  35,\n",
              "  52,\n",
              "  65,\n",
              "  65,\n",
              "  55,\n",
              "  5,\n",
              "  16,\n",
              "  100,\n",
              "  10]}"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate_with_judge(\"Regex\", regex_val_responses, batch_size=4)\n",
        "evaluate_with_judge(\"GPT-2\", gpt2_val_responses,  batch_size=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc978a66",
      "metadata": {},
      "source": [
        "## Part 5 - Using GPT-2 Pretrained Weights to Compare with Our Regex and GPT-2 Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d420018",
      "metadata": {},
      "source": [
        "\n",
        "In this part, we use the **GPT-2 (small, 124M parameters)** pretrained weights to establish a stronger baseline and compare against our Regex and GPT-2 models.\n",
        "\n",
        "**Steps:**\n",
        "1. **Download** the pretrained GPT-2o model weights (`gpt2-small (124M)`) and load it.\n",
        "2. **Fine-tune** the model on our custom **instruction dataset**.\n",
        "3. **Generate** validation responses using the fine-tuned GPT-2o model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "9f8b45c0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
            "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
            "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
            "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
            "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
            "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
            "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "# import tensorflow\n",
        "url = (\"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/01_main-chapter-code/gpt_download.py\") \n",
        "filename = url.split('/')[-1]\n",
        "urllib.request.urlretrieve(url, filename)\n",
        "\n",
        "from gpt_download import download_and_load_gpt2\n",
        "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "0b868f2f",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_configs = {\n",
        "\"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "\"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "\"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "\"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "\n",
        "model_name = \"gpt2-small (124M)\"\n",
        "gpt2 = True\n",
        "NEW_CONFIG = GPT_CONFIG.copy()\n",
        "NEW_CONFIG.update(model_configs[model_name])\n",
        "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
        "gpt2o = GPTModel(NEW_CONFIG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "54cdbf26",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 768)\n",
              "  (pos_emb): Embedding(1024, 768)\n",
              "  (drop_emb): Dropout(p=0.3, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (drop_shortcut): Dropout(p=0.3, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (drop_shortcut): Dropout(p=0.3, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (drop_shortcut): Dropout(p=0.3, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (drop_shortcut): Dropout(p=0.3, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (drop_shortcut): Dropout(p=0.3, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (drop_shortcut): Dropout(p=0.3, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (drop_shortcut): Dropout(p=0.3, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (drop_shortcut): Dropout(p=0.3, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (drop_shortcut): Dropout(p=0.3, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (drop_shortcut): Dropout(p=0.3, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (drop_shortcut): Dropout(p=0.3, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (drop_shortcut): Dropout(p=0.3, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def assign(left, right):\n",
        "    if left.shape != right.shape:\n",
        "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
        "    return torch.nn.Parameter(torch.tensor(right))\n",
        "\n",
        "def load_weights_into_gpt(gpt2o, params):\n",
        "    \"\"\"Load GPT-2o weights from a params dict into a PyTorch GPT model.\"\"\"\n",
        "\n",
        "    # --- Embeddings ---\n",
        "    gpt2o.pos_emb.weight = assign(gpt2o.pos_emb.weight, params[\"wpe\"])\n",
        "    gpt2o.tok_emb.weight = assign(gpt2o.tok_emb.weight, params[\"wte\"])\n",
        "\n",
        "    # --- Transformer Blocks ---\n",
        "    num_blocks = len(params[\"blocks\"])\n",
        "    for b in range(num_blocks):\n",
        "        block = params[\"blocks\"][b]\n",
        "\n",
        "        # === Attention Projections ===\n",
        "        # Split qkv weights\n",
        "        q_w, k_w, v_w = np.split(block[\"attn\"][\"c_attn\"][\"w\"], 3, axis=-1)\n",
        "        q_b, k_b, v_b = np.split(block[\"attn\"][\"c_attn\"][\"b\"], 3, axis=-1)\n",
        "\n",
        "        gpt2o.trf_blocks[b].att.W_query.weight = assign(\n",
        "            gpt2o.trf_blocks[b].att.W_query.weight, q_w.T)\n",
        "        gpt2o.trf_blocks[b].att.W_key.weight = assign(\n",
        "            gpt2o.trf_blocks[b].att.W_key.weight, k_w.T)\n",
        "        gpt2o.trf_blocks[b].att.W_value.weight = assign(\n",
        "            gpt2o.trf_blocks[b].att.W_value.weight, v_w.T)\n",
        "\n",
        "        gpt2o.trf_blocks[b].att.W_query.bias = assign(\n",
        "            gpt2o.trf_blocks[b].att.W_query.bias, q_b)\n",
        "        gpt2o.trf_blocks[b].att.W_key.bias = assign(\n",
        "            gpt2o.trf_blocks[b].att.W_key.bias, k_b)\n",
        "        gpt2o.trf_blocks[b].att.W_value.bias = assign(\n",
        "            gpt2o.trf_blocks[b].att.W_value.bias, v_b)\n",
        "\n",
        "        # Output projection\n",
        "        gpt2o.trf_blocks[b].att.out_proj.weight = assign(\n",
        "            gpt2o.trf_blocks[b].att.out_proj.weight, block[\"attn\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt2o.trf_blocks[b].att.out_proj.bias = assign(\n",
        "            gpt2o.trf_blocks[b].att.out_proj.bias, block[\"attn\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        # === MLP / Feed Forward ===\n",
        "        gpt2o.trf_blocks[b].ff.layers[0].weight = assign(\n",
        "            gpt2o.trf_blocks[b].ff.layers[0].weight, block[\"mlp\"][\"c_fc\"][\"w\"].T)\n",
        "        gpt2o.trf_blocks[b].ff.layers[0].bias = assign(\n",
        "            gpt2o.trf_blocks[b].ff.layers[0].bias, block[\"mlp\"][\"c_fc\"][\"b\"])\n",
        "\n",
        "        gpt2o.trf_blocks[b].ff.layers[2].weight = assign(\n",
        "            gpt2o.trf_blocks[b].ff.layers[2].weight, block[\"mlp\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt2o.trf_blocks[b].ff.layers[2].bias = assign(\n",
        "            gpt2o.trf_blocks[b].ff.layers[2].bias, block[\"mlp\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        # === LayerNorm Fix (PyTorch uses weight/bias, not scale/shift) ===\n",
        "        gpt2o.trf_blocks[b].norm1.weight = assign(\n",
        "            gpt2o.trf_blocks[b].norm1.weight, block[\"ln_1\"][\"g\"])\n",
        "        gpt2o.trf_blocks[b].norm1.bias = assign(\n",
        "            gpt2o.trf_blocks[b].norm1.bias, block[\"ln_1\"][\"b\"])\n",
        "\n",
        "        gpt2o.trf_blocks[b].norm2.weight = assign(\n",
        "            gpt2o.trf_blocks[b].norm2.weight, block[\"ln_2\"][\"g\"])\n",
        "        gpt2o.trf_blocks[b].norm2.bias = assign(\n",
        "            gpt2o.trf_blocks[b].norm2.bias, block[\"ln_2\"][\"b\"])\n",
        "\n",
        "    # --- Final LayerNorm ---\n",
        "    gpt2o.final_norm.weight = assign(gpt2o.final_norm.weight, params[\"g\"])\n",
        "    gpt2o.final_norm.bias = assign(gpt2o.final_norm.bias, params[\"b\"])\n",
        "\n",
        "    # --- Output head tied to embedding ---\n",
        "    gpt2o.out_head.weight = assign(gpt2o.out_head.weight, params[\"wte\"])\n",
        "    \n",
        "load_weights_into_gpt(gpt2o, params)\n",
        "gpt2o.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "03b2fc7f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🟨 GPT-2o (Pretrained Weights) tokenizer model output:\n",
            " large language models are not very. of is more like a real; if a- what's the size to the different- is, as are are a really people on being around the people who are. being are the people. That (and are) a) the\n",
            "\n",
            "🟨 GPT-2o loaders already ready.\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "token_ids = generate(\n",
        "    model=gpt2o,\n",
        "    idx=text_to_token_ids(\"large language models are\", tokenizer_gpt2).to(device),\n",
        "    max_new_tokens=50,\n",
        "    context_size=NEW_CONFIG[\"context_length\"],\n",
        "    top_k=25,\n",
        "    temperature=1.4\n",
        ")\n",
        "print(\"\\n🟨 GPT-2o (Pretrained Weights) tokenizer model output:\\n\", token_ids_to_text(token_ids, tokenizer_gpt2))\n",
        "print(\"\\n🟨 GPT-2o loaders already ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "57b70d15",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep 1 (Step 000000): Train loss 3.277, Val loss 3.246\n",
            "Ep 1 (Step 000050): Train loss 2.662, Val loss 2.654\n",
            "Ep 1 (Step 000100): Train loss 2.549, Val loss 2.506\n",
            "Ep 1 (Step 000150): Train loss 2.437, Val loss 2.431\n",
            "Ep 1 (Step 000200): Train loss 2.403, Val loss 2.381\n",
            "Ep 1 (Step 000250): Train loss 2.317, Val loss 2.348\n",
            "Ep 1 (Step 000300): Train loss 2.350, Val loss 2.317\n",
            "Ep 1 (Step 000350): Train loss 2.304, Val loss 2.296\n",
            "Ep 1 (Step 000400): Train loss 2.236, Val loss 2.270\n",
            "Ep 1 (Step 000450): Train loss 2.233, Val loss 2.253\n",
            "Ep 1 (Step 000500): Train loss 2.247, Val loss 2.232\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to mitigate downstream degradation, scaling laws can be applied to mitigate downstream degradation.  ### Response: excerpt discusses downstream performance metrics for different models, particularly in the context of downstream scaling laws. Specifically, downstream performance metrics are evaluated using downstream scaling laws\n",
            "Ep 2 (Step 000550): Train loss 2.234, Val loss 2.217\n",
            "Ep 2 (Step 000600): Train loss 2.176, Val loss 2.204\n",
            "Ep 2 (Step 000650): Train loss 2.199, Val loss 2.192\n",
            "Ep 2 (Step 000700): Train loss 2.201, Val loss 2.176\n",
            "Ep 2 (Step 000750): Train loss 2.196, Val loss 2.165\n",
            "Ep 2 (Step 000800): Train loss 2.123, Val loss 2.152\n",
            "Ep 2 (Step 000850): Train loss 2.085, Val loss 2.143\n",
            "Ep 2 (Step 000900): Train loss 2.094, Val loss 2.132\n",
            "Ep 2 (Step 000950): Train loss 2.108, Val loss 2.116\n",
            "Ep 2 (Step 001000): Train loss 2.114, Val loss 2.110\n",
            "Ep 2 (Step 001050): Train loss 2.065, Val loss 2.094\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to improve downstream performance, scaling laws can be applied to improve downstream performance.  ### Response: Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is around 9.5 billion parameters, indicating that\n",
            "Ep 3 (Step 001100): Train loss 2.031, Val loss 2.087\n",
            "Ep 3 (Step 001150): Train loss 2.023, Val loss 2.080\n",
            "Ep 3 (Step 001200): Train loss 2.011, Val loss 2.070\n",
            "Ep 3 (Step 001250): Train loss 2.019, Val loss 2.059\n",
            "Ep 3 (Step 001300): Train loss 1.994, Val loss 2.055\n",
            "Ep 3 (Step 001350): Train loss 2.000, Val loss 2.049\n",
            "Ep 3 (Step 001400): Train loss 1.957, Val loss 2.043\n",
            "Ep 3 (Step 001450): Train loss 1.988, Val loss 2.033\n",
            "Ep 3 (Step 001500): Train loss 1.956, Val loss 2.025\n",
            "Ep 3 (Step 001550): Train loss 1.977, Val loss 2.020\n",
            "Ep 3 (Step 001600): Train loss 1.941, Val loss 2.014\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to improve downstream performance, scaling laws can be applied to improve downstream performance.  ### Response: Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is around 9.5 billion parameters, indicating that\n",
            "Ep 4 (Step 001650): Train loss 1.959, Val loss 2.005\n",
            "Ep 4 (Step 001700): Train loss 1.936, Val loss 1.998\n",
            "Ep 4 (Step 001750): Train loss 1.933, Val loss 1.997\n",
            "Ep 4 (Step 001800): Train loss 1.916, Val loss 1.989\n",
            "Ep 4 (Step 001850): Train loss 1.892, Val loss 1.981\n",
            "Ep 4 (Step 001900): Train loss 1.879, Val loss 1.973\n",
            "Ep 4 (Step 001950): Train loss 1.862, Val loss 1.969\n",
            "Ep 4 (Step 002000): Train loss 1.865, Val loss 1.964\n",
            "Ep 4 (Step 002050): Train loss 1.866, Val loss 1.955\n",
            "Ep 4 (Step 002100): Train loss 1.834, Val loss 1.949\n",
            "Ep 4 (Step 002150): Train loss 1.880, Val loss 1.945\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to improve downstream performance, they are not sufficient to fully capture downstream performance.  ### Response: The downstream results of different models in Table 15 show that the optimal number of parameters for a 1 × 1022 FLOPs model is around 9.\n",
            "Ep 5 (Step 002200): Train loss 1.827, Val loss 1.945\n",
            "Ep 5 (Step 002250): Train loss 1.829, Val loss 1.941\n",
            "Ep 5 (Step 002300): Train loss 1.854, Val loss 1.942\n",
            "Ep 5 (Step 002350): Train loss 1.833, Val loss 1.928\n",
            "Ep 5 (Step 002400): Train loss 1.842, Val loss 1.923\n",
            "Ep 5 (Step 002450): Train loss 1.818, Val loss 1.915\n",
            "Ep 5 (Step 002500): Train loss 1.780, Val loss 1.910\n",
            "Ep 5 (Step 002550): Train loss 1.819, Val loss 1.906\n",
            "Ep 5 (Step 002600): Train loss 1.764, Val loss 1.905\n",
            "Ep 5 (Step 002650): Train loss 1.802, Val loss 1.902\n",
            "Ep 5 (Step 002700): Train loss 1.752, Val loss 1.892\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to predict downstream performance, they are not sufficient to fully capture downstream performance.  ### Response: The downstream results of different models show that the optimal number of parameters for a 1 × 1022 FLOPs model is around 9.5 billion parameters\n",
            "Ep 6 (Step 002750): Train loss 1.762, Val loss 1.895\n",
            "Ep 6 (Step 002800): Train loss 1.768, Val loss 1.886\n",
            "Ep 6 (Step 002850): Train loss 1.738, Val loss 1.884\n",
            "Ep 6 (Step 002900): Train loss 1.749, Val loss 1.877\n",
            "Ep 6 (Step 002950): Train loss 1.736, Val loss 1.874\n",
            "Ep 6 (Step 003000): Train loss 1.734, Val loss 1.869\n",
            "Ep 6 (Step 003050): Train loss 1.699, Val loss 1.863\n",
            "Ep 6 (Step 003100): Train loss 1.710, Val loss 1.865\n",
            "Ep 6 (Step 003150): Train loss 1.715, Val loss 1.858\n",
            "Ep 6 (Step 003200): Train loss 1.686, Val loss 1.849\n",
            "Ep 6 (Step 003250): Train loss 1.734, Val loss 1.851\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to predict the optimal number of parameters for a 1 × 1022 FLOP model, they may not always be optimal for downstream tasks.  ### Response: The study shows downstream results of different models in Table 15 in Table 15, showing that the\n",
            "Ep 7 (Step 003300): Train loss 1.711, Val loss 1.849\n",
            "Ep 7 (Step 003350): Train loss 1.724, Val loss 1.841\n",
            "Ep 7 (Step 003400): Train loss 1.698, Val loss 1.838\n",
            "Ep 7 (Step 003450): Train loss 1.685, Val loss 1.836\n",
            "Ep 7 (Step 003500): Train loss 1.683, Val loss 1.831\n",
            "Ep 7 (Step 003550): Train loss 1.630, Val loss 1.830\n",
            "Ep 7 (Step 003600): Train loss 1.639, Val loss 1.825\n",
            "Ep 7 (Step 003650): Train loss 1.662, Val loss 1.820\n",
            "Ep 7 (Step 003700): Train loss 1.647, Val loss 1.813\n",
            "Ep 7 (Step 003750): Train loss 1.654, Val loss 1.808\n",
            "Ep 7 (Step 003800): Train loss 1.635, Val loss 1.805\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to predict the optimal number of parameters for a 1 × 1022 FLOPs model, they may not always be optimal for downstream tasks.  ### Response: The study shows downstream results of differently-sized models in Table 15, showing that the\n",
            "Ep 8 (Step 003850): Train loss 1.597, Val loss 1.803\n",
            "Ep 8 (Step 003900): Train loss 1.610, Val loss 1.798\n",
            "Ep 8 (Step 003950): Train loss 1.632, Val loss 1.797\n",
            "Ep 8 (Step 004000): Train loss 1.613, Val loss 1.796\n",
            "Ep 8 (Step 004050): Train loss 1.624, Val loss 1.786\n",
            "Ep 8 (Step 004100): Train loss 1.636, Val loss 1.784\n",
            "Ep 8 (Step 004150): Train loss 1.573, Val loss 1.781\n",
            "Ep 8 (Step 004200): Train loss 1.643, Val loss 1.780\n",
            "Ep 8 (Step 004250): Train loss 1.605, Val loss 1.775\n",
            "Ep 8 (Step 004300): Train loss 1.565, Val loss 1.769\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to predict downstream performance, they are not sufficient to fully capture the full potential of the model.  ### Response: The study shows downstream performance of different models in Table 15, showing that the optimal number of parameters for a 1 × 1022 FL\n",
            "Ep 9 (Step 004350): Train loss 1.618, Val loss 1.769\n",
            "Ep 9 (Step 004400): Train loss 1.563, Val loss 1.770\n",
            "Ep 9 (Step 004450): Train loss 1.549, Val loss 1.763\n",
            "Ep 9 (Step 004500): Train loss 1.573, Val loss 1.762\n",
            "Ep 9 (Step 004550): Train loss 1.567, Val loss 1.759\n",
            "Ep 9 (Step 004600): Train loss 1.567, Val loss 1.754\n",
            "Ep 9 (Step 004650): Train loss 1.572, Val loss 1.755\n",
            "Ep 9 (Step 004700): Train loss 1.562, Val loss 1.748\n",
            "Ep 9 (Step 004750): Train loss 1.558, Val loss 1.742\n",
            "Ep 9 (Step 004800): Train loss 1.533, Val loss 1.737\n",
            "Ep 9 (Step 004850): Train loss 1.554, Val loss 1.733\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to predict optimal model parameters, they are not sufficient to fully capture downstream performance.  ### Response: The study shows downstream performance of different models in Table 15, showing that the optimal number of parameters for a 1 × 1022 FLOPs model\n",
            "Ep 10 (Step 004900): Train loss 1.537, Val loss 1.731\n",
            "Ep 10 (Step 004950): Train loss 1.534, Val loss 1.729\n",
            "Ep 10 (Step 005000): Train loss 1.513, Val loss 1.726\n",
            "Ep 10 (Step 005050): Train loss 1.485, Val loss 1.722\n",
            "Ep 10 (Step 005100): Train loss 1.484, Val loss 1.717\n",
            "Ep 10 (Step 005150): Train loss 1.486, Val loss 1.717\n",
            "Ep 10 (Step 005200): Train loss 1.487, Val loss 1.711\n",
            "Ep 10 (Step 005250): Train loss 1.483, Val loss 1.710\n",
            "Ep 10 (Step 005300): Train loss 1.480, Val loss 1.707\n",
            "Ep 10 (Step 005350): Train loss 1.461, Val loss 1.701\n",
            "Ep 10 (Step 005400): Train loss 1.489, Val loss 1.699\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to predict optimal model performance, they are not sufficient to fully capture downstream performance.  ### Response: The study shows downstream performance differences between different models in Table 15, showing that the optimal number of parameters for a 1 × 1022 FLOPs\n",
            "Ep 11 (Step 005450): Train loss 1.482, Val loss 1.701\n",
            "Ep 11 (Step 005500): Train loss 1.465, Val loss 1.693\n",
            "Ep 11 (Step 005550): Train loss 1.415, Val loss 1.696\n",
            "Ep 11 (Step 005600): Train loss 1.429, Val loss 1.691\n",
            "Ep 11 (Step 005650): Train loss 1.463, Val loss 1.688\n",
            "Ep 11 (Step 005700): Train loss 1.448, Val loss 1.686\n",
            "Ep 11 (Step 005750): Train loss 1.470, Val loss 1.682\n",
            "Ep 11 (Step 005800): Train loss 1.416, Val loss 1.680\n",
            "Ep 11 (Step 005850): Train loss 1.419, Val loss 1.677\n",
            "Ep 11 (Step 005900): Train loss 1.409, Val loss 1.668\n",
            "Ep 11 (Step 005950): Train loss 1.419, Val loss 1.669\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to predict optimal model performance, they do not fully capture the true scaling dynamics of the model.  ### Response: The study shows downstream performance of different-sized models in Table 15, showing that the optimal number of parameters for a 1 × 10\n",
            "Ep 12 (Step 006000): Train loss 1.432, Val loss 1.669\n",
            "Ep 12 (Step 006050): Train loss 1.423, Val loss 1.664\n",
            "Ep 12 (Step 006100): Train loss 1.394, Val loss 1.659\n",
            "Ep 12 (Step 006150): Train loss 1.387, Val loss 1.655\n",
            "Ep 12 (Step 006200): Train loss 1.396, Val loss 1.652\n",
            "Ep 12 (Step 006250): Train loss 1.403, Val loss 1.647\n",
            "Ep 12 (Step 006300): Train loss 1.369, Val loss 1.645\n",
            "Ep 12 (Step 006350): Train loss 1.415, Val loss 1.649\n",
            "Ep 12 (Step 006400): Train loss 1.366, Val loss 1.641\n",
            "Ep 12 (Step 006450): Train loss 1.368, Val loss 1.640\n",
            "Ep 12 (Step 006500): Train loss 1.357, Val loss 1.636\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to predict optimal model performance, they are not sufficient to fully capture the full potential of a 1 × 1022 FLOP model.  ### Response: The study shows downstream performance of different models in Table 15, showing that the optimal number of parameters\n",
            "Ep 13 (Step 006550): Train loss 1.368, Val loss 1.638\n",
            "Ep 13 (Step 006600): Train loss 1.371, Val loss 1.636\n",
            "Ep 13 (Step 006650): Train loss 1.344, Val loss 1.633\n",
            "Ep 13 (Step 006700): Train loss 1.363, Val loss 1.633\n",
            "Ep 13 (Step 006750): Train loss 1.346, Val loss 1.628\n",
            "Ep 13 (Step 006800): Train loss 1.334, Val loss 1.624\n",
            "Ep 13 (Step 006850): Train loss 1.365, Val loss 1.618\n",
            "Ep 13 (Step 006900): Train loss 1.326, Val loss 1.616\n",
            "Ep 13 (Step 006950): Train loss 1.320, Val loss 1.617\n",
            "Ep 13 (Step 007000): Train loss 1.310, Val loss 1.612\n",
            "Ep 13 (Step 007050): Train loss 1.293, Val loss 1.605\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to predict optimal model performance, they do not always capture the full potential of the model.  ### Response: The study shows that the optimal number of parameters for a 1 × 1022 FLOPs model is around 9.5 billion, closely\n",
            "Ep 14 (Step 007100): Train loss 1.304, Val loss 1.605\n",
            "Ep 14 (Step 007150): Train loss 1.343, Val loss 1.604\n",
            "Ep 14 (Step 007200): Train loss 1.299, Val loss 1.598\n",
            "Ep 14 (Step 007250): Train loss 1.317, Val loss 1.598\n",
            "Ep 14 (Step 007300): Train loss 1.329, Val loss 1.596\n",
            "Ep 14 (Step 007350): Train loss 1.283, Val loss 1.593\n",
            "Ep 14 (Step 007400): Train loss 1.280, Val loss 1.587\n",
            "Ep 14 (Step 007450): Train loss 1.288, Val loss 1.585\n",
            "Ep 14 (Step 007500): Train loss 1.271, Val loss 1.583\n",
            "Ep 14 (Step 007550): Train loss 1.285, Val loss 1.578\n",
            "Ep 14 (Step 007600): Train loss 1.251, Val loss 1.577\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to predict optimal model performance, they do not fully capture the true scaling dynamics of the training data.  ### Response: The study shows downstream performance of differently-sized models in Table 15, showing that the optimal number of parameters for a 1 ×\n",
            "Ep 15 (Step 007650): Train loss 1.264, Val loss 1.577\n",
            "Ep 15 (Step 007700): Train loss 1.253, Val loss 1.570\n",
            "Ep 15 (Step 007750): Train loss 1.266, Val loss 1.568\n",
            "Ep 15 (Step 007800): Train loss 1.253, Val loss 1.567\n",
            "Ep 15 (Step 007850): Train loss 1.273, Val loss 1.566\n",
            "Ep 15 (Step 007900): Train loss 1.264, Val loss 1.565\n",
            "Ep 15 (Step 007950): Train loss 1.233, Val loss 1.567\n",
            "Ep 15 (Step 008000): Train loss 1.246, Val loss 1.558\n",
            "Ep 15 (Step 008050): Train loss 1.227, Val loss 1.557\n",
            "Ep 15 (Step 008100): Train loss 1.217, Val loss 1.553\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to predict optimal model sizes, they do not fully capture the complexities of downstream tasks.  ### Response: The study shows downstream performance of different-sized models in Table 15, showing that the optimal number of parameters for a 1 × 1022 FL\n",
            "Ep 16 (Step 008150): Train loss 1.236, Val loss 1.549\n",
            "Ep 16 (Step 008200): Train loss 1.203, Val loss 1.547\n",
            "Ep 16 (Step 008250): Train loss 1.217, Val loss 1.547\n",
            "Ep 16 (Step 008300): Train loss 1.194, Val loss 1.545\n",
            "Ep 16 (Step 008350): Train loss 1.250, Val loss 1.548\n",
            "Ep 16 (Step 008400): Train loss 1.221, Val loss 1.538\n",
            "Ep 16 (Step 008450): Train loss 1.208, Val loss 1.534\n",
            "Ep 16 (Step 008500): Train loss 1.210, Val loss 1.530\n",
            "Ep 16 (Step 008550): Train loss 1.177, Val loss 1.529\n",
            "Ep 16 (Step 008600): Train loss 1.196, Val loss 1.525\n",
            "Ep 16 (Step 008650): Train loss 1.186, Val loss 1.521\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to predict optimal model sizes, they may not always be the best choice for downstream tasks.  ### Response: The study shows that different-sized models perform well on downstream tasks, with the 9.5B model showing the lowest loss and the\n",
            "Ep 17 (Step 008700): Train loss 1.201, Val loss 1.523\n",
            "Ep 17 (Step 008750): Train loss 1.179, Val loss 1.522\n",
            "Ep 17 (Step 008800): Train loss 1.155, Val loss 1.520\n",
            "Ep 17 (Step 008850): Train loss 1.172, Val loss 1.517\n",
            "Ep 17 (Step 008900): Train loss 1.182, Val loss 1.513\n",
            "Ep 17 (Step 008950): Train loss 1.146, Val loss 1.513\n",
            "Ep 17 (Step 009000): Train loss 1.146, Val loss 1.507\n",
            "Ep 17 (Step 009050): Train loss 1.156, Val loss 1.504\n",
            "Ep 17 (Step 009100): Train loss 1.127, Val loss 1.504\n",
            "Ep 17 (Step 009150): Train loss 1.140, Val loss 1.499\n",
            "Ep 17 (Step 009200): Train loss 1.144, Val loss 1.494\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to predict optimal model sizes, they are not sufficient to fully capture the true scaling behavior of a 1 × 1022 FLOP model.  ### Response: The study shows downstream performance of differently-sized models in Table 15, showing that the optimal\n",
            "Ep 18 (Step 009250): Train loss 1.157, Val loss 1.496\n",
            "Ep 18 (Step 009300): Train loss 1.154, Val loss 1.490\n",
            "Ep 18 (Step 009350): Train loss 1.149, Val loss 1.491\n",
            "Ep 18 (Step 009400): Train loss 1.106, Val loss 1.492\n",
            "Ep 18 (Step 009450): Train loss 1.109, Val loss 1.489\n",
            "Ep 18 (Step 009500): Train loss 1.119, Val loss 1.482\n",
            "Ep 18 (Step 009550): Train loss 1.116, Val loss 1.483\n",
            "Ep 18 (Step 009600): Train loss 1.132, Val loss 1.478\n",
            "Ep 18 (Step 009650): Train loss 1.104, Val loss 1.471\n",
            "Ep 18 (Step 009700): Train loss 1.118, Val loss 1.471\n",
            "Ep 18 (Step 009750): Train loss 1.098, Val loss 1.471\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to predict optimal model sizes, they do not fully capture the complexities of downstream task performance.  ### Response: The study shows downstream performance of differently-sized models in Table 15, showing that the optimal number of parameters for a 1 × 1022\n",
            "Ep 19 (Step 009800): Train loss 1.096, Val loss 1.469\n",
            "Ep 19 (Step 009850): Train loss 1.128, Val loss 1.468\n",
            "Ep 19 (Step 009900): Train loss 1.099, Val loss 1.465\n",
            "Ep 19 (Step 009950): Train loss 1.083, Val loss 1.466\n",
            "Ep 19 (Step 010000): Train loss 1.110, Val loss 1.461\n",
            "Ep 19 (Step 010050): Train loss 1.089, Val loss 1.462\n",
            "Ep 19 (Step 010100): Train loss 1.054, Val loss 1.457\n",
            "Ep 19 (Step 010150): Train loss 1.076, Val loss 1.458\n",
            "Ep 19 (Step 010200): Train loss 1.055, Val loss 1.451\n",
            "Ep 19 (Step 010250): Train loss 1.055, Val loss 1.449\n",
            "Ep 19 (Step 010300): Train loss 1.050, Val loss 1.446\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to predict optimal model sizes, they do not fully capture the complexities of downstream task performance.  ### Response: The passage discusses downstream performance differences between different models, noting that the optimal number of parameters for a 1 × 1022 FLOPs model\n",
            "Ep 20 (Step 010350): Train loss 1.038, Val loss 1.445\n",
            "Ep 20 (Step 010400): Train loss 1.062, Val loss 1.443\n",
            "Ep 20 (Step 010450): Train loss 1.063, Val loss 1.436\n",
            "Ep 20 (Step 010500): Train loss 1.045, Val loss 1.435\n",
            "Ep 20 (Step 010550): Train loss 1.013, Val loss 1.432\n",
            "Ep 20 (Step 010600): Train loss 1.044, Val loss 1.430\n",
            "Ep 20 (Step 010650): Train loss 1.049, Val loss 1.431\n",
            "Ep 20 (Step 010700): Train loss 1.049, Val loss 1.427\n",
            "Ep 20 (Step 010750): Train loss 1.042, Val loss 1.426\n",
            "Ep 20 (Step 010800): Train loss 1.016, Val loss 1.424\n",
            "Ep 20 (Step 010850): Train loss 1.010, Val loss 1.419\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to predict optimal model sizes, they do not fully capture the complexities of downstream tasks.  ### Response: The passage discusses downstream performance differences between different-sized models, showing that the optimal number of parameters for a 1 × 1022 FLOPs\n",
            "Ep 21 (Step 010900): Train loss 0.992, Val loss 1.420\n",
            "Ep 21 (Step 010950): Train loss 1.008, Val loss 1.418\n",
            "Ep 21 (Step 011000): Train loss 0.983, Val loss 1.418\n",
            "Ep 21 (Step 011050): Train loss 1.008, Val loss 1.418\n",
            "Ep 21 (Step 011100): Train loss 0.999, Val loss 1.414\n",
            "Ep 21 (Step 011150): Train loss 1.004, Val loss 1.414\n",
            "Ep 21 (Step 011200): Train loss 1.003, Val loss 1.410\n",
            "Ep 21 (Step 011250): Train loss 0.976, Val loss 1.408\n",
            "Ep 21 (Step 011300): Train loss 1.014, Val loss 1.402\n",
            "Ep 21 (Step 011350): Train loss 0.987, Val loss 1.398\n",
            "Ep 21 (Step 011400): Train loss 0.968, Val loss 1.396\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to predict optimal model performance, they do not fully capture the complexities of downstream tasks.  ### Response: The study shows that smaller models perform better on downstream tasks compared to larger models, with the 9.5B model showing the lowest loss and\n",
            "Ep 22 (Step 011450): Train loss 0.954, Val loss 1.398\n",
            "Ep 22 (Step 011500): Train loss 0.980, Val loss 1.396\n",
            "Ep 22 (Step 011550): Train loss 0.976, Val loss 1.395\n",
            "Ep 22 (Step 011600): Train loss 0.967, Val loss 1.390\n",
            "Ep 22 (Step 011650): Train loss 0.964, Val loss 1.386\n",
            "Ep 22 (Step 011700): Train loss 0.964, Val loss 1.389\n",
            "Ep 22 (Step 011750): Train loss 0.961, Val loss 1.387\n",
            "Ep 22 (Step 011800): Train loss 0.959, Val loss 1.382\n",
            "Ep 22 (Step 011850): Train loss 0.970, Val loss 1.385\n",
            "Ep 22 (Step 011900): Train loss 0.964, Val loss 1.381\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to predict optimal model sizes, they do not fully capture the complexities of downstream tasks.  ### Response: The passage discusses downstream performance differences between different-sized models, showing that the optimal number of parameters for a 1 × 1022 FLOPs\n",
            "Ep 23 (Step 011950): Train loss 0.968, Val loss 1.377\n",
            "Ep 23 (Step 012000): Train loss 0.963, Val loss 1.378\n",
            "Ep 23 (Step 012050): Train loss 0.950, Val loss 1.373\n",
            "Ep 23 (Step 012100): Train loss 0.946, Val loss 1.373\n",
            "Ep 23 (Step 012150): Train loss 0.939, Val loss 1.368\n",
            "Ep 23 (Step 012200): Train loss 0.932, Val loss 1.366\n",
            "Ep 23 (Step 012250): Train loss 0.930, Val loss 1.362\n",
            "Ep 23 (Step 012300): Train loss 0.906, Val loss 1.363\n",
            "Ep 23 (Step 012350): Train loss 0.928, Val loss 1.365\n",
            "Ep 23 (Step 012400): Train loss 0.915, Val loss 1.364\n",
            "Ep 23 (Step 012450): Train loss 0.924, Val loss 1.357\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to predict optimal model sizes, they do not fully capture the complexities of downstream task performance.  ### Response: The study shows that smaller models perform better on downstream tasks, but they fall short of the optimal model size, suggesting that scaling laws alone\n",
            "Ep 24 (Step 012500): Train loss 0.905, Val loss 1.355\n",
            "Ep 24 (Step 012550): Train loss 0.921, Val loss 1.355\n",
            "Ep 24 (Step 012600): Train loss 0.919, Val loss 1.346\n",
            "Ep 24 (Step 012650): Train loss 0.929, Val loss 1.349\n",
            "Ep 24 (Step 012700): Train loss 0.901, Val loss 1.347\n",
            "Ep 24 (Step 012750): Train loss 0.910, Val loss 1.347\n",
            "Ep 24 (Step 012800): Train loss 0.905, Val loss 1.345\n",
            "Ep 24 (Step 012850): Train loss 0.876, Val loss 1.340\n",
            "Ep 24 (Step 012900): Train loss 0.905, Val loss 1.339\n",
            "Ep 24 (Step 012950): Train loss 0.891, Val loss 1.335\n",
            "Ep 24 (Step 013000): Train loss 0.855, Val loss 1.334\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to predict optimal model sizes, they do not fully capture the complexities of downstream task performance.  ### Response: The passage discusses downstream performance differences between different-sized models, showing that the optimal number of parameters for a 1 × 1022 FLOP\n",
            "Ep 25 (Step 013050): Train loss 0.900, Val loss 1.339\n",
            "Ep 25 (Step 013100): Train loss 0.871, Val loss 1.333\n",
            "Ep 25 (Step 013150): Train loss 0.868, Val loss 1.330\n",
            "Ep 25 (Step 013200): Train loss 0.866, Val loss 1.329\n",
            "Ep 25 (Step 013250): Train loss 0.877, Val loss 1.326\n",
            "Ep 25 (Step 013300): Train loss 0.870, Val loss 1.323\n",
            "Ep 25 (Step 013350): Train loss 0.874, Val loss 1.322\n",
            "Ep 25 (Step 013400): Train loss 0.868, Val loss 1.317\n",
            "Ep 25 (Step 013450): Train loss 0.852, Val loss 1.321\n",
            "Ep 25 (Step 013500): Train loss 0.862, Val loss 1.315\n",
            "Ep 25 (Step 013550): Train loss 0.862, Val loss 1.314\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to determine optimal model sizes, they do not fully capture the complexities of downstream tasks.  ### Response: The study shows downstream performance differences between differently-sized models in Table 15, showing that the optimal number of parameters for a 1 × 1022\n",
            "Ep 26 (Step 013600): Train loss 0.829, Val loss 1.312\n",
            "Ep 26 (Step 013650): Train loss 0.859, Val loss 1.314\n",
            "Ep 26 (Step 013700): Train loss 0.838, Val loss 1.309\n",
            "Ep 26 (Step 013750): Train loss 0.835, Val loss 1.304\n",
            "Ep 26 (Step 013800): Train loss 0.853, Val loss 1.305\n",
            "Ep 26 (Step 013850): Train loss 0.820, Val loss 1.307\n",
            "Ep 26 (Step 013900): Train loss 0.839, Val loss 1.302\n",
            "Ep 26 (Step 013950): Train loss 0.834, Val loss 1.300\n",
            "Ep 26 (Step 014000): Train loss 0.823, Val loss 1.299\n",
            "Ep 26 (Step 014050): Train loss 0.831, Val loss 1.296\n",
            "Ep 26 (Step 014100): Train loss 0.815, Val loss 1.295\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to determine optimal model sizes, they cannot always be generalizable.  ### Response: The passage discusses downstream performance differences between different models, showing that the optimal number of parameters for a 1 × 1022 FLOPs model is around 9.5\n",
            "Ep 27 (Step 014150): Train loss 0.819, Val loss 1.294\n",
            "Ep 27 (Step 014200): Train loss 0.823, Val loss 1.292\n",
            "Ep 27 (Step 014250): Train loss 0.799, Val loss 1.290\n",
            "Ep 27 (Step 014300): Train loss 0.818, Val loss 1.289\n",
            "Ep 27 (Step 014350): Train loss 0.805, Val loss 1.283\n",
            "Ep 27 (Step 014400): Train loss 0.842, Val loss 1.285\n",
            "Ep 27 (Step 014450): Train loss 0.811, Val loss 1.279\n",
            "Ep 27 (Step 014500): Train loss 0.797, Val loss 1.281\n",
            "Ep 27 (Step 014550): Train loss 0.796, Val loss 1.284\n",
            "Ep 27 (Step 014600): Train loss 0.812, Val loss 1.276\n",
            "Ep 27 (Step 014650): Train loss 0.788, Val loss 1.268\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to predict optimal model sizes, they do not fully capture the complexities of downstream task performance.  ### Response: The study shows that different-sized models perform well on downstream tasks, with the 9.5B model showing the lowest loss and the\n",
            "Ep 28 (Step 014700): Train loss 0.768, Val loss 1.271\n",
            "Ep 28 (Step 014750): Train loss 0.807, Val loss 1.276\n",
            "Ep 28 (Step 014800): Train loss 0.817, Val loss 1.270\n",
            "Ep 28 (Step 014850): Train loss 0.785, Val loss 1.267\n",
            "Ep 28 (Step 014900): Train loss 0.786, Val loss 1.265\n",
            "Ep 28 (Step 014950): Train loss 0.799, Val loss 1.265\n",
            "Ep 28 (Step 015000): Train loss 0.777, Val loss 1.263\n",
            "Ep 28 (Step 015050): Train loss 0.778, Val loss 1.258\n",
            "Ep 28 (Step 015100): Train loss 0.762, Val loss 1.258\n",
            "Ep 28 (Step 015150): Train loss 0.766, Val loss 1.258\n",
            "Ep 28 (Step 015200): Train loss 0.739, Val loss 1.255\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to determine optimal model sizes, they do not always generalize well to very long contexts.  ### Response: The passage discusses downstream performance differences between different models, showing that models with 1 × 1022 FLOPs often perform well on downstream tasks\n",
            "Ep 29 (Step 015250): Train loss 0.780, Val loss 1.255\n",
            "Ep 29 (Step 015300): Train loss 0.726, Val loss 1.252\n",
            "Ep 29 (Step 015350): Train loss 0.745, Val loss 1.249\n",
            "Ep 29 (Step 015400): Train loss 0.756, Val loss 1.246\n",
            "Ep 29 (Step 015450): Train loss 0.733, Val loss 1.245\n",
            "Ep 29 (Step 015500): Train loss 0.745, Val loss 1.243\n",
            "Ep 29 (Step 015550): Train loss 0.743, Val loss 1.242\n",
            "Ep 29 (Step 015600): Train loss 0.754, Val loss 1.240\n",
            "Ep 29 (Step 015650): Train loss 0.736, Val loss 1.239\n",
            "Ep 29 (Step 015700): Train loss 0.760, Val loss 1.238\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to predict loss, they do not fully capture the complexities of downstream tasks.  ### Response: The passage discusses downstream performance differences between different models, showing that the optimal number of parameters for a 1 × 1022 FLOPs model is around 9\n",
            "Ep 30 (Step 015750): Train loss 0.738, Val loss 1.233\n",
            "Ep 30 (Step 015800): Train loss 0.737, Val loss 1.236\n",
            "Ep 30 (Step 015850): Train loss 0.746, Val loss 1.235\n",
            "Ep 30 (Step 015900): Train loss 0.729, Val loss 1.233\n",
            "Ep 30 (Step 015950): Train loss 0.740, Val loss 1.233\n",
            "Ep 30 (Step 016000): Train loss 0.728, Val loss 1.233\n",
            "Ep 30 (Step 016050): Train loss 0.700, Val loss 1.227\n",
            "Ep 30 (Step 016100): Train loss 0.737, Val loss 1.226\n",
            "Ep 30 (Step 016150): Train loss 0.709, Val loss 1.223\n",
            "Ep 30 (Step 016200): Train loss 0.727, Val loss 1.222\n",
            "Ep 30 (Step 016250): Train loss 0.699, Val loss 1.221\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to determine optimal model sizes, they do not always generalize well to very long context tasks.  ### Response: The passage discusses downstream performance differences between different-sized models, showing that the optimal number of parameters for a 1 × 1022 FL\n",
            "Ep 31 (Step 016300): Train loss 0.711, Val loss 1.217\n",
            "Ep 31 (Step 016350): Train loss 0.719, Val loss 1.220\n",
            "Ep 31 (Step 016400): Train loss 0.697, Val loss 1.218\n",
            "Ep 31 (Step 016450): Train loss 0.682, Val loss 1.216\n",
            "Ep 31 (Step 016500): Train loss 0.699, Val loss 1.217\n",
            "Ep 31 (Step 016550): Train loss 0.688, Val loss 1.216\n",
            "Ep 31 (Step 016600): Train loss 0.705, Val loss 1.213\n",
            "Ep 31 (Step 016650): Train loss 0.714, Val loss 1.205\n",
            "Ep 31 (Step 016700): Train loss 0.693, Val loss 1.204\n",
            "Ep 31 (Step 016750): Train loss 0.710, Val loss 1.203\n",
            "Ep 31 (Step 016800): Train loss 0.709, Val loss 1.198\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to predict optimal model sizes, they do not always hold for very large models.  ### Response: The passage discusses downstream performance differences between different-sized models, showing that the optimal number of parameters for a 1 × 1022 FLOPs model\n",
            "Ep 32 (Step 016850): Train loss 0.659, Val loss 1.202\n",
            "Ep 32 (Step 016900): Train loss 0.685, Val loss 1.201\n",
            "Ep 32 (Step 016950): Train loss 0.668, Val loss 1.195\n",
            "Ep 32 (Step 017000): Train loss 0.676, Val loss 1.196\n",
            "Ep 32 (Step 017050): Train loss 0.692, Val loss 1.195\n",
            "Ep 32 (Step 017100): Train loss 0.668, Val loss 1.197\n",
            "Ep 32 (Step 017150): Train loss 0.682, Val loss 1.194\n",
            "Ep 32 (Step 017200): Train loss 0.671, Val loss 1.189\n",
            "Ep 32 (Step 017250): Train loss 0.675, Val loss 1.192\n",
            "Ep 32 (Step 017300): Train loss 0.702, Val loss 1.187\n",
            "Ep 32 (Step 017350): Train loss 0.684, Val loss 1.187\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to predict optimal model sizes, they cannot be rigorously evaluated.  ### Response: The passage discusses downstream performance of differently-sized models, showing that the optimal number of parameters for a 1 × 10² FLOPs model is around 9.\n",
            "Ep 33 (Step 017400): Train loss 0.656, Val loss 1.187\n",
            "Ep 33 (Step 017450): Train loss 0.659, Val loss 1.184\n",
            "Ep 33 (Step 017500): Train loss 0.659, Val loss 1.182\n",
            "Ep 33 (Step 017550): Train loss 0.659, Val loss 1.182\n",
            "Ep 33 (Step 017600): Train loss 0.659, Val loss 1.178\n",
            "Ep 33 (Step 017650): Train loss 0.669, Val loss 1.179\n",
            "Ep 33 (Step 017700): Train loss 0.673, Val loss 1.170\n",
            "Ep 33 (Step 017750): Train loss 0.648, Val loss 1.174\n",
            "Ep 33 (Step 017800): Train loss 0.664, Val loss 1.167\n",
            "Ep 33 (Step 017850): Train loss 0.635, Val loss 1.167\n",
            "Ep 33 (Step 017900): Train loss 0.658, Val loss 1.168\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to determine optimal model sizes, they cannot be generalizable to all downstream tasks.  ### Response: The passage discusses downstream performance differences between different models, showing that the optimal number of parameters for a 1 × 10² FLOPs model is around\n",
            "Ep 34 (Step 017950): Train loss 0.631, Val loss 1.177\n",
            "Ep 34 (Step 018000): Train loss 0.665, Val loss 1.170\n",
            "Ep 34 (Step 018050): Train loss 0.657, Val loss 1.169\n",
            "Ep 34 (Step 018100): Train loss 0.631, Val loss 1.169\n",
            "Ep 34 (Step 018150): Train loss 0.631, Val loss 1.164\n",
            "Ep 34 (Step 018200): Train loss 0.626, Val loss 1.164\n",
            "Ep 34 (Step 018250): Train loss 0.624, Val loss 1.163\n",
            "Ep 34 (Step 018300): Train loss 0.631, Val loss 1.158\n",
            "Ep 34 (Step 018350): Train loss 0.609, Val loss 1.160\n",
            "Ep 34 (Step 018400): Train loss 0.626, Val loss 1.157\n",
            "Ep 34 (Step 018450): Train loss 0.627, Val loss 1.152\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to predict optimal model sizes, they are not generalizable and cannot generalize well to very long training schedules.  ### Response: The passage discusses downstream performance differences between different models, showing that the optimal number of parameters for a 1 × 1022\n",
            "Ep 35 (Step 018500): Train loss 0.609, Val loss 1.155\n",
            "Ep 35 (Step 018550): Train loss 0.629, Val loss 1.155\n",
            "Ep 35 (Step 018600): Train loss 0.624, Val loss 1.154\n",
            "Ep 35 (Step 018650): Train loss 0.613, Val loss 1.152\n",
            "Ep 35 (Step 018700): Train loss 0.596, Val loss 1.147\n",
            "Ep 35 (Step 018750): Train loss 0.613, Val loss 1.145\n",
            "Ep 35 (Step 018800): Train loss 0.621, Val loss 1.149\n",
            "Ep 35 (Step 018850): Train loss 0.631, Val loss 1.143\n",
            "Ep 35 (Step 018900): Train loss 0.608, Val loss 1.144\n",
            "Ep 35 (Step 018950): Train loss 0.617, Val loss 1.135\n",
            "Ep 35 (Step 019000): Train loss 0.609, Val loss 1.134\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to predict optimal model sizes, they may not always be the best metric for downstream tasks.  ### Response: The study shows that smaller models perform better on downstream tasks, but they fall short of matching the training loss and scaling predictions. For instance\n",
            "Ep 36 (Step 019050): Train loss 0.615, Val loss 1.139\n",
            "Ep 36 (Step 019100): Train loss 0.613, Val loss 1.135\n",
            "Ep 36 (Step 019150): Train loss 0.595, Val loss 1.141\n",
            "Ep 36 (Step 019200): Train loss 0.605, Val loss 1.137\n",
            "Ep 36 (Step 019250): Train loss 0.598, Val loss 1.128\n",
            "Ep 36 (Step 019300): Train loss 0.582, Val loss 1.129\n",
            "Ep 36 (Step 019350): Train loss 0.579, Val loss 1.130\n",
            "Ep 36 (Step 019400): Train loss 0.579, Val loss 1.129\n",
            "Ep 36 (Step 019450): Train loss 0.582, Val loss 1.132\n",
            "Ep 36 (Step 019500): Train loss 0.588, Val loss 1.128\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to predict optimal model sizes, they cannot be generalizable to all downstream tasks.  ### Response: The passage discusses downstream performance differences between different models, showing that the optimal number of parameters for a 1 × 1022 FLOPs model is around\n",
            "Ep 37 (Step 019550): Train loss 0.593, Val loss 1.125\n",
            "Ep 37 (Step 019600): Train loss 0.582, Val loss 1.122\n",
            "Ep 37 (Step 019650): Train loss 0.585, Val loss 1.128\n",
            "Ep 37 (Step 019700): Train loss 0.577, Val loss 1.118\n",
            "Ep 37 (Step 019750): Train loss 0.587, Val loss 1.117\n",
            "Ep 37 (Step 019800): Train loss 0.577, Val loss 1.119\n",
            "Ep 37 (Step 019850): Train loss 0.580, Val loss 1.115\n",
            "Ep 37 (Step 019900): Train loss 0.577, Val loss 1.116\n",
            "Ep 37 (Step 019950): Train loss 0.573, Val loss 1.115\n",
            "Ep 37 (Step 020000): Train loss 0.552, Val loss 1.115\n",
            "Ep 37 (Step 020050): Train loss 0.591, Val loss 1.112\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to predict optimal model sizes, they do not generalize well to very long training schedules.  ### Response: The study shows that smaller models perform better on downstream tasks, but they fall short of matching the training loss and scaling predictions. For instance\n",
            "Ep 38 (Step 020100): Train loss 0.563, Val loss 1.111\n",
            "Ep 38 (Step 020150): Train loss 0.562, Val loss 1.113\n",
            "Ep 38 (Step 020200): Train loss 0.578, Val loss 1.114\n",
            "Ep 38 (Step 020250): Train loss 0.563, Val loss 1.114\n",
            "Ep 38 (Step 020300): Train loss 0.557, Val loss 1.112\n",
            "Ep 38 (Step 020350): Train loss 0.554, Val loss 1.107\n",
            "Ep 38 (Step 020400): Train loss 0.550, Val loss 1.106\n",
            "Ep 38 (Step 020450): Train loss 0.549, Val loss 1.108\n",
            "Ep 38 (Step 020500): Train loss 0.558, Val loss 1.105\n",
            "Ep 38 (Step 020550): Train loss 0.553, Val loss 1.099\n",
            "Ep 38 (Step 020600): Train loss 0.550, Val loss 1.100\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to select optimal model sizes, they cannot be generalizable.  ### Response: The passage discusses downstream performance differences between different-sized models, noting that the optimal number of parameters for a 1 × 1022 FLOPs model is around 9.\n",
            "Ep 39 (Step 020650): Train loss 0.547, Val loss 1.101\n",
            "Ep 39 (Step 020700): Train loss 0.551, Val loss 1.099\n",
            "Ep 39 (Step 020750): Train loss 0.551, Val loss 1.099\n",
            "Ep 39 (Step 020800): Train loss 0.562, Val loss 1.101\n",
            "Ep 39 (Step 020850): Train loss 0.540, Val loss 1.099\n",
            "Ep 39 (Step 020900): Train loss 0.531, Val loss 1.093\n",
            "Ep 39 (Step 020950): Train loss 0.531, Val loss 1.092\n",
            "Ep 39 (Step 021000): Train loss 0.539, Val loss 1.093\n",
            "Ep 39 (Step 021050): Train loss 0.538, Val loss 1.092\n",
            "Ep 39 (Step 021100): Train loss 0.538, Val loss 1.090\n",
            "Ep 39 (Step 021150): Train loss 0.538, Val loss 1.087\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to predict optimal model sizes, they cannot be exhaustive in identifying the optimal model sizes.  ### Response: The passage discusses downstream performance of different-sized models, showing that the optimal number of parameters for a 1 × 10² FLOPs model\n",
            "Ep 40 (Step 021200): Train loss 0.545, Val loss 1.086\n",
            "Ep 40 (Step 021250): Train loss 0.514, Val loss 1.087\n",
            "Ep 40 (Step 021300): Train loss 0.508, Val loss 1.085\n",
            "Ep 40 (Step 021350): Train loss 0.521, Val loss 1.084\n",
            "Ep 40 (Step 021400): Train loss 0.510, Val loss 1.087\n",
            "Ep 40 (Step 021450): Train loss 0.528, Val loss 1.083\n",
            "Ep 40 (Step 021500): Train loss 0.544, Val loss 1.079\n",
            "Ep 40 (Step 021550): Train loss 0.515, Val loss 1.078\n",
            "Ep 40 (Step 021600): Train loss 0.524, Val loss 1.076\n",
            "Ep 40 (Step 021650): Train loss 0.518, Val loss 1.078\n",
            "Ep 40 (Step 021700): Train loss 0.526, Val loss 1.077\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to guide optimal model performance, they may not always be the best interpretable metric.  ### Response: The passage discusses downstream performance of different-sized models, showing that the optimal number of parameters for a 1 × 10² FLOPs model\n",
            "Ep 41 (Step 021750): Train loss 0.514, Val loss 1.079\n",
            "Ep 41 (Step 021800): Train loss 0.510, Val loss 1.081\n",
            "Ep 41 (Step 021850): Train loss 0.516, Val loss 1.083\n",
            "Ep 41 (Step 021900): Train loss 0.505, Val loss 1.079\n",
            "Ep 41 (Step 021950): Train loss 0.493, Val loss 1.076\n",
            "Ep 41 (Step 022000): Train loss 0.516, Val loss 1.074\n",
            "Ep 41 (Step 022050): Train loss 0.502, Val loss 1.070\n",
            "Ep 41 (Step 022100): Train loss 0.514, Val loss 1.069\n",
            "Ep 41 (Step 022150): Train loss 0.512, Val loss 1.069\n",
            "Ep 41 (Step 022200): Train loss 0.497, Val loss 1.066\n",
            "Ep 41 (Step 022250): Train loss 0.516, Val loss 1.065\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to predict optimal model sizes, they cannot be exhaustive in identifying optimal model sizes.  ### Response: The passage discusses downstream performance differences between different-sized models, showing that the optimal number of parameters for a 1 × 10² FLOPs model\n",
            "Ep 42 (Step 022300): Train loss 0.497, Val loss 1.069\n",
            "Ep 42 (Step 022350): Train loss 0.498, Val loss 1.066\n",
            "Ep 42 (Step 022400): Train loss 0.501, Val loss 1.069\n",
            "Ep 42 (Step 022450): Train loss 0.494, Val loss 1.067\n",
            "Ep 42 (Step 022500): Train loss 0.497, Val loss 1.065\n",
            "Ep 42 (Step 022550): Train loss 0.503, Val loss 1.068\n",
            "Ep 42 (Step 022600): Train loss 0.500, Val loss 1.061\n",
            "Ep 42 (Step 022650): Train loss 0.484, Val loss 1.062\n",
            "Ep 42 (Step 022700): Train loss 0.496, Val loss 1.055\n",
            "Ep 42 (Step 022750): Train loss 0.481, Val loss 1.057\n",
            "Ep 42 (Step 022800): Train loss 0.476, Val loss 1.056\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to predict optimal model sizes, they cannot be generalizable to all downstream tasks.  ### Response: The passage discusses downstream performance differences between different-sized models, showing that the optimal number of parameters for a 1 × 1022 FLOPs model\n",
            "Ep 43 (Step 022850): Train loss 0.471, Val loss 1.061\n",
            "Ep 43 (Step 022900): Train loss 0.479, Val loss 1.060\n",
            "Ep 43 (Step 022950): Train loss 0.476, Val loss 1.058\n",
            "Ep 43 (Step 023000): Train loss 0.492, Val loss 1.056\n",
            "Ep 43 (Step 023050): Train loss 0.508, Val loss 1.053\n",
            "Ep 43 (Step 023100): Train loss 0.474, Val loss 1.052\n",
            "Ep 43 (Step 023150): Train loss 0.493, Val loss 1.053\n",
            "Ep 43 (Step 023200): Train loss 0.486, Val loss 1.050\n",
            "Ep 43 (Step 023250): Train loss 0.494, Val loss 1.052\n",
            "Ep 43 (Step 023300): Train loss 0.480, Val loss 1.046\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to predict generalization, they may not be generalizable.  ### Response: The passage discusses downstream performance of different-sized models, showing that the optimal number of parameters for a 1 × 10² FLOPs model is around 9.5\n",
            "Ep 44 (Step 023350): Train loss 0.476, Val loss 1.046\n",
            "Ep 44 (Step 023400): Train loss 0.476, Val loss 1.049\n",
            "Ep 44 (Step 023450): Train loss 0.470, Val loss 1.050\n",
            "Ep 44 (Step 023500): Train loss 0.455, Val loss 1.049\n",
            "Ep 44 (Step 023550): Train loss 0.460, Val loss 1.047\n",
            "Ep 44 (Step 023600): Train loss 0.463, Val loss 1.045\n",
            "Ep 44 (Step 023650): Train loss 0.456, Val loss 1.042\n",
            "Ep 44 (Step 023700): Train loss 0.468, Val loss 1.042\n",
            "Ep 44 (Step 023750): Train loss 0.464, Val loss 1.045\n",
            "Ep 44 (Step 023800): Train loss 0.455, Val loss 1.039\n",
            "Ep 44 (Step 023850): Train loss 0.459, Val loss 1.039\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to predict optimal model sizes, they do not generalize well to very long training schedules.  ### Response: The passage discusses downstream performance of different-sized models, showing that the optimal number of parameters for a 1 × 10² FLOPs\n",
            "Ep 45 (Step 023900): Train loss 0.473, Val loss 1.039\n",
            "Ep 45 (Step 023950): Train loss 0.460, Val loss 1.039\n",
            "Ep 45 (Step 024000): Train loss 0.471, Val loss 1.040\n",
            "Ep 45 (Step 024050): Train loss 0.472, Val loss 1.038\n",
            "Ep 45 (Step 024100): Train loss 0.454, Val loss 1.039\n",
            "Ep 45 (Step 024150): Train loss 0.444, Val loss 1.035\n",
            "Ep 45 (Step 024200): Train loss 0.444, Val loss 1.031\n",
            "Ep 45 (Step 024250): Train loss 0.467, Val loss 1.036\n",
            "Ep 45 (Step 024300): Train loss 0.445, Val loss 1.034\n",
            "Ep 45 (Step 024350): Train loss 0.454, Val loss 1.032\n",
            "Ep 45 (Step 024400): Train loss 0.454, Val loss 1.029\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to  ### Response: The passage discusses downstream performance of different-sized models, showing that the optimal number of parameters for a 1 × 1022 FLOPs model is approximately 9.5 billion, closely matching the training loss and scaling predictions.\n",
            "Ep 46 (Step 024450): Train loss 0.441, Val loss 1.031\n",
            "Ep 46 (Step 024500): Train loss 0.427, Val loss 1.036\n",
            "Ep 46 (Step 024550): Train loss 0.447, Val loss 1.036\n",
            "Ep 46 (Step 024600): Train loss 0.438, Val loss 1.034\n",
            "Ep 46 (Step 024650): Train loss 0.449, Val loss 1.030\n",
            "Ep 46 (Step 024700): Train loss 0.436, Val loss 1.032\n",
            "Ep 46 (Step 024750): Train loss 0.450, Val loss 1.026\n",
            "Ep 46 (Step 024800): Train loss 0.445, Val loss 1.029\n",
            "Ep 46 (Step 024850): Train loss 0.445, Val loss 1.026\n",
            "Ep 46 (Step 024900): Train loss 0.443, Val loss 1.022\n",
            "Ep 46 (Step 024950): Train loss 0.439, Val loss 1.024\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to  ### Response: The passage discusses downstream performance differences between different-sized models, showing that the optimal number of parameters for a 1 × 1022 FLOPs model is around 9.5 billion, closely matching the training loss and scaling predictions\n",
            "Ep 47 (Step 025000): Train loss 0.434, Val loss 1.022\n",
            "Ep 47 (Step 025050): Train loss 0.420, Val loss 1.026\n",
            "Ep 47 (Step 025100): Train loss 0.428, Val loss 1.022\n",
            "Ep 47 (Step 025150): Train loss 0.432, Val loss 1.025\n",
            "Ep 47 (Step 025200): Train loss 0.446, Val loss 1.025\n",
            "Ep 47 (Step 025250): Train loss 0.433, Val loss 1.020\n",
            "Ep 47 (Step 025300): Train loss 0.426, Val loss 1.022\n",
            "Ep 47 (Step 025350): Train loss 0.437, Val loss 1.018\n",
            "Ep 47 (Step 025400): Train loss 0.434, Val loss 1.016\n",
            "Ep 47 (Step 025450): Train loss 0.424, Val loss 1.019\n",
            "Ep 47 (Step 025500): Train loss 0.413, Val loss 1.013\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to  ### Response: The passage discusses downstream metrics for different-sized language models, showing that the optimal number of parameters for a 1 × 10²² FLOPs model is around 9.5 billion, closely matching the training loss and scaling\n",
            "Ep 48 (Step 025550): Train loss 0.412, Val loss 1.020\n",
            "Ep 48 (Step 025600): Train loss 0.419, Val loss 1.017\n",
            "Ep 48 (Step 025650): Train loss 0.423, Val loss 1.018\n",
            "Ep 48 (Step 025700): Train loss 0.411, Val loss 1.017\n",
            "Ep 48 (Step 025750): Train loss 0.421, Val loss 1.016\n",
            "Ep 48 (Step 025800): Train loss 0.413, Val loss 1.014\n",
            "Ep 48 (Step 025850): Train loss 0.399, Val loss 1.012\n",
            "Ep 48 (Step 025900): Train loss 0.418, Val loss 1.011\n",
            "Ep 48 (Step 025950): Train loss 0.414, Val loss 1.010\n",
            "Ep 48 (Step 026000): Train loss 0.404, Val loss 1.012\n",
            "Ep 48 (Step 026050): Train loss 0.389, Val loss 1.008\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to  ### Response: The study shows that smaller models perform better on downstream tasks, they fall short of matching optimal model performance, especially on downstream tasks. Despite using scaling laws to predict optimal model sizes, the training loss remains a crucial indicator for\n",
            "Ep 49 (Step 026100): Train loss 0.412, Val loss 1.012\n",
            "Ep 49 (Step 026150): Train loss 0.394, Val loss 1.012\n",
            "Ep 49 (Step 026200): Train loss 0.396, Val loss 1.012\n",
            "Ep 49 (Step 026250): Train loss 0.408, Val loss 1.011\n",
            "Ep 49 (Step 026300): Train loss 0.409, Val loss 1.010\n",
            "Ep 49 (Step 026350): Train loss 0.411, Val loss 1.007\n",
            "Ep 49 (Step 026400): Train loss 0.412, Val loss 1.008\n",
            "Ep 49 (Step 026450): Train loss 0.406, Val loss 1.007\n",
            "Ep 49 (Step 026500): Train loss 0.416, Val loss 1.008\n",
            "Ep 49 (Step 026550): Train loss 0.409, Val loss 1.003\n",
            "Ep 49 (Step 026600): Train loss 0.398, Val loss 1.001\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to  ### Response: The passage discusses downstream performance differences between different-sized models, showing that the optimal number of parameters for a 1 × 10² FLOPs model is approximately 9.5 billion, closely matching the training loss and scaling predictions\n",
            "Ep 50 (Step 026650): Train loss 0.402, Val loss 1.009\n",
            "Ep 50 (Step 026700): Train loss 0.394, Val loss 1.010\n",
            "Ep 50 (Step 026750): Train loss 0.400, Val loss 1.007\n",
            "Ep 50 (Step 026800): Train loss 0.388, Val loss 1.009\n",
            "Ep 50 (Step 026850): Train loss 0.406, Val loss 1.006\n",
            "Ep 50 (Step 026900): Train loss 0.400, Val loss 1.005\n",
            "Ep 50 (Step 026950): Train loss 0.398, Val loss 1.002\n",
            "Ep 50 (Step 027000): Train loss 0.384, Val loss 1.002\n",
            "Ep 50 (Step 027050): Train loss 0.390, Val loss 0.999\n",
            "Ep 50 (Step 027100): Train loss 0.396, Val loss 1.001\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to  ### Response: The passage discusses downstream metrics of different-sized language models, showing that the optimal number of parameters for a 1 × 1022 FLOPs model is approximately 9.5 billion, closely matching the training loss and scaling predictions\n",
            "Ep 51 (Step 027150): Train loss 0.397, Val loss 0.998\n",
            "Ep 51 (Step 027200): Train loss 0.393, Val loss 1.002\n",
            "Ep 51 (Step 027250): Train loss 0.402, Val loss 1.002\n",
            "Ep 51 (Step 027300): Train loss 0.382, Val loss 1.003\n",
            "Ep 51 (Step 027350): Train loss 0.368, Val loss 1.006\n",
            "Ep 51 (Step 027400): Train loss 0.386, Val loss 1.004\n",
            "Ep 51 (Step 027450): Train loss 0.390, Val loss 1.002\n",
            "Ep 51 (Step 027500): Train loss 0.381, Val loss 0.998\n",
            "Ep 51 (Step 027550): Train loss 0.381, Val loss 0.995\n",
            "Ep 51 (Step 027600): Train loss 0.382, Val loss 0.996\n",
            "Ep 51 (Step 027650): Train loss 0.387, Val loss 0.992\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to  ### Response: The passage discusses downstream metrics of different-sized language models, showing that the optimal number of parameters for a 1 × 1022 FLOPs model is approximately 9.5 billion, closely matching the training loss and scaling predictions\n",
            "Ep 52 (Step 027700): Train loss 0.396, Val loss 0.999\n",
            "Ep 52 (Step 027750): Train loss 0.367, Val loss 0.997\n",
            "Ep 52 (Step 027800): Train loss 0.385, Val loss 1.000\n",
            "Ep 52 (Step 027850): Train loss 0.366, Val loss 0.999\n",
            "Ep 52 (Step 027900): Train loss 0.385, Val loss 0.995\n",
            "Ep 52 (Step 027950): Train loss 0.382, Val loss 0.996\n",
            "Ep 52 (Step 028000): Train loss 0.373, Val loss 0.995\n",
            "Ep 52 (Step 028050): Train loss 0.359, Val loss 0.994\n",
            "Ep 52 (Step 028100): Train loss 0.378, Val loss 0.993\n",
            "Ep 52 (Step 028150): Train loss 0.375, Val loss 0.990\n",
            "Ep 52 (Step 028200): Train loss 0.370, Val loss 0.992\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to  ### Response: The passage discusses downstream metrics of different-sized language models, showing that the optimal number of parameters for a 1 × 1022 FLOPs model is approximately 9.5 billion, closely matching the training loss and scaling predictions\n",
            "Ep 53 (Step 028250): Train loss 0.355, Val loss 0.994\n",
            "Ep 53 (Step 028300): Train loss 0.365, Val loss 0.997\n",
            "Ep 53 (Step 028350): Train loss 0.371, Val loss 0.995\n",
            "Ep 53 (Step 028400): Train loss 0.370, Val loss 0.992\n",
            "Ep 53 (Step 028450): Train loss 0.359, Val loss 0.990\n",
            "Ep 53 (Step 028500): Train loss 0.350, Val loss 0.986\n",
            "Ep 53 (Step 028550): Train loss 0.372, Val loss 0.992\n",
            "Ep 53 (Step 028600): Train loss 0.372, Val loss 0.991\n",
            "Ep 53 (Step 028650): Train loss 0.362, Val loss 0.987\n",
            "Ep 53 (Step 028700): Train loss 0.367, Val loss 0.986\n",
            "Ep 53 (Step 028750): Train loss 0.364, Val loss 0.985\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to  ### Response: The passage discusses downstream metrics of different-sized language models, showing that the optimal number of parameters for a 1 × 1022 FLOPs model is approximately 9.5 billion, closely matching the training loss and scaling predictions\n",
            "Ep 54 (Step 028800): Train loss 0.349, Val loss 0.987\n",
            "Ep 54 (Step 028850): Train loss 0.356, Val loss 0.989\n",
            "Ep 54 (Step 028900): Train loss 0.354, Val loss 0.993\n",
            "Ep 54 (Step 028950): Train loss 0.367, Val loss 0.990\n",
            "Ep 54 (Step 029000): Train loss 0.345, Val loss 0.989\n",
            "Ep 54 (Step 029050): Train loss 0.366, Val loss 0.991\n",
            "Ep 54 (Step 029100): Train loss 0.354, Val loss 0.989\n",
            "Ep 54 (Step 029150): Train loss 0.356, Val loss 0.986\n",
            "Ep 54 (Step 029200): Train loss 0.338, Val loss 0.987\n",
            "Ep 54 (Step 029250): Train loss 0.336, Val loss 0.982\n",
            "Ep 54 (Step 029300): Train loss 0.348, Val loss 0.981\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to  ### Response: The passage discusses downstream metrics of different-sized language models, showing that the optimal number of parameters for a 1 × 10² FLOPs model is approximately 9.5 billion, closely matching the training loss and scaling predictions\n",
            "Ep 55 (Step 029350): Train loss 0.371, Val loss 0.988\n",
            "Ep 55 (Step 029400): Train loss 0.365, Val loss 0.989\n",
            "Ep 55 (Step 029450): Train loss 0.341, Val loss 0.989\n",
            "Ep 55 (Step 029500): Train loss 0.356, Val loss 0.982\n",
            "Ep 55 (Step 029550): Train loss 0.347, Val loss 0.985\n",
            "Ep 55 (Step 029600): Train loss 0.349, Val loss 0.985\n",
            "Ep 55 (Step 029650): Train loss 0.345, Val loss 0.986\n",
            "Ep 55 (Step 029700): Train loss 0.365, Val loss 0.980\n",
            "Ep 55 (Step 029750): Train loss 0.337, Val loss 0.985\n",
            "Ep 55 (Step 029800): Train loss 0.340, Val loss 0.982\n",
            "Ep 55 (Step 029850): Train loss 0.340, Val loss 0.981\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Summarize the following passage in 1–3 sentences.  ### Input: We show downstream results of differently-sized models in Table 15 in the Appendix. Downstream metrics suggest that the optimal number of parameters for a 1 × 1022 FLOPs model is in fact around 9.5B, in close agreement with the training loss and scaling predictions. However, we note that the training loss is not a perfect proxy for downstream metrics. For example, the 9.5B model, which shows the lowest loss (in Table 1) and is closest to the optimal model, slightly underperforms the 16.1B model on downstream tasks. This suggests that while scaling laws can be used to  ### Response: The passage discusses downstream metrics of different-sized language models, showing that the optimal number of parameters for a 1 × 10² FLOPs model is approximately 9.5 billion, closely matching the training loss and scaling predictions\n",
            "🟨 saved → models/gpt2o_finetuned.pth\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gpt2o = gpt2o.to(device)\n",
        "gpt2o.train()\n",
        "\n",
        "optimizer_gpt2o = torch.optim.AdamW(gpt2o.parameters(), lr=5e-5, weight_decay=0.1)\n",
        "\n",
        "torch.manual_seed(123)\n",
        "train_losses_g, val_losses_g, tokens_g = train_model_simple(\n",
        "    gpt2o, train_loader_gpt2, val_loader_gpt2, optimizer_gpt2o, device,\n",
        "    num_epochs=55, eval_freq=50, eval_iter=20,\n",
        "    start_context=format_input(val_set[0]), tokenizer=tokenizer_gpt2)\n",
        "\n",
        "torch.save(gpt2o.state_dict(), \"models/gpt2o_finetuned.pth\")\n",
        "print(\"🟨 saved → models/gpt2o_finetuned.pth\")\n",
        "\n",
        "epochs_tensor = torch.linspace(0, 55, len(train_losses_g))\n",
        "plot_losses(epochs_tensor, tokens_g, train_losses_g, val_losses_g)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "76584e05",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🟨 GPT-2o finetuned model loaded.\n"
          ]
        }
      ],
      "source": [
        "# === Load fine-tuned GPT-2o model ===\n",
        "gpt2 = True\n",
        "tokenizer_gpt2 = tiktoken.get_encoding(\"gpt2\")\n",
        "vocab_size = tokenizer_gpt2.n_vocab\n",
        "NEW_CONFIG[\"vocab_size\"] = vocab_size\n",
        "\n",
        "gpt2o = GPTModel(NEW_CONFIG).to(device)\n",
        "gpt2o.load_state_dict(torch.load(\"models/gpt2o_finetuned.pth\", map_location=device))\n",
        "gpt2o.eval()\n",
        "print(\"🟨 GPT-2o finetuned model loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ead01a4",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating models responses: 100%|██████████| 500/500 [30:06<00:00,  3.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🟨 Saved GPT-2o model responses → data/gpt2o_val_responses.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# === Generate GPT model responses ===\n",
        "gpt2 = True\n",
        "for i, entry in tqdm(enumerate(val_set), total=len(val_set), desc=\"Generating models responses\"):\n",
        "    alpaca_prompt = (\n",
        "        f\"Below is an instruction that describes a task. \"\n",
        "        f\"Write a response that appropriately completes the request.\"\n",
        "        f\"\\n\\n### Instruction:\\n{entry['Instruction']}\")\n",
        "    if entry['Input']:\n",
        "        alpaca_prompt += f\"\\n\\n### Input:\\n{entry['Input']}\"\n",
        "    alpaca_prompt += \"\\n\\n### Response:\\n\"\n",
        "\n",
        "    entry[\"model_response\"] = generate_text(gpt2o, tokenizer_gpt2, alpaca_prompt)\n",
        "\n",
        "with open(\"responces/gpt2o_val_responses.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(val_set, f, indent=2, ensure_ascii=False)\n",
        "print(\"🟨 Saved GPT-2o model responses → responces/gpt2o_val_responses.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab8e22f8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🟨 Loaded 500 GPT2o val responses.\n"
          ]
        }
      ],
      "source": [
        "# Load GPT model responses\n",
        "with open(\"responces/gpt2o_val_responses.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    gpt2o_val_responses = json.load(f)\n",
        "print(f\"🟨 Loaded {len(gpt2o_val_responses)} GPT2o val responses.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9327cb5",
      "metadata": {},
      "source": [
        "# Thirdly: Compare Performance \n",
        "Compare performance both **Quantitatively** and **Qualitatively** for the three models:\n",
        "   - **Regex**,\n",
        "   - **GPT-2**,\n",
        "   - **GPT-2o (Pretrained Weights)**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "44ded91f",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Regex Quantitative Metrics ===\n",
            "BLEU-4:    4.99  (BP=1.00, P1=24.68, P2=7.71, P3=2.84, P4=1.14)\n",
            "ROUGE-1/2/L-F1: 0.383 / 0.114 / 0.198\n",
            "METEOR:    0.237 (used 500)\n",
            "Token-F1:  0.299 (P=0.250, R=0.373)\n",
            "BERTScore: -0.104 (P=-0.239, R=0.039, model=roberta-large, baseline=True)\n",
            "Count:     500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== GPT-2 Quantitative Metrics ===\n",
            "BLEU-4:    9.18  (BP=1.00, P1=35.78, P2=12.66, P3=5.61, P4=2.79)\n",
            "ROUGE-1/2/L-F1: 0.400 / 0.120 / 0.212\n",
            "METEOR:    0.235 (used 500)\n",
            "Token-F1:  0.324 (P=0.306, R=0.345)\n",
            "BERTScore: 0.061 (P=0.009, R=0.115, model=roberta-large, baseline=True)\n",
            "Count:     500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== GPT-2o Quantitative Metrics ===\n",
            "BLEU-4:    20.26  (BP=1.00, P1=44.91, P2=23.15, P3=15.05, P4=10.77)\n",
            "ROUGE-1/2/L-F1: 0.510 / 0.244 / 0.312\n",
            "METEOR:    0.357 (used 500)\n",
            "Token-F1:  0.426 (P=0.394, R=0.464)\n",
            "BERTScore: 0.268 (P=0.227, R=0.311, model=roberta-large, baseline=True)\n",
            "Count:     500\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'BLEU': {'BLEU-4': 20.259972701572533,\n",
              "  'P1': 44.90943873073818,\n",
              "  'P2': 23.151706388379637,\n",
              "  'P3': 15.04656680477354,\n",
              "  'P4': 10.769545422209575,\n",
              "  'BP': 1.0,\n",
              "  'sys_len': 113956,\n",
              "  'ref_len': 93198},\n",
              " 'ROUGE': {'ROUGE-1-F1': 0.5095246558825287,\n",
              "  'ROUGE-2-F1': 0.24385807699281553,\n",
              "  'ROUGE-L-F1': 0.31217573124736775},\n",
              " 'METEOR': {'METEOR': 0.35693324638064283, 'used': 500},\n",
              " 'TokenF1': {'precision': 0.39395781747435443,\n",
              "  'recall': 0.4643154932513419,\n",
              "  'F1': 0.4262528405693099},\n",
              " 'BERTScore': {'P': 0.22745652496814728,\n",
              "  'R': 0.3108866512775421,\n",
              "  'F1': 0.26827484369277954,\n",
              "  'model_type': 'roberta-large',\n",
              "  'baseline_rescale': True},\n",
              " 'count': 500}"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate_model_outputs(\"Regex\", regex_val_responses)\n",
        "evaluate_model_outputs(\"GPT-2\", gpt2_val_responses)\n",
        "evaluate_model_outputs(\"GPT-2o\", gpt2o_val_responses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "564fade6",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Judge Scoring (Regex):   8%|▊         | 10/125 [00:42<08:43,  4.55s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "Judge Scoring (Regex): 100%|██████████| 125/125 [07:53<00:00,  3.79s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Judge] Regex: avg=43.3% | min=0 | max=100 | n=500\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Judge Scoring (GPT-2): 100%|██████████| 125/125 [07:42<00:00,  3.70s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Judge] GPT-2: avg=49.9% | min=0 | max=100 | n=500\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Judge Scoring (GPT-2o): 100%|██████████| 125/125 [07:52<00:00,  3.78s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Judge] GPT-2o: avg=68.6% | min=0 | max=100 | n=500\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'model': 'GPT-2o',\n",
              " 'avg': 68.602,\n",
              " 'min': 0,\n",
              " 'max': 100,\n",
              " 'n': 500,\n",
              " 'scores': [78,\n",
              "  85,\n",
              "  42,\n",
              "  75,\n",
              "  100,\n",
              "  65,\n",
              "  65,\n",
              "  100,\n",
              "  78,\n",
              "  100,\n",
              "  95,\n",
              "  85,\n",
              "  3,\n",
              "  85,\n",
              "  65,\n",
              "  78,\n",
              "  0,\n",
              "  75,\n",
              "  85,\n",
              "  85,\n",
              "  85,\n",
              "  87,\n",
              "  87,\n",
              "  78,\n",
              "  85,\n",
              "  100,\n",
              "  85,\n",
              "  65,\n",
              "  70,\n",
              "  72,\n",
              "  85,\n",
              "  35,\n",
              "  100,\n",
              "  5,\n",
              "  75,\n",
              "  85,\n",
              "  85,\n",
              "  85,\n",
              "  100,\n",
              "  65,\n",
              "  72,\n",
              "  92,\n",
              "  100,\n",
              "  75,\n",
              "  5,\n",
              "  5,\n",
              "  85,\n",
              "  78,\n",
              "  85,\n",
              "  85,\n",
              "  87,\n",
              "  85,\n",
              "  85,\n",
              "  87,\n",
              "  65,\n",
              "  85,\n",
              "  45,\n",
              "  0,\n",
              "  87,\n",
              "  85,\n",
              "  87,\n",
              "  85,\n",
              "  100,\n",
              "  0,\n",
              "  75,\n",
              "  75,\n",
              "  85,\n",
              "  85,\n",
              "  75,\n",
              "  72,\n",
              "  1,\n",
              "  78,\n",
              "  25,\n",
              "  92,\n",
              "  3,\n",
              "  85,\n",
              "  52,\n",
              "  2,\n",
              "  95,\n",
              "  75,\n",
              "  6,\n",
              "  3,\n",
              "  75,\n",
              "  85,\n",
              "  62,\n",
              "  72,\n",
              "  100,\n",
              "  100,\n",
              "  2,\n",
              "  87,\n",
              "  3,\n",
              "  78,\n",
              "  85,\n",
              "  75,\n",
              "  100,\n",
              "  75,\n",
              "  65,\n",
              "  5,\n",
              "  75,\n",
              "  65,\n",
              "  75,\n",
              "  72,\n",
              "  85,\n",
              "  58,\n",
              "  8,\n",
              "  85,\n",
              "  85,\n",
              "  85,\n",
              "  87,\n",
              "  85,\n",
              "  7,\n",
              "  75,\n",
              "  85,\n",
              "  75,\n",
              "  25,\n",
              "  3,\n",
              "  100,\n",
              "  85,\n",
              "  72,\n",
              "  75,\n",
              "  65,\n",
              "  75,\n",
              "  87,\n",
              "  85,\n",
              "  85,\n",
              "  8,\n",
              "  10,\n",
              "  75,\n",
              "  85,\n",
              "  72,\n",
              "  85,\n",
              "  83,\n",
              "  65,\n",
              "  85,\n",
              "  85,\n",
              "  75,\n",
              "  85,\n",
              "  95,\n",
              "  65,\n",
              "  78,\n",
              "  100,\n",
              "  85,\n",
              "  75,\n",
              "  85,\n",
              "  75,\n",
              "  85,\n",
              "  65,\n",
              "  72,\n",
              "  85,\n",
              "  92,\n",
              "  75,\n",
              "  75,\n",
              "  65,\n",
              "  75,\n",
              "  3,\n",
              "  85,\n",
              "  65,\n",
              "  85,\n",
              "  3,\n",
              "  5,\n",
              "  85,\n",
              "  75,\n",
              "  65,\n",
              "  65,\n",
              "  75,\n",
              "  78,\n",
              "  85,\n",
              "  65,\n",
              "  8,\n",
              "  85,\n",
              "  85,\n",
              "  75,\n",
              "  3,\n",
              "  75,\n",
              "  95,\n",
              "  75,\n",
              "  72,\n",
              "  82,\n",
              "  78,\n",
              "  75,\n",
              "  85,\n",
              "  85,\n",
              "  67,\n",
              "  78,\n",
              "  78,\n",
              "  87,\n",
              "  85,\n",
              "  85,\n",
              "  73,\n",
              "  72,\n",
              "  7,\n",
              "  78,\n",
              "  82,\n",
              "  85,\n",
              "  8,\n",
              "  85,\n",
              "  85,\n",
              "  10,\n",
              "  78,\n",
              "  75,\n",
              "  75,\n",
              "  85,\n",
              "  8,\n",
              "  75,\n",
              "  65,\n",
              "  87,\n",
              "  1,\n",
              "  85,\n",
              "  73,\n",
              "  75,\n",
              "  85,\n",
              "  85,\n",
              "  85,\n",
              "  65,\n",
              "  65,\n",
              "  75,\n",
              "  100,\n",
              "  75,\n",
              "  100,\n",
              "  78,\n",
              "  35,\n",
              "  3,\n",
              "  85,\n",
              "  78,\n",
              "  98,\n",
              "  85,\n",
              "  58,\n",
              "  12,\n",
              "  85,\n",
              "  75,\n",
              "  65,\n",
              "  3,\n",
              "  75,\n",
              "  75,\n",
              "  100,\n",
              "  85,\n",
              "  72,\n",
              "  85,\n",
              "  78,\n",
              "  85,\n",
              "  100,\n",
              "  65,\n",
              "  87,\n",
              "  1,\n",
              "  87,\n",
              "  55,\n",
              "  85,\n",
              "  75,\n",
              "  0,\n",
              "  10,\n",
              "  85,\n",
              "  78,\n",
              "  85,\n",
              "  87,\n",
              "  7,\n",
              "  85,\n",
              "  82,\n",
              "  64,\n",
              "  68,\n",
              "  83,\n",
              "  3,\n",
              "  65,\n",
              "  85,\n",
              "  78,\n",
              "  75,\n",
              "  85,\n",
              "  10,\n",
              "  85,\n",
              "  75,\n",
              "  87,\n",
              "  100,\n",
              "  80,\n",
              "  3,\n",
              "  78,\n",
              "  72,\n",
              "  75,\n",
              "  75,\n",
              "  85,\n",
              "  65,\n",
              "  75,\n",
              "  100,\n",
              "  65,\n",
              "  3,\n",
              "  85,\n",
              "  100,\n",
              "  83,\n",
              "  72,\n",
              "  85,\n",
              "  72,\n",
              "  72,\n",
              "  78,\n",
              "  100,\n",
              "  85,\n",
              "  87,\n",
              "  65,\n",
              "  75,\n",
              "  58,\n",
              "  72,\n",
              "  70,\n",
              "  72,\n",
              "  87,\n",
              "  100,\n",
              "  75,\n",
              "  72,\n",
              "  85,\n",
              "  93,\n",
              "  85,\n",
              "  50,\n",
              "  72,\n",
              "  85,\n",
              "  85,\n",
              "  14,\n",
              "  3,\n",
              "  85,\n",
              "  85,\n",
              "  85,\n",
              "  85,\n",
              "  85,\n",
              "  100,\n",
              "  100,\n",
              "  92,\n",
              "  2,\n",
              "  75,\n",
              "  75,\n",
              "  85,\n",
              "  25,\n",
              "  85,\n",
              "  65,\n",
              "  7,\n",
              "  85,\n",
              "  75,\n",
              "  85,\n",
              "  87,\n",
              "  85,\n",
              "  35,\n",
              "  65,\n",
              "  75,\n",
              "  85,\n",
              "  65,\n",
              "  85,\n",
              "  75,\n",
              "  65,\n",
              "  10,\n",
              "  75,\n",
              "  100,\n",
              "  5,\n",
              "  85,\n",
              "  78,\n",
              "  85,\n",
              "  85,\n",
              "  85,\n",
              "  75,\n",
              "  0,\n",
              "  87,\n",
              "  85,\n",
              "  78,\n",
              "  55,\n",
              "  5,\n",
              "  45,\n",
              "  45,\n",
              "  3,\n",
              "  87,\n",
              "  87,\n",
              "  75,\n",
              "  85,\n",
              "  85,\n",
              "  72,\n",
              "  85,\n",
              "  85,\n",
              "  87,\n",
              "  75,\n",
              "  100,\n",
              "  78,\n",
              "  75,\n",
              "  85,\n",
              "  78,\n",
              "  72,\n",
              "  85,\n",
              "  78,\n",
              "  87,\n",
              "  85,\n",
              "  100,\n",
              "  75,\n",
              "  4,\n",
              "  75,\n",
              "  100,\n",
              "  65,\n",
              "  75,\n",
              "  85,\n",
              "  7,\n",
              "  75,\n",
              "  20,\n",
              "  6,\n",
              "  85,\n",
              "  54,\n",
              "  85,\n",
              "  85,\n",
              "  1,\n",
              "  3,\n",
              "  1,\n",
              "  85,\n",
              "  87,\n",
              "  87,\n",
              "  83,\n",
              "  72,\n",
              "  3,\n",
              "  72,\n",
              "  72,\n",
              "  75,\n",
              "  72,\n",
              "  3,\n",
              "  87,\n",
              "  85,\n",
              "  85,\n",
              "  72,\n",
              "  75,\n",
              "  8,\n",
              "  3,\n",
              "  3,\n",
              "  60,\n",
              "  5,\n",
              "  5,\n",
              "  65,\n",
              "  85,\n",
              "  85,\n",
              "  78,\n",
              "  85,\n",
              "  65,\n",
              "  85,\n",
              "  85,\n",
              "  85,\n",
              "  45,\n",
              "  100,\n",
              "  85,\n",
              "  75,\n",
              "  85,\n",
              "  75,\n",
              "  85,\n",
              "  65,\n",
              "  0,\n",
              "  75,\n",
              "  85,\n",
              "  75,\n",
              "  72,\n",
              "  85,\n",
              "  8,\n",
              "  87,\n",
              "  3,\n",
              "  85,\n",
              "  72,\n",
              "  85,\n",
              "  100,\n",
              "  85,\n",
              "  85,\n",
              "  85,\n",
              "  55,\n",
              "  75,\n",
              "  100,\n",
              "  75,\n",
              "  85,\n",
              "  85,\n",
              "  85,\n",
              "  85,\n",
              "  85,\n",
              "  87,\n",
              "  85,\n",
              "  85,\n",
              "  75,\n",
              "  85,\n",
              "  83,\n",
              "  100,\n",
              "  75,\n",
              "  0,\n",
              "  0,\n",
              "  100,\n",
              "  78,\n",
              "  100,\n",
              "  20,\n",
              "  92,\n",
              "  75,\n",
              "  85,\n",
              "  85,\n",
              "  85,\n",
              "  75,\n",
              "  85,\n",
              "  75,\n",
              "  0,\n",
              "  46,\n",
              "  85,\n",
              "  25,\n",
              "  85,\n",
              "  87,\n",
              "  85,\n",
              "  75,\n",
              "  75,\n",
              "  75,\n",
              "  85,\n",
              "  78,\n",
              "  78,\n",
              "  23]}"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate_with_judge(\"Regex\", regex_val_responses, batch_size=4)\n",
        "evaluate_with_judge(\"GPT-2\", gpt2_val_responses,  batch_size=4)\n",
        "evaluate_with_judge(\"GPT-2o\", gpt2o_val_responses,  batch_size=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c99283e7",
      "metadata": {},
      "source": [
        "### Interpretation\n",
        "\n",
        "- The **GPT-2 Pretrained model** performs best across all metrics, as expected.\n",
        "- The **GPT-2 tokenizer model trained from scratch** performs reasonably well and significantly better than the regex version.\n",
        "- The **Regex tokenizer model** performs the weakest, highlighting limitations of simple tokenizers in LLM training.\n",
        "\n",
        "This comparison demonstrates the impact of:\n",
        "- Tokenizer choice  \n",
        "- Pretraining vs. starting from scratch  \n",
        "- Dataset quality and instruction tuning  \n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "LLM",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
